<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>nloptr • nloptr</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="nloptr">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">nloptr</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.2.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/nloptr.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-news" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">News</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-news">
<li><h6 class="dropdown-header" data-toc-skip>Releases</h6></li>
    <li><a class="external-link dropdown-item" href="https://astamm.github.io/posts/2024-06-19-nloptr-210/">nloptr 2.1.0</a></li>
    <li><a class="external-link dropdown-item" href="https://astamm.github.io/posts/2025-03-12-nloptr-220/">nloptr 2.2.0</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><a class="dropdown-item" href="../news/index.html">Changelog</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/astamm/nloptr/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>nloptr</h1>
                        <h4 data-toc-skip class="author">Jelmer Ypma,
Aymeric Stamm, and Avraham Adler</h4>
            
            <h4 data-toc-skip class="date">2025-03-12</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/astamm/nloptr/blob/master/vignettes/nloptr.Rmd" class="external-link"><code>vignettes/nloptr.Rmd</code></a></small>
      <div class="d-none name"><code>nloptr.Rmd</code></div>
    </div>

    
    
<p>This document is an introduction to <code>nloptr</code>: an R
interface to NLopt. <a href="https://nlopt.readthedocs.io/en/latest/" class="external-link">NLopt</a> is a
free/open-source library for nonlinear optimization, started by Steven
G. Johnson, providing a common interface for a number of different free
optimization routines available online as well as original
implementations of various other algorithms. The NLopt library is
available under the GNU Lesser General Public License (LGPL), and the
copyrights are owned by a variety of authors. This package should be
considered in beta and comments about any aspect of the package are
welcome. This document is an R vignette prepared with the aid of
<code>knitr</code> <span class="citation">(Xie 2014, 2015, 2016)</span>.
Financial support of the UK Economic and Social Research Council through
a grant (RES-589-28-0001) to the ESRC Centre for Microdata Methods and
Practice (CeMMAP) is gratefully acknowledged.</p>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>NLopt addresses general nonlinear optimization problems of the form:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><munder><mo>min</mo><mrow><mi>x</mi><mo>∈</mo><msup><mi>R</mi><mi>n</mi></msup></mrow></munder><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>s</mi><mi>.</mi><mi>t</mi><mi>.</mi></mtd><mtd columnalign="left" style="text-align: left"><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><msub><mi>x</mi><mi>L</mi></msub><mo>≤</mo><mi>x</mi><mo>≤</mo><msub><mi>x</mi><mi>U</mi></msub></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
&amp;\min_{x \in R^n} f(x) \\
s.t.&amp; g(x) \leq 0 \\
&amp; h(x) = 0 \\
&amp; x_L \leq x \leq x_U
\end{aligned}
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot)</annotation></semantics></math>
is the objective function and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
represents the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
optimization parameters. This problem may optionally be subject to the
bound constraints (also called box constraints),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>L</mi></msub><annotation encoding="application/x-tex">x_L</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>U</mi></msub><annotation encoding="application/x-tex">x_U</annotation></semantics></math>.
For partially or totally unconstrained problems the bounds can take
values
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>∞</mi></mrow><annotation encoding="application/x-tex">-\infty</annotation></semantics></math>
or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>∞</mi><annotation encoding="application/x-tex">\infty</annotation></semantics></math>.
One may also optionally have
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
nonlinear inequality constraints—sometimes called a nonlinear
programming problem—which may be specified in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation></semantics></math>,
and equality constraints which may be specified in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(\cdot)</annotation></semantics></math>.
Note that not all of the algorithms in NLopt can handle constraints.</p>
<p>This vignette describes how to formulate minimization problems to be
solved with the R interface to NLopt. If you want to use the C interface
directly or are interested in the Matlab interface, there are other
sources of documentation available. Some of the information here has
been taken from the NLopt website, where more details are available. All
credit for implementing the C code for the different algorithms
available in NLopt should go to the respective authors. Also, please see
the <a href="https://nlopt.readthedocs.io/en/latest/Citing_NLopt/" class="external-link">website</a>
for information on how to cite NLopt and the algorithms you use.</p>
</div>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>This package is on CRAN and can be installed from within R using</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"nloptr"</span><span class="op">)</span></span></code></pre></div>
<p>or</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"nloptr"</span>, type <span class="op">=</span> <span class="st">"source"</span><span class="op">)</span></span></code></pre></div>
<p>to install the package from source. You should now be able to load
the R interface to NLopt and read the help.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/astamm/nloptr" class="external-link">"nloptr"</a></span><span class="op">)</span></span>
<span><span class="op">?</span><span class="va">nloptr</span></span></code></pre></div>
<p>The most recent experimental <em>source</em> version of
<code>nloptr</code> can be installed from Github using the
<code>remotes</code> package:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># install.packages("remotes")</span></span>
<span><span class="fu">remotes</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"astamm/nloptr"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="minimizing-the-rosenbrock-banana-function">Minimizing the Rosenbrock Banana function<a class="anchor" aria-label="anchor" href="#minimizing-the-rosenbrock-banana-function"></a>
</h2>
<p>As a first example we will solve an unconstrained minimization
problem. The function we look at is the Rosenbrock Banana function:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>100</mn><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo>−</mo><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
f(x) = 100 \left(x_2-x_1^2\right)^2 + \left(1-x_1\right)^2,
</annotation></semantics></math></p>
<p>which is also used as an example in the documentation for the
standard R optimizer <code>optim</code>. The gradient of the objective
function is given by: <span class="math display">$$
\nabla f(x) =
\left(\begin{array}[1]{c}
-400 \cdot x_1 \cdot (x_2 - x_1^2) - 2 \cdot (1 - x_1) \\
200 \cdot (x_2 - x_1^2)
\end{array} \right).
$$</span></p>
<p>Not all of the algorithms in NLopt need gradients to be supplied by
the user. We will show examples with and without supplying the gradient.
After loading the library.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/astamm/nloptr" class="external-link">nloptr</a></span><span class="op">)</span></span></code></pre></div>
<p>We start by specifying the objective function and its gradient:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Rosenbrock Banana function</span></span>
<span><span class="va">eval_f</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fl">100</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co">## Gradient of Rosenbrock Banana function</span></span>
<span><span class="va">eval_grad_f</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">400</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">-</span> <span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>,</span>
<span>    <span class="fl">200</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>We define initial values</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># initial values</span></span>
<span><span class="va">x0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1.2</span>, <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p>and then minimize the function using the <code>nloptr</code> command.
This command runs some checks on the supplied inputs and returns an
object with the exit code of the solver, the optimal value of the
objective function and the solution. Before we can minimize the function
we need to specify which algorithm we want to use</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">opts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"algorithm"</span> <span class="op">=</span> <span class="st">"NLOPT_LD_LBFGS"</span>,</span>
<span>             <span class="st">"xtol_rel"</span> <span class="op">=</span> <span class="fl">1.0e-8</span><span class="op">)</span></span></code></pre></div>
<p>Here we use the L-BFGS algorithm <span class="citation">(Nocedal
1980; Liu and Nocedal 1989)</span>. The characters <code>LD</code> in
the algorithm show that this algorithm looks for local minima
(<code>L</code>) using a derivative-based (<code>D</code>) algorithm.
Other algorithms look for global (<code>G</code>) minima, or they don’t
need derivatives (<code>N</code>). We also specified the termination
criterium in terms of the relative x-tolerance. Other termination
criteria are available (see Appendix <code>\ref{sec:descoptions}</code>
for a full list of options). We then solve the minimization problem
using</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># solve Rosenbrock Banana function</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nloptr.html">nloptr</a></span><span class="op">(</span>x0 <span class="op">=</span> <span class="va">x0</span>,</span>
<span>              eval_f <span class="op">=</span> <span class="va">eval_f</span>,</span>
<span>              eval_grad_f <span class="op">=</span> <span class="va">eval_grad_f</span>,</span>
<span>              opts <span class="op">=</span> <span class="va">opts</span><span class="op">)</span></span></code></pre></div>
<p>We can see the results by printing the resulting object.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## nloptr(x0 = x0, eval_f = eval_f, eval_grad_f = eval_grad_f, opts = opts)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Minimization using NLopt version 2.7.1 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success </span></span>
<span><span class="co">## return value. )</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Number of Iterations....: 56 </span></span>
<span><span class="co">## Termination conditions:  xtol_rel: 1e-08 </span></span>
<span><span class="co">## Number of inequality constraints:  0 </span></span>
<span><span class="co">## Number of equality constraints:    0 </span></span>
<span><span class="co">## Optimal value of objective function:  7.35727226897802e-23 </span></span>
<span><span class="co">## Optimal value of controls: 1 1</span></span></code></pre>
<p>Sometimes the objective function and its gradient contain common
terms. To economize on calculations, we can return the objective and its
gradient in a list. For the Rosenbrock Banana function we have for
instance:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Rosenbrock Banana function and gradient in one function</span></span>
<span><span class="va">eval_f_list</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">common_term</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"objective"</span> <span class="op">=</span> <span class="fl">100</span> <span class="op">*</span> <span class="va">common_term</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span>,</span>
<span>       <span class="st">"gradient"</span>  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">400</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">common_term</span> <span class="op">-</span> <span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>,</span>
<span>                       <span class="fl">200</span> <span class="op">*</span> <span class="va">common_term</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>which we minimize using</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nloptr.html">nloptr</a></span><span class="op">(</span>x0 <span class="op">=</span> <span class="va">x0</span>,</span>
<span>              eval_f <span class="op">=</span> <span class="va">eval_f_list</span>,</span>
<span>              opts <span class="op">=</span> <span class="va">opts</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## nloptr(x0 = x0, eval_f = eval_f_list, opts = opts)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Minimization using NLopt version 2.7.1 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success </span></span>
<span><span class="co">## return value. )</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Number of Iterations....: 56 </span></span>
<span><span class="co">## Termination conditions:  xtol_rel: 1e-08 </span></span>
<span><span class="co">## Number of inequality constraints:  0 </span></span>
<span><span class="co">## Number of equality constraints:    0 </span></span>
<span><span class="co">## Optimal value of objective function:  7.35727226897802e-23 </span></span>
<span><span class="co">## Optimal value of controls: 1 1</span></span></code></pre>
<p>This gives the same results as before.</p>
</div>
<div class="section level2">
<h2 id="minimization-with-inequality-constraints">Minimization with inequality constraints<a class="anchor" aria-label="anchor" href="#minimization-with-inequality-constraints"></a>
</h2>
<p>This section shows how to minimize a function subject to inequality
constraints. This example is the same as the one used in the tutorial on
the NLopt website. The problem we want to solve is:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><munder><mo>min</mo><mrow><mi>x</mi><mo>∈</mo><msup><mi>R</mi><mi>n</mi></msup></mrow></munder><msqrt><msub><mi>x</mi><mn>2</mn></msub></msqrt></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>s</mi><mi>.</mi><mi>t</mi><mi>.</mi></mtd><mtd columnalign="left" style="text-align: left"><msub><mi>x</mi><mn>2</mn></msub><mo>≥</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><msub><mi>x</mi><mn>2</mn></msub><mo>≥</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>a</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><msub><mi>x</mi><mn>2</mn></msub><mo>≥</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>a</mi><mn>2</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
&amp;\min_{x \in R^n} \sqrt{x_2} \\
s.t.&amp; x_2 \geq 0 \\
&amp; x_2 \geq (a_1 x_1 + b_1)^3 \\
&amp; x_2 \geq (a_2 x_1 + b_2)^3, 
\end{aligned}
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>1</mn></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">a_1 = 2</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mn>1</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">b_1 = 0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>2</mn></msub><mo>=</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">a_2 = -1</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mn>2</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">b_2 = 1</annotation></semantics></math>.
In order to solve this problem, we first have to re-formulate the
constraints to be of the form
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">g(x)
\leq 0</annotation></semantics></math>. Note that the first constraint
is a bound on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>2</mn></msub><annotation encoding="application/x-tex">x_2</annotation></semantics></math>,
which we will add later. The other two constraints can be re-written
as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>a</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup><mo>−</mo><msub><mi>x</mi><mn>2</mn></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>≤</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>a</mi><mn>2</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup><mo>−</mo><msub><mi>x</mi><mn>2</mn></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>≤</mo><mn>0</mn></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
(a_1 x_1 + b_1)^3 - x_2 &amp;\leq 0 \\
(a_2 x_1 + b_2)^3 - x_2 &amp;\leq 0
\end{aligned}
</annotation></semantics></math></p>
<p>First, define R functions to calculate the objective function and its
gradient:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># objective function</span></span>
<span><span class="va">eval_f0</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">a</span>, <span class="va">b</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># gradient of objective function</span></span>
<span><span class="va">eval_grad_f0</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">a</span>, <span class="va">b</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.5</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>If needed, these can of course be calculated in the same function as
before. Then define the two constraints and the Jacobian of the
constraints:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># constraint function</span></span>
<span><span class="va">eval_g0</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">a</span>, <span class="va">b</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="op">(</span><span class="va">a</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">b</span><span class="op">)</span> <span class="op">^</span> <span class="fl">3</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Jacobian of constraint</span></span>
<span><span class="va">eval_jac_g0</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">a</span>, <span class="va">b</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span> <span class="op">*</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="op">(</span><span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">b</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span>, <span class="op">-</span><span class="fl">1.0</span><span class="op">)</span>,</span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span> <span class="op">*</span> <span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">*</span> <span class="op">(</span><span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">b</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span>, <span class="op">-</span><span class="fl">1.0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Note that all of the functions above depend on additional parameters,
<code>a</code> and <code>b</code>. We have to supply specific values for
these when we invoke the optimization command. The constraint function
<code>eval_g0</code> returns a vector with in this case the same length
as the vectors <code>a</code> and <code>b</code>. The function
calculating the Jacobian of the constraint should return a matrix where
the number of rows equal the number of constraints (in this case two).
The number of columns should equal the number of control variables (two
in this case as well).</p>
<p>After defining values for the parameters</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># define parameters</span></span>
<span><span class="va">a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="op">-</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p>we can minimize the function subject to the constraints with the
following command:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Solve using NLOPT_LD_MMA with gradient information supplied in separate</span></span>
<span><span class="co"># function</span></span>
<span><span class="va">res0</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nloptr.html">nloptr</a></span><span class="op">(</span>x0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1.234</span>, <span class="fl">5.678</span><span class="op">)</span>,</span>
<span>               eval_f <span class="op">=</span> <span class="va">eval_f0</span>,</span>
<span>               eval_grad_f <span class="op">=</span> <span class="va">eval_grad_f0</span>,</span>
<span>               lb <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="cn">Inf</span>, <span class="fl">0</span><span class="op">)</span>,</span>
<span>               ub <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">Inf</span>, <span class="cn">Inf</span><span class="op">)</span>,</span>
<span>               eval_g_ineq <span class="op">=</span> <span class="va">eval_g0</span>,</span>
<span>               eval_jac_g_ineq <span class="op">=</span> <span class="va">eval_jac_g0</span>,</span>
<span>               opts <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"algorithm"</span> <span class="op">=</span> <span class="st">"NLOPT_LD_MMA"</span>,</span>
<span>                           <span class="st">"xtol_rel"</span> <span class="op">=</span> <span class="fl">1.0e-8</span>,</span>
<span>                           <span class="st">"print_level"</span> <span class="op">=</span> <span class="fl">2</span>,</span>
<span>                           <span class="st">"check_derivatives"</span> <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                           <span class="st">"check_derivatives_print"</span> <span class="op">=</span> <span class="st">"all"</span><span class="op">)</span>,</span>
<span>               a <span class="op">=</span> <span class="va">a</span>,</span>
<span>               b <span class="op">=</span> <span class="va">b</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Checking gradients of objective function.</span></span></code></pre>
<pre><code><span><span class="co">## Derivative checker results: 0 error(s) detected.</span></span></code></pre>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##   eval_grad_f[1] = 0.000000e+00 ~ 0.000000e+00   [0.000000e+00]</span></span>
<span><span class="co">##   eval_grad_f[2] = 2.098323e-01 ~ 2.098323e-01   [1.422937e-09]</span></span></code></pre>
<pre><code><span><span class="co">## Checking gradients of inequality constraints.</span></span></code></pre>
<pre><code><span><span class="co">## Derivative checker results: 0 error(s) detected.</span></span></code></pre>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##   eval_jac_g_ineq[1, 1] =  3.654614e+01 ~  3.654614e+01   [1.667794e-08]</span></span>
<span><span class="co">##   eval_jac_g_ineq[2, 1] = -1.642680e-01 ~ -1.642680e-01   [2.103453e-07]</span></span>
<span><span class="co">##   eval_jac_g_ineq[1, 2] = -1.000000e+00 ~ -1.000000e+00   [0.000000e+00]</span></span>
<span><span class="co">##   eval_jac_g_ineq[2, 2] = -1.000000e+00 ~ -1.000000e+00   [0.000000e+00]</span></span></code></pre>
<pre><code><span><span class="co">## iteration: 1</span></span>
<span><span class="co">##  f(x) = 2.382855</span></span>
<span><span class="co">##  g(x) = (9.354647, -5.690813)</span></span>
<span><span class="co">## iteration: 2</span></span>
<span><span class="co">##  f(x) = 2.356135</span></span>
<span><span class="co">##  g(x) = (-0.122988, -5.549587)</span></span>
<span><span class="co">## iteration: 3</span></span>
<span><span class="co">##  f(x) = 2.245864</span></span>
<span><span class="co">##  g(x) = (-0.531886, -5.038655)</span></span>
<span><span class="co">## iteration: 4</span></span>
<span><span class="co">##  f(x) = 2.019102</span></span>
<span><span class="co">##  g(x) = (-3.225103, -3.931195)</span></span>
<span><span class="co">## iteration: 5</span></span>
<span><span class="co">##  f(x) = 1.740934</span></span>
<span><span class="co">##  g(x) = (-2.676263, -2.761136)</span></span>
<span><span class="co">## iteration: 6</span></span>
<span><span class="co">##  f(x) = 1.404206</span></span>
<span><span class="co">##  g(x) = (-1.674055, -1.676216)</span></span>
<span><span class="co">## iteration: 7</span></span>
<span><span class="co">##  f(x) = 1.022295</span></span>
<span><span class="co">##  g(x) = (-0.748790, -0.748792)</span></span>
<span><span class="co">## iteration: 8</span></span>
<span><span class="co">##  f(x) = 0.685203</span></span>
<span><span class="co">##  g(x) = (-0.173206, -0.173207)</span></span>
<span><span class="co">## iteration: 9</span></span>
<span><span class="co">##  f(x) = 0.552985</span></span>
<span><span class="co">##  g(x) = (-0.009496, -0.009496)</span></span>
<span><span class="co">## iteration: 10</span></span>
<span><span class="co">##  f(x) = 0.544354</span></span>
<span><span class="co">##  g(x) = (-0.000025, -0.000025)</span></span>
<span><span class="co">## iteration: 11</span></span>
<span><span class="co">##  f(x) = 0.544331</span></span>
<span><span class="co">##  g(x) = (0.000000, 0.000000)</span></span>
<span><span class="co">## iteration: 12</span></span>
<span><span class="co">##  f(x) = 0.544331</span></span>
<span><span class="co">##  g(x) = (-0.000000, 0.000000)</span></span>
<span><span class="co">## iteration: 13</span></span>
<span><span class="co">##  f(x) = 0.544331</span></span>
<span><span class="co">##  g(x) = (-0.000000, 0.000000)</span></span>
<span><span class="co">## iteration: 14</span></span>
<span><span class="co">##  f(x) = 0.544331</span></span>
<span><span class="co">##  g(x) = (-0.000000, 0.000000)</span></span>
<span><span class="co">## iteration: 15</span></span>
<span><span class="co">##  f(x) = 0.544331</span></span>
<span><span class="co">##  g(x) = (-0.000000, 0.000000)</span></span>
<span><span class="co">## iteration: 16</span></span>
<span><span class="co">##  f(x) = 0.544331</span></span>
<span><span class="co">##  g(x) = (-0.000000, 0.000000)</span></span>
<span><span class="co">## iteration: 17</span></span>
<span><span class="co">##  f(x) = 0.544331</span></span>
<span><span class="co">##  g(x) = (-0.000000, 0.000000)</span></span>
<span><span class="co">## iteration: 18</span></span>
<span><span class="co">##  f(x) = 0.544331</span></span>
<span><span class="co">##  g(x) = (-0.000000, 0.000000)</span></span>
<span><span class="co">## iteration: 19</span></span>
<span><span class="co">##  f(x) = 0.544331</span></span>
<span><span class="co">##  g(x) = (0.000000, 0.000000)</span></span>
<span><span class="co">## iteration: 20</span></span>
<span><span class="co">##  f(x) = 0.544331</span></span>
<span><span class="co">##  g(x) = (-0.000000, -0.000000)</span></span>
<span><span class="co">## iteration: 21</span></span>
<span><span class="co">##  f(x) = 0.544331</span></span>
<span><span class="co">##  g(x) = (0.000000, 0.000000)</span></span></code></pre>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">res0</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## nloptr(x0 = c(1.234, 5.678), eval_f = eval_f0, eval_grad_f = eval_grad_f0, </span></span>
<span><span class="co">##     lb = c(-Inf, 0), ub = c(Inf, Inf), eval_g_ineq = eval_g0, </span></span>
<span><span class="co">##     eval_jac_g_ineq = eval_jac_g0, opts = list(algorithm = "NLOPT_LD_MMA", </span></span>
<span><span class="co">##         xtol_rel = 1e-08, print_level = 2, check_derivatives = TRUE, </span></span>
<span><span class="co">##         check_derivatives_print = "all"), a = a, b = b)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Minimization using NLopt version 2.7.1 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization </span></span>
<span><span class="co">## stopped because xtol_rel or xtol_abs (above) was reached. </span></span>
<span><span class="co">## )</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Number of Iterations....: 21 </span></span>
<span><span class="co">## Termination conditions:  xtol_rel: 1e-08 </span></span>
<span><span class="co">## Number of inequality constraints:  2 </span></span>
<span><span class="co">## Number of equality constraints:    0 </span></span>
<span><span class="co">## Optimal value of objective function:  0.54433104762009 </span></span>
<span><span class="co">## Optimal value of controls: 0.3333333 0.2962963</span></span></code></pre>
<p>Here we supplied lower bounds for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>2</mn></msub><annotation encoding="application/x-tex">x_2</annotation></semantics></math>
in <code>lb</code>. There are no upper bounds for both control
variables, so we supply <code>Inf</code> values. If we don’t supply
lower or upper bounds, plus or minus infinity is chosen by default. The
inequality constraints and its Jacobian are defined using
<code>eval_g_ineq</code> and <code>eval_jac_g_ineq</code>. Not all
algorithms can handle inequality constraints, so we have to specify one
that does, <code>NLOPT_LD_MMA</code> <span class="citation">(Svanberg
2002)</span>.</p>
<p>We also specify the option <code>print_level</code> to obtain output
during the optimization process. For the available
<code>print_level</code> values, see <code><a href="../reference/nloptr.html">?nloptr</a></code>. Setting the
<code>check_derivatives</code> option to <code>TRUE</code>, compares the
gradients supplied by the user with a finite difference approximation in
the initial point (<code>x0</code>). When this check is run, the option
<code>check_derivatives_print</code> can be used to print all values of
the derivative checker (<code>all</code> (default)), only those values
that result in an error (<code>errors</code>) or no output
(<code>none</code>), in which case only the number of errors is shown.
The tolerance that determines if a difference between the analytic
gradient and the finite difference approximation results in an error can
be set using the option <code>check_derivatives_tol</code> (default =
1e-04). The first column shows the value of the analytic gradient, the
second column shows the value of the finite difference approximation,
and the third column shows the relative error. Stars are added at the
front of a line if the relative error is larger than the specified
tolerance.</p>
<p>Finally, we add all the parameters that have to be passed on to the
objective and constraint functions, <code>a</code> and
<code>b</code>.</p>
<p>We can also use a different algorithm to solve the same minimization
problem. The only thing we have to change is the algorithm that we want
to use, in this case <code>NLOPT_LN_COBYLA</code>, which is an algorithm
that doesn’t need gradient information <span class="citation">(Powell
1994, 1998)</span>.</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Solve using NLOPT_LN_COBYLA without gradient information</span></span>
<span><span class="va">res1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nloptr.html">nloptr</a></span><span class="op">(</span>x0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1.234</span>, <span class="fl">5.678</span><span class="op">)</span>,</span>
<span>               eval_f <span class="op">=</span> <span class="va">eval_f0</span>,</span>
<span>               lb <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="cn">Inf</span>, <span class="fl">0</span><span class="op">)</span>,</span>
<span>               ub <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">Inf</span>, <span class="cn">Inf</span><span class="op">)</span>,</span>
<span>               eval_g_ineq <span class="op">=</span> <span class="va">eval_g0</span>,</span>
<span>               opts <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"algorithm"</span> <span class="op">=</span> <span class="st">"NLOPT_LN_COBYLA"</span>,</span>
<span>                           <span class="st">"xtol_rel"</span> <span class="op">=</span> <span class="fl">1.0e-8</span><span class="op">)</span>,</span>
<span>               a <span class="op">=</span> <span class="va">a</span>,</span>
<span>               b <span class="op">=</span> <span class="va">b</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">res1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## nloptr(x0 = c(1.234, 5.678), eval_f = eval_f0, lb = c(-Inf, 0), </span></span>
<span><span class="co">##     ub = c(Inf, Inf), eval_g_ineq = eval_g0, opts = list(algorithm = "NLOPT_LN_COBYLA", </span></span>
<span><span class="co">##         xtol_rel = 1e-08), a = a, b = b)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Minimization using NLopt version 2.7.1 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization </span></span>
<span><span class="co">## stopped because xtol_rel or xtol_abs (above) was reached. </span></span>
<span><span class="co">## )</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Number of Iterations....: 50 </span></span>
<span><span class="co">## Termination conditions:  xtol_rel: 1e-08 </span></span>
<span><span class="co">## Number of inequality constraints:  2 </span></span>
<span><span class="co">## Number of equality constraints:    0 </span></span>
<span><span class="co">## Optimal value of objective function:  0.544331053951819 </span></span>
<span><span class="co">## Optimal value of controls: 0.3333333 0.2962963</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="derivative-checker">Derivative checker<a class="anchor" aria-label="anchor" href="#derivative-checker"></a>
</h2>
<p>The derivative checker can be called when supplying a minimization
problem to <code>nloptr</code>, using the options
<code>check_derivatives</code>, <code>check_derivatives_tol</code> and
<code>check_derivatives_print</code>, but it can also be used
separately. For example, define the function <code>g</code>, with vector
outcome, and its gradient <code>g_grad</code>:</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">g</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">a</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,</span>
<span>    <span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>,</span>
<span>    <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span>,</span>
<span>    <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span>,</span>
<span>    <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">^</span> <span class="fl">3</span>,</span>
<span>    <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">^</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">g_grad</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">a</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>, <span class="fl">0</span><span class="op">)</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>, <span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span>, <span class="fl">0</span><span class="op">)</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">3</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p><code>a</code> is some vector containing data. The gradient contains
some errors in this case. By calling the function
<code>check.derivatives</code> we can check the user-supplied analytic
gradients with a finite difference approximation at a point
<code>.x</code>.</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/check.derivatives.html">check.derivatives</a></span><span class="op">(</span></span>
<span>  .x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span>,</span>
<span>  func <span class="op">=</span> <span class="va">g</span>,</span>
<span>  func_grad <span class="op">=</span> <span class="va">g_grad</span>,</span>
<span>  check_derivatives_print <span class="op">=</span> <span class="st">"all"</span>,</span>
<span>  a <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">0.8</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Derivative checker results: 2 error(s) detected.</span></span></code></pre>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##   grad_f[1, 1] = 1.00e+00 ~ 1.00e+00   [0.000000e+00]</span></span>
<span><span class="co">##   grad_f[2, 1] = 0.00e+00 ~ 0.00e+00   [0.000000e+00]</span></span>
<span><span class="co">##   grad_f[3, 1] = 1.40e+00 ~ 1.40e+00   [9.579318e-09]</span></span>
<span><span class="co">## * grad_f[4, 1] = 1.40e+00 ~ 0.00e+00   [1.400000e+00]</span></span>
<span><span class="co">## * grad_f[5, 1] = 1.20e-01 ~ 1.47e+00   [9.183673e-01]</span></span>
<span><span class="co">##   grad_f[6, 1] = 0.00e+00 ~ 0.00e+00   [0.000000e+00]</span></span>
<span><span class="co">##   grad_f[1, 2] = 0.00e+00 ~ 0.00e+00   [0.000000e+00]</span></span>
<span><span class="co">##   grad_f[2, 2] = 1.00e+00 ~ 1.00e+00   [0.000000e+00]</span></span>
<span><span class="co">##   grad_f[3, 2] = 0.00e+00 ~ 0.00e+00   [0.000000e+00]</span></span>
<span><span class="co">##   grad_f[4, 2] = 2.40e+00 ~ 2.40e+00   [1.179675e-08]</span></span>
<span><span class="co">##   grad_f[5, 2] = 0.00e+00 ~ 0.00e+00   [0.000000e+00]</span></span>
<span><span class="co">##   grad_f[6, 2] = 4.32e+00 ~ 4.32e+00   [2.593906e-08]</span></span></code></pre>
<p>The errors are shown on screen, where the option
<code>check_derivatives_print</code> determines the amount of output you
see. The value of the analytic gradient and the value of the finite
difference approximation at the supplied point is returned in a
list.</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">res</span></span></code></pre></div>
<pre><code><span><span class="co">## $analytic</span></span>
<span><span class="co">##      [,1] [,2]</span></span>
<span><span class="co">## [1,] 1.00 0.00</span></span>
<span><span class="co">## [2,] 0.00 1.00</span></span>
<span><span class="co">## [3,] 1.40 0.00</span></span>
<span><span class="co">## [4,] 1.40 2.40</span></span>
<span><span class="co">## [5,] 0.12 0.00</span></span>
<span><span class="co">## [6,] 0.00 4.32</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $finite_difference</span></span>
<span><span class="co">##      [,1] [,2]</span></span>
<span><span class="co">## [1,] 1.00 0.00</span></span>
<span><span class="co">## [2,] 0.00 1.00</span></span>
<span><span class="co">## [3,] 1.40 0.00</span></span>
<span><span class="co">## [4,] 0.00 2.40</span></span>
<span><span class="co">## [5,] 1.47 0.00</span></span>
<span><span class="co">## [6,] 0.00 4.32</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $relative_error</span></span>
<span><span class="co">##              [,1]         [,2]</span></span>
<span><span class="co">## [1,] 0.000000e+00 0.000000e+00</span></span>
<span><span class="co">## [2,] 0.000000e+00 0.000000e+00</span></span>
<span><span class="co">## [3,] 9.579318e-09 0.000000e+00</span></span>
<span><span class="co">## [4,] 1.400000e+00 1.179675e-08</span></span>
<span><span class="co">## [5,] 9.183673e-01 0.000000e+00</span></span>
<span><span class="co">## [6,] 0.000000e+00 2.593906e-08</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $flag_derivative_warning</span></span>
<span><span class="co">##       [,1]  [,2]</span></span>
<span><span class="co">## [1,] FALSE FALSE</span></span>
<span><span class="co">## [2,] FALSE FALSE</span></span>
<span><span class="co">## [3,] FALSE FALSE</span></span>
<span><span class="co">## [4,]  TRUE FALSE</span></span>
<span><span class="co">## [5,]  TRUE FALSE</span></span>
<span><span class="co">## [6,] FALSE FALSE</span></span></code></pre>
<p>Note that not all errors will be picked up by the derivative checker.
For instance, if we run the check with <code>a = c(.5, .5)</code>, one
of the errors is not flagged as an error.</p>
</div>
<div class="section level2">
<h2 id="notes">Notes<a class="anchor" aria-label="anchor" href="#notes"></a>
</h2>
<p>The <code>.R</code> scripts in the <code>tests</code> directory
contain more examples. For instance, <code>hs071.R</code> and
<code>systemofeq.R</code> show how to solve problems with equality
constraints. See the <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#augmented-lagrangian-algorithm" class="external-link">NLopt
website</a> for more details. Please let us know if there are any of
features implemented in NLopt which should be implemented in
<code>nloptr</code>.</p>
<p>Sometimes the optimization procedure terminates with a message
<code>maxtime was reached</code> without evaluating the objective
function. Submitting the same problem again usually solves this
problem.</p>
</div>
<div class="section level2">
<h2 id="description-of-options">Description of options<a class="anchor" aria-label="anchor" href="#description-of-options"></a>
</h2>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">nloptr</span><span class="fu">::</span><span class="fu"><a href="../reference/nloptr.print.options.html">nloptr.print.options</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## algorithm</span></span>
<span><span class="co">##  possible values: NLOPT_GN_DIRECT, NLOPT_GN_DIRECT_L,</span></span>
<span><span class="co">##           NLOPT_GN_DIRECT_L_RAND, NLOPT_GN_DIRECT_NOSCAL,</span></span>
<span><span class="co">##           NLOPT_GN_DIRECT_L_NOSCAL,</span></span>
<span><span class="co">##           NLOPT_GN_DIRECT_L_RAND_NOSCAL,</span></span>
<span><span class="co">##           NLOPT_GN_ORIG_DIRECT, NLOPT_GN_ORIG_DIRECT_L,</span></span>
<span><span class="co">##           NLOPT_GD_STOGO, NLOPT_GD_STOGO_RAND,</span></span>
<span><span class="co">##           NLOPT_LD_SLSQP, NLOPT_LD_LBFGS, NLOPT_LN_PRAXIS,</span></span>
<span><span class="co">##           NLOPT_LD_VAR1, NLOPT_LD_VAR2, NLOPT_LD_TNEWTON,</span></span>
<span><span class="co">##           NLOPT_LD_TNEWTON_RESTART,</span></span>
<span><span class="co">##           NLOPT_LD_TNEWTON_PRECOND,</span></span>
<span><span class="co">##           NLOPT_LD_TNEWTON_PRECOND_RESTART,</span></span>
<span><span class="co">##           NLOPT_GN_CRS2_LM, NLOPT_GN_MLSL, NLOPT_GD_MLSL,</span></span>
<span><span class="co">##           NLOPT_GN_MLSL_LDS, NLOPT_GD_MLSL_LDS,</span></span>
<span><span class="co">##           NLOPT_LD_MMA, NLOPT_LD_CCSAQ, NLOPT_LN_COBYLA,</span></span>
<span><span class="co">##           NLOPT_LN_NEWUOA, NLOPT_LN_NEWUOA_BOUND,</span></span>
<span><span class="co">##           NLOPT_LN_NELDERMEAD, NLOPT_LN_SBPLX,</span></span>
<span><span class="co">##           NLOPT_LN_AUGLAG, NLOPT_LD_AUGLAG,</span></span>
<span><span class="co">##           NLOPT_LN_AUGLAG_EQ, NLOPT_LD_AUGLAG_EQ,</span></span>
<span><span class="co">##           NLOPT_LN_BOBYQA, NLOPT_GN_ISRES</span></span>
<span><span class="co">##  default value:   none</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  This option is required. Check the NLopt website for a description of</span></span>
<span><span class="co">##  the algorithms.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## stopval</span></span>
<span><span class="co">##  possible values: -Inf &lt;= stopval &lt;= Inf</span></span>
<span><span class="co">##  default value:   -Inf</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  Stop minimization when an objective value &lt;= stopval is found.</span></span>
<span><span class="co">##  Setting stopval to -Inf disables this stopping criterion (default).</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## ftol_rel</span></span>
<span><span class="co">##  possible values: ftol_rel &gt; 0</span></span>
<span><span class="co">##  default value:   0.0</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  Stop when an optimization step (or an estimate of the optimum)</span></span>
<span><span class="co">##  changes the objective function value by less than ftol_rel multiplied</span></span>
<span><span class="co">##  by the absolute value of the function value. If there is any chance</span></span>
<span><span class="co">##  that your optimum function value is close to zero, you might want to</span></span>
<span><span class="co">##  set an absolute tolerance with ftol_abs as well. Criterion is</span></span>
<span><span class="co">##  disabled if ftol_rel is non-positive (default).</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## ftol_abs</span></span>
<span><span class="co">##  possible values: ftol_abs &gt; 0</span></span>
<span><span class="co">##  default value:   0.0</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  Stop when an optimization step (or an estimate of the optimum)</span></span>
<span><span class="co">##  changes the function value by less than ftol_abs. Criterion is</span></span>
<span><span class="co">##  disabled if ftol_abs is non-positive (default).</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## xtol_rel</span></span>
<span><span class="co">##  possible values: xtol_rel &gt; 0</span></span>
<span><span class="co">##  default value:   1.0e-04</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  Stop when an optimization step (or an estimate of the optimum)</span></span>
<span><span class="co">##  changes every parameter by less than xtol_rel multiplied by the</span></span>
<span><span class="co">##  absolute value of the parameter. If there is any chance that an</span></span>
<span><span class="co">##  optimal parameter is close to zero, you might want to set an absolute</span></span>
<span><span class="co">##  tolerance with xtol_abs as well. Criterion is disabled if xtol_rel is</span></span>
<span><span class="co">##  non-positive.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## xtol_abs</span></span>
<span><span class="co">##  possible values: xtol_abs &gt; 0</span></span>
<span><span class="co">##  default value:   rep(0.0, length(x0))</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  xtol_abs is a vector of length n (the number of elements in x) giving</span></span>
<span><span class="co">##  the tolerances: stop when an optimization step (or an estimate of the</span></span>
<span><span class="co">##  optimum) changes every parameter x[i] by less than xtol_abs[i].</span></span>
<span><span class="co">##  Criterion is disabled if all elements of xtol_abs are non-positive</span></span>
<span><span class="co">##  (default).</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## maxeval</span></span>
<span><span class="co">##  possible values: maxeval is a positive integer</span></span>
<span><span class="co">##  default value:   100</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  Stop when the number of function evaluations exceeds maxeval. This is</span></span>
<span><span class="co">##  not a strict maximum: the number of function evaluations may exceed</span></span>
<span><span class="co">##  maxeval slightly, depending upon the algorithm. Criterion is disabled</span></span>
<span><span class="co">##  if maxeval is non-positive.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## maxtime</span></span>
<span><span class="co">##  possible values: maxtime &gt; 0</span></span>
<span><span class="co">##  default value:   -1.0</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  Stop when the optimization time (in seconds) exceeds maxtime. This is</span></span>
<span><span class="co">##  not a strict maximum: the time may exceed maxtime slightly, depending</span></span>
<span><span class="co">##  upon the algorithm and on how slow your function evaluation is.</span></span>
<span><span class="co">##  Criterion is disabled if maxtime is non-positive (default).</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## tol_constraints_ineq</span></span>
<span><span class="co">##  possible values: tol_constraints_ineq &gt; 0.0</span></span>
<span><span class="co">##  default value:   rep(1e-8, num_constraints_ineq)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  The parameter tol_constraints_ineq is a vector of tolerances. Each</span></span>
<span><span class="co">##  tolerance corresponds to one of the inequality constraints. The</span></span>
<span><span class="co">##  tolerance is used for the purpose of stopping criteria only: a point</span></span>
<span><span class="co">##  x is considered feasible for judging whether to stop the optimization</span></span>
<span><span class="co">##  if eval_g_ineq(x) &lt;= tol. A tolerance of zero means that NLopt will</span></span>
<span><span class="co">##  try not to consider any x to be converged unless eval_g_ineq(x) is</span></span>
<span><span class="co">##  strictly non-positive; generally, at least a small positive tolerance</span></span>
<span><span class="co">##  is advisable to reduce sensitivity to rounding errors. By default the</span></span>
<span><span class="co">##  tolerances for all inequality constraints are set to 1e-8.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## tol_constraints_eq</span></span>
<span><span class="co">##  possible values: tol_constraints_eq &gt; 0.0</span></span>
<span><span class="co">##  default value:   rep(1e-8, num_constraints_eq)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  The parameter tol_constraints_eq is a vector of tolerances. Each</span></span>
<span><span class="co">##  tolerance corresponds to one of the equality constraints. The</span></span>
<span><span class="co">##  tolerance is used for the purpose of stopping criteria only: a point</span></span>
<span><span class="co">##  x is considered feasible for judging whether to stop the optimization</span></span>
<span><span class="co">##  if abs(eval_g_ineq(x)) &lt;= tol. For equality constraints, a small</span></span>
<span><span class="co">##  positive tolerance is strongly advised in order to allow NLopt to</span></span>
<span><span class="co">##  converge even if the equality constraint is slightly nonzero. By</span></span>
<span><span class="co">##  default the tolerances for all quality constraints are set to 1e-8.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## print_level</span></span>
<span><span class="co">##  possible values: 0, 1, 2, or 3</span></span>
<span><span class="co">##  default value:   0</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  The option print_level controls how much output is shown during the</span></span>
<span><span class="co">##  optimization process. Possible values: 0 (default): no output; 1:</span></span>
<span><span class="co">##  show iteration number and value of objective function; 2: 1 + show</span></span>
<span><span class="co">##  value of (in)equalities; 3: 2 + show value of controls.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## check_derivatives</span></span>
<span><span class="co">##  possible values: TRUE or FALSE</span></span>
<span><span class="co">##  default value:   FALSE</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  The option check_derivatives can be activated to compare the</span></span>
<span><span class="co">##  user-supplied analytic gradients with finite difference</span></span>
<span><span class="co">##  approximations.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## check_derivatives_tol</span></span>
<span><span class="co">##  possible values: check_derivatives_tol &gt; 0.0</span></span>
<span><span class="co">##  default value:   1e-04</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  The option check_derivatives_tol determines when a difference between</span></span>
<span><span class="co">##  an analytic gradient and its finite difference approximation is</span></span>
<span><span class="co">##  flagged as an error.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## check_derivatives_print</span></span>
<span><span class="co">##  possible values: 'none', 'all', 'errors',</span></span>
<span><span class="co">##  default value:   all</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  The option check_derivatives_print controls the output of the</span></span>
<span><span class="co">##  derivative checker (if check_derivatives == TRUE). All comparisons</span></span>
<span><span class="co">##  are shown ('all'), only those comparisions that resulted in an error</span></span>
<span><span class="co">##  ('error'), or only the number of errors is shown ('none').</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## print_options_doc</span></span>
<span><span class="co">##  possible values: TRUE or FALSE</span></span>
<span><span class="co">##  default value:   FALSE</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  If TRUE, a description of all options and their current and default</span></span>
<span><span class="co">##  values is printed to the screen.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## population</span></span>
<span><span class="co">##  possible values: population is a positive integer</span></span>
<span><span class="co">##  default value:   0</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  Several of the stochastic search algorithms (e.g., CRS, MLSL, and</span></span>
<span><span class="co">##  ISRES) start by generating some initial population of random points</span></span>
<span><span class="co">##  x. By default, this initial population size is chosen heuristically</span></span>
<span><span class="co">##  in some algorithm-specific way, but the initial population can by</span></span>
<span><span class="co">##  changed by setting a positive integer value for population. A</span></span>
<span><span class="co">##  population of zero implies that the heuristic default will be used.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## vector_storage</span></span>
<span><span class="co">##  possible values: vector_storage is a positive integer</span></span>
<span><span class="co">##  default value:   20</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  Number of gradients to remember from previous optimization steps.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## ranseed</span></span>
<span><span class="co">##  possible values: ranseed is a positive integer</span></span>
<span><span class="co">##  default value:   0</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  For stochastic optimization algorithms, pseudorandom numbers are</span></span>
<span><span class="co">##  generated. Set the random seed using ranseed if you want to use a</span></span>
<span><span class="co">##  'deterministic' sequence of pseudorandom numbers, i.e. the same</span></span>
<span><span class="co">##  sequence from run to run. If ranseed is 0 (default), the seed for the</span></span>
<span><span class="co">##  random numbers is generated from the system time, so that you will</span></span>
<span><span class="co">##  get a different sequence of pseudorandom numbers each time you run</span></span>
<span><span class="co">##  your program.</span></span></code></pre>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-NLopt:website" class="csl-entry">
Johnson, Steven G. n.d. <span>“The <span>N</span><span>L</span>opt
Nonlinear-Optimization Package.”</span> <a href="https://nlopt.readthedocs.io/en/latest/" class="external-link">https://nlopt.readthedocs.io/en/latest/</a>.
</div>
<div id="ref-LiuNocedal:1989" class="csl-entry">
Liu, D. C., and J. Nocedal. 1989. <span>“On the Limited Memory
<span>B</span><span>F</span><span>G</span><span>S</span> Method for
Large Scale Optimization.”</span> <em>Math. Programming</em> 45: 503–28.
</div>
<div id="ref-Nocedal:1980" class="csl-entry">
Nocedal, J. 1980. <span>“Updating Quasi-<span>N</span>ewton Matrices
with Limited Storage.”</span> <em>Math. Comput.</em> 35: 773–82.
</div>
<div id="ref-Powell:1994" class="csl-entry">
Powell, M. J. D. 1994. <span>“A Direct Search Optimization Method That
Models the Objective and Constraint Functions by Linear
Interpolation.”</span> In <em>Advances in Optimization and Numerical
Analysis</em>, edited by S. Gomez and J.-P. Hennart, 51–67. Kluwer
Academic, Dordrecht.
</div>
<div id="ref-Powell:1998" class="csl-entry">
———. 1998. <span>“Direct Search Algorithms for Optimization
Calculations.”</span> <em>Acta Numerica</em> 7: 287–336.
</div>
<div id="ref-Svanberg:2002" class="csl-entry">
Svanberg, Krister. 2002. <span>“A Class of Globally Convergent
Optimization Methods Based on Conservative Convex Separable
Approximations.”</span> <em>SIAM J. Optim.</em> 12 (2): 555–73.
</div>
<div id="ref-Xie:2014" class="csl-entry">
Xie, Yihui. 2014. <span>“Knitr: A Comprehensive Tool for Reproducible
Research in <span>R</span>.”</span> In <em>Implementing Reproducible
Computational Research</em>, edited by Victoria Stodden, Friedrich
Leisch, and Roger D. Peng. Chapman; Hall/CRC. <a href="https://www.routledge.com/Implementing-Reproducible-Research/Stodden-Leisch-Peng/p/book/9781466561595" class="external-link">https://www.routledge.com/Implementing-Reproducible-Research/Stodden-Leisch-Peng/p/book/9781466561595</a>.
</div>
<div id="ref-Xie:2015" class="csl-entry">
———. 2015. <em>Dynamic Documents with <span>R</span> and Knitr</em>. 2nd
ed. Boca Raton, Florida: Chapman; Hall/CRC. <a href="https://yihui.org/knitr/" class="external-link">https://yihui.org/knitr/</a>.
</div>
<div id="ref-Xie:2016" class="csl-entry">
———. 2016. <em>Knitr: A General-Purpose Package for Dynamic Report
Generation in r</em>. <a href="https://yihui.org/knitr/" class="external-link">https://yihui.org/knitr/</a>.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Jelmer Ypma, Steven G. Johnson, Aymeric Stamm.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
