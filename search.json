[{"path":"https://astamm.github.io/nloptr/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU Lesser General Public License","title":"GNU Lesser General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed. version GNU Lesser General Public License incorporates terms conditions version 3 GNU General Public License, supplemented additional permissions listed .","code":""},{"path":"https://astamm.github.io/nloptr/LICENSE.html","id":"id_0-additional-definitions","dir":"","previous_headings":"","what":"0. Additional Definitions","title":"GNU Lesser General Public License","text":"used herein, “License” refers version 3 GNU Lesser General Public License, “GNU GPL” refers version 3 GNU General Public License. “Library” refers covered work governed License, Application Combined Work defined . “Application” work makes use interface provided Library, otherwise based Library. Defining subclass class defined Library deemed mode using interface provided Library. “Combined Work” work produced combining linking Application Library. particular version Library Combined Work made also called “Linked Version”. “Minimal Corresponding Source” Combined Work means Corresponding Source Combined Work, excluding source code portions Combined Work , considered isolation, based Application, Linked Version. “Corresponding Application Code” Combined Work means object code /source code Application, including data utility programs needed reproducing Combined Work Application, excluding System Libraries Combined Work.","code":""},{"path":"https://astamm.github.io/nloptr/LICENSE.html","id":"id_1-exception-to-section-3-of-the-gnu-gpl","dir":"","previous_headings":"","what":"1. Exception to Section 3 of the GNU GPL","title":"GNU Lesser General Public License","text":"may convey covered work sections 3 4 License without bound section 3 GNU GPL.","code":""},{"path":"https://astamm.github.io/nloptr/LICENSE.html","id":"id_2-conveying-modified-versions","dir":"","previous_headings":"","what":"2. Conveying Modified Versions","title":"GNU Lesser General Public License","text":"modify copy Library, , modifications, facility refers function data supplied Application uses facility (argument passed facility invoked), may convey copy modified version: ) License, provided make good faith effort ensure , event Application supply function data, facility still operates, performs whatever part purpose remains meaningful, b) GNU GPL, none additional permissions License applicable copy.","code":""},{"path":"https://astamm.github.io/nloptr/LICENSE.html","id":"id_3-object-code-incorporating-material-from-library-header-files","dir":"","previous_headings":"","what":"3. Object Code Incorporating Material from Library Header Files","title":"GNU Lesser General Public License","text":"object code form Application may incorporate material header file part Library. may convey object code terms choice, provided , incorporated material limited numerical parameters, data structure layouts accessors, small macros, inline functions templates (ten fewer lines length), following: ) Give prominent notice copy object code Library used Library use covered License. b) Accompany object code copy GNU GPL license document.","code":""},{"path":"https://astamm.github.io/nloptr/LICENSE.html","id":"id_4-combined-works","dir":"","previous_headings":"","what":"4. Combined Works","title":"GNU Lesser General Public License","text":"may convey Combined Work terms choice , taken together, effectively restrict modification portions Library contained Combined Work reverse engineering debugging modifications, also following: ) Give prominent notice copy Combined Work Library used Library use covered License. b) Accompany Combined Work copy GNU GPL license document. c) Combined Work displays copyright notices execution, include copyright notice Library among notices, well reference directing user copies GNU GPL license document. d) one following: 0) Convey Minimal Corresponding Source terms License, Corresponding Application Code form suitable , terms permit, user recombine relink Application modified version Linked Version produce modified Combined Work, manner specified section 6 GNU GPL conveying Corresponding Source. 1) Use suitable shared library mechanism linking Library. suitable mechanism one () uses run time copy Library already present user’s computer system, (b) operate properly modified version Library interface-compatible Linked Version. e) Provide Installation Information, otherwise required provide information section 6 GNU GPL, extent information necessary install execute modified version Combined Work produced recombining relinking Application modified version Linked Version. (use option 4d0, Installation Information must accompany Minimal Corresponding Source Corresponding Application Code. use option 4d1, must provide Installation Information manner specified section 6 GNU GPL conveying Corresponding Source.)","code":""},{"path":"https://astamm.github.io/nloptr/LICENSE.html","id":"id_5-combined-libraries","dir":"","previous_headings":"","what":"5. Combined Libraries","title":"GNU Lesser General Public License","text":"may place library facilities work based Library side side single library together library facilities Applications covered License, convey combined library terms choice, following: ) Accompany combined library copy work based Library, uncombined library facilities, conveyed terms License. b) Give prominent notice combined library part work based Library, explaining find accompanying uncombined form work.","code":""},{"path":"https://astamm.github.io/nloptr/LICENSE.html","id":"id_6-revised-versions-of-the-gnu-lesser-general-public-license","dir":"","previous_headings":"","what":"6. Revised Versions of the GNU Lesser General Public License","title":"GNU Lesser General Public License","text":"Free Software Foundation may publish revised /new versions GNU Lesser General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Library received specifies certain numbered version GNU Lesser General Public License “later version” applies , option following terms conditions either published version later version published Free Software Foundation. Library received specify version number GNU Lesser General Public License, may choose version GNU Lesser General Public License ever published Free Software Foundation. Library received specifies proxy can decide whether future versions GNU Lesser General Public License shall apply, proxy’s public statement acceptance version permanent authorization choose version Library.","code":""},{"path":"https://astamm.github.io/nloptr/articles/nloptr.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"nloptr","text":"NLopt addresses general nonlinear optimization problems form: minx∈Rnf(x)s.t.g(x)≤0h(x)=0xL≤x≤xU \\begin{aligned} &\\min_{x \\R^n} f(x) \\\\ s.t.& g(x) \\leq 0 \\\\ & h(x) = 0 \\\\ & x_L \\leq x \\leq x_U \\end{aligned} f(⋅)f(\\cdot) objective function xx represents nn optimization parameters. problem may optionally subject bound constraints (also called box constraints), xLx_L xUx_U. partially totally unconstrained problems bounds can take values −∞-\\infty ∞\\infty. One may also optionally mm nonlinear inequality constraints—sometimes called nonlinear programming problem—may specified g(⋅)g(\\cdot), equality constraints may specified h(⋅)h(\\cdot). Note algorithms NLopt can handle constraints. vignette describes formulate minimization problems solved R interface NLopt. want use C interface directly interested Matlab interface, sources documentation available. information taken NLopt website, details available. credit implementing C code different algorithms available NLopt go respective authors. Also, please see website information cite NLopt algorithms use.","code":""},{"path":"https://astamm.github.io/nloptr/articles/nloptr.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"nloptr","text":"package CRAN can installed within R using install package source. now able load R interface NLopt read help. recent experimental source version nloptr can installed Github using remotes package:","code":"install.packages(\"nloptr\") install.packages(\"nloptr\", type = \"source\") library(\"nloptr\") ?nloptr # install.packages(\"remotes\") remotes::install_github(\"astamm/nloptr\")"},{"path":"https://astamm.github.io/nloptr/articles/nloptr.html","id":"minimizing-the-rosenbrock-banana-function","dir":"Articles","previous_headings":"","what":"Minimizing the Rosenbrock Banana function","title":"nloptr","text":"first example solve unconstrained minimization problem. function look Rosenbrock Banana function: f(x)=100(x2−x12)2+(1−x1)2, f(x) = 100 \\left(x_2-x_1^2\\right)^2 + \\left(1-x_1\\right)^2, also used example documentation standard R optimizer optim. gradient objective function given : $$ \\nabla f(x) = \\left(\\begin{array}[1]{c} -400 \\cdot x_1 \\cdot (x_2 - x_1^2) - 2 \\cdot (1 - x_1) \\\\ 200 \\cdot (x_2 - x_1^2) \\end{array} \\right). $$ algorithms NLopt need gradients supplied user. show examples without supplying gradient. loading library. start specifying objective function gradient: define initial values minimize function using nloptr command. command runs checks supplied inputs returns object exit code solver, optimal value objective function solution. can minimize function need specify algorithm want use use L-BFGS algorithm (Nocedal 1980; Liu Nocedal 1989). characters LD algorithm show algorithm looks local minima (L) using derivative-based (D) algorithm. algorithms look global (G) minima, don’t need derivatives (N). also specified termination criterium terms relative x-tolerance. termination criteria available (see Appendix \\ref{sec:descoptions} full list options). solve minimization problem using can see results printing resulting object. Sometimes objective function gradient contain common terms. economize calculations, can return objective gradient list. Rosenbrock Banana function instance: minimize using gives results .","code":"library(nloptr) ## Rosenbrock Banana function eval_f <- function(x) {   100 * (x[2] - x[1] * x[1]) ^ 2 + (1 - x[1]) ^ 2 }  ## Gradient of Rosenbrock Banana function eval_grad_f <- function(x) {   c(-400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),     200 * (x[2] - x[1] * x[1])) } # initial values x0 <- c(-1.2, 1) opts <- list(\"algorithm\" = \"NLOPT_LD_LBFGS\",              \"xtol_rel\" = 1.0e-8) # solve Rosenbrock Banana function res <- nloptr(x0 = x0,               eval_f = eval_f,               eval_grad_f = eval_grad_f,               opts = opts) print(res) ##  ## Call: ##  ## nloptr(x0 = x0, eval_f = eval_f, eval_grad_f = eval_grad_f, opts = opts) ##  ##  ##  ## Minimization using NLopt version 2.7.1  ##  ## NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success  ## return value. ) ##  ## Number of Iterations....: 56  ## Termination conditions:  xtol_rel: 1e-08  ## Number of inequality constraints:  0  ## Number of equality constraints:    0  ## Optimal value of objective function:  7.35727226897802e-23  ## Optimal value of controls: 1 1 ## Rosenbrock Banana function and gradient in one function eval_f_list <- function(x) {   common_term <- x[2] - x[1] * x[1]   list(\"objective\" = 100 * common_term ^ 2 + (1 - x[1]) ^ 2,        \"gradient\"  = c(-400 * x[1] * common_term - 2 * (1 - x[1]),                        200 * common_term)) } res <- nloptr(x0 = x0,               eval_f = eval_f_list,               opts = opts) print(res) ##  ## Call: ## nloptr(x0 = x0, eval_f = eval_f_list, opts = opts) ##  ##  ## Minimization using NLopt version 2.7.1  ##  ## NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success  ## return value. ) ##  ## Number of Iterations....: 56  ## Termination conditions:  xtol_rel: 1e-08  ## Number of inequality constraints:  0  ## Number of equality constraints:    0  ## Optimal value of objective function:  7.35727226897802e-23  ## Optimal value of controls: 1 1"},{"path":"https://astamm.github.io/nloptr/articles/nloptr.html","id":"minimization-with-inequality-constraints","dir":"Articles","previous_headings":"","what":"Minimization with inequality constraints","title":"nloptr","text":"section shows minimize function subject inequality constraints. example one used tutorial NLopt website. problem want solve : minx∈Rnx2s.t.x2≥0x2≥(a1x1+b1)3x2≥(a2x1+b2)3, \\begin{aligned} &\\min_{x \\R^n} \\sqrt{x_2} \\\\ s.t.& x_2 \\geq 0 \\\\ & x_2 \\geq (a_1 x_1 + b_1)^3 \\\\ & x_2 \\geq (a_2 x_1 + b_2)^3,  \\end{aligned} a1=2a_1 = 2, b1=0b_1 = 0, a2=−1a_2 = -1, b2=1b_2 = 1. order solve problem, first re-formulate constraints form g(x)≤0g(x) \\leq 0. Note first constraint bound x2x_2, add later. two constraints can re-written : (a1x1+b1)3−x2≤0(a2x1+b2)3−x2≤0 \\begin{aligned} (a_1 x_1 + b_1)^3 - x_2 &\\leq 0 \\\\ (a_2 x_1 + b_2)^3 - x_2 &\\leq 0 \\end{aligned} First, define R functions calculate objective function gradient: needed, can course calculated function . define two constraints Jacobian constraints: Note functions depend additional parameters, b. supply specific values invoke optimization command. constraint function eval_g0 returns vector case length vectors b. function calculating Jacobian constraint return matrix number rows equal number constraints (case two). number columns equal number control variables (two case well). defining values parameters can minimize function subject constraints following command: supplied lower bounds x2x_2 lb. upper bounds control variables, supply Inf values. don’t supply lower upper bounds, plus minus infinity chosen default. inequality constraints Jacobian defined using eval_g_ineq eval_jac_g_ineq. algorithms can handle inequality constraints, specify one , NLOPT_LD_MMA (Svanberg 2002). also specify option print_level obtain output optimization process. available print_level values, see ?nloptr. Setting check_derivatives option TRUE, compares gradients supplied user finite difference approximation initial point (x0). check run, option check_derivatives_print can used print values derivative checker ((default)), values result error (errors) output (none), case number errors shown. tolerance determines difference analytic gradient finite difference approximation results error can set using option check_derivatives_tol (default = 1e-04). first column shows value analytic gradient, second column shows value finite difference approximation, third column shows relative error. Stars added front line relative error larger specified tolerance. Finally, add parameters passed objective constraint functions, b. can also use different algorithm solve minimization problem. thing change algorithm want use, case NLOPT_LN_COBYLA, algorithm doesn’t need gradient information (Powell 1994, 1998).","code":"# objective function eval_f0 <- function(x, a, b) {   sqrt(x[2]) }  # gradient of objective function eval_grad_f0 <- function(x, a, b) {   c(0, 0.5 / sqrt(x[2])) } # constraint function eval_g0 <- function(x, a, b) {   (a * x[1] + b) ^ 3 - x[2] }  # Jacobian of constraint eval_jac_g0 <- function(x, a, b) {   rbind(c(3 * a[1] * (a[1] * x[1] + b[1]) ^ 2, -1.0),         c(3 * a[2] * (a[2] * x[1] + b[2]) ^ 2, -1.0)) } # define parameters a <- c(2, -1) b <- c(0, 1) # Solve using NLOPT_LD_MMA with gradient information supplied in separate # function res0 <- nloptr(x0 = c(1.234, 5.678),                eval_f = eval_f0,                eval_grad_f = eval_grad_f0,                lb = c(-Inf, 0),                ub = c(Inf, Inf),                eval_g_ineq = eval_g0,                eval_jac_g_ineq = eval_jac_g0,                opts = list(\"algorithm\" = \"NLOPT_LD_MMA\",                            \"xtol_rel\" = 1.0e-8,                            \"print_level\" = 2,                            \"check_derivatives\" = TRUE,                            \"check_derivatives_print\" = \"all\"),                a = a,                b = b) ## Checking gradients of objective function. ## Derivative checker results: 0 error(s) detected. ##  ##   eval_grad_f[1] = 0.000000e+00 ~ 0.000000e+00   [0.000000e+00] ##   eval_grad_f[2] = 2.098323e-01 ~ 2.098323e-01   [1.422937e-09] ## Checking gradients of inequality constraints. ## Derivative checker results: 0 error(s) detected. ##  ##   eval_jac_g_ineq[1, 1] =  3.654614e+01 ~  3.654614e+01   [1.667794e-08] ##   eval_jac_g_ineq[2, 1] = -1.642680e-01 ~ -1.642680e-01   [2.103453e-07] ##   eval_jac_g_ineq[1, 2] = -1.000000e+00 ~ -1.000000e+00   [0.000000e+00] ##   eval_jac_g_ineq[2, 2] = -1.000000e+00 ~ -1.000000e+00   [0.000000e+00] ## iteration: 1 ##  f(x) = 2.382855 ##  g(x) = (9.354647, -5.690813) ## iteration: 2 ##  f(x) = 2.356135 ##  g(x) = (-0.122988, -5.549587) ## iteration: 3 ##  f(x) = 2.245864 ##  g(x) = (-0.531886, -5.038655) ## iteration: 4 ##  f(x) = 2.019102 ##  g(x) = (-3.225103, -3.931195) ## iteration: 5 ##  f(x) = 1.740934 ##  g(x) = (-2.676263, -2.761136) ## iteration: 6 ##  f(x) = 1.404206 ##  g(x) = (-1.674055, -1.676216) ## iteration: 7 ##  f(x) = 1.022295 ##  g(x) = (-0.748790, -0.748792) ## iteration: 8 ##  f(x) = 0.685203 ##  g(x) = (-0.173206, -0.173207) ## iteration: 9 ##  f(x) = 0.552985 ##  g(x) = (-0.009496, -0.009496) ## iteration: 10 ##  f(x) = 0.544354 ##  g(x) = (-0.000025, -0.000025) ## iteration: 11 ##  f(x) = 0.544331 ##  g(x) = (0.000000, 0.000000) ## iteration: 12 ##  f(x) = 0.544331 ##  g(x) = (-0.000000, 0.000000) ## iteration: 13 ##  f(x) = 0.544331 ##  g(x) = (-0.000000, 0.000000) ## iteration: 14 ##  f(x) = 0.544331 ##  g(x) = (-0.000000, 0.000000) ## iteration: 15 ##  f(x) = 0.544331 ##  g(x) = (-0.000000, 0.000000) ## iteration: 16 ##  f(x) = 0.544331 ##  g(x) = (-0.000000, 0.000000) ## iteration: 17 ##  f(x) = 0.544331 ##  g(x) = (-0.000000, 0.000000) ## iteration: 18 ##  f(x) = 0.544331 ##  g(x) = (-0.000000, 0.000000) ## iteration: 19 ##  f(x) = 0.544331 ##  g(x) = (0.000000, 0.000000) ## iteration: 20 ##  f(x) = 0.544331 ##  g(x) = (-0.000000, -0.000000) ## iteration: 21 ##  f(x) = 0.544331 ##  g(x) = (0.000000, 0.000000) print(res0) ##  ## Call: ##  ## nloptr(x0 = c(1.234, 5.678), eval_f = eval_f0, eval_grad_f = eval_grad_f0,  ##     lb = c(-Inf, 0), ub = c(Inf, Inf), eval_g_ineq = eval_g0,  ##     eval_jac_g_ineq = eval_jac_g0, opts = list(algorithm = \"NLOPT_LD_MMA\",  ##         xtol_rel = 1e-08, print_level = 2, check_derivatives = TRUE,  ##         check_derivatives_print = \"all\"), a = a, b = b) ##  ##  ## Minimization using NLopt version 2.7.1  ##  ## NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization  ## stopped because xtol_rel or xtol_abs (above) was reached.  ## ) ##  ## Number of Iterations....: 21  ## Termination conditions:  xtol_rel: 1e-08  ## Number of inequality constraints:  2  ## Number of equality constraints:    0  ## Optimal value of objective function:  0.54433104762009  ## Optimal value of controls: 0.3333333 0.2962963 # Solve using NLOPT_LN_COBYLA without gradient information res1 <- nloptr(x0 = c(1.234, 5.678),                eval_f = eval_f0,                lb = c(-Inf, 0),                ub = c(Inf, Inf),                eval_g_ineq = eval_g0,                opts = list(\"algorithm\" = \"NLOPT_LN_COBYLA\",                            \"xtol_rel\" = 1.0e-8),                a = a,                b = b) print(res1) ##  ## Call: ##  ## nloptr(x0 = c(1.234, 5.678), eval_f = eval_f0, lb = c(-Inf, 0),  ##     ub = c(Inf, Inf), eval_g_ineq = eval_g0, opts = list(algorithm = \"NLOPT_LN_COBYLA\",  ##         xtol_rel = 1e-08), a = a, b = b) ##  ##  ## Minimization using NLopt version 2.7.1  ##  ## NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization  ## stopped because xtol_rel or xtol_abs (above) was reached.  ## ) ##  ## Number of Iterations....: 50  ## Termination conditions:  xtol_rel: 1e-08  ## Number of inequality constraints:  2  ## Number of equality constraints:    0  ## Optimal value of objective function:  0.544331053951819  ## Optimal value of controls: 0.3333333 0.2962963"},{"path":"https://astamm.github.io/nloptr/articles/nloptr.html","id":"derivative-checker","dir":"Articles","previous_headings":"","what":"Derivative checker","title":"nloptr","text":"derivative checker can called supplying minimization problem nloptr, using options check_derivatives, check_derivatives_tol check_derivatives_print, can also used separately. example, define function g, vector outcome, gradient g_grad: vector containing data. gradient contains errors case. calling function check.derivatives can check user-supplied analytic gradients finite difference approximation point .x. errors shown screen, option check_derivatives_print determines amount output see. value analytic gradient value finite difference approximation supplied point returned list. Note errors picked derivative checker. instance, run check = c(.5, .5), one errors flagged error.","code":"g <- function(x, a) {   c(x[1] - a[1],     x[2] - a[2],     (x[1] - a[1]) ^ 2,     (x[2] - a[2]) ^ 2,     (x[1] - a[1]) ^ 3,     (x[2] - a[2]) ^ 3) }  g_grad <- function(x, a) {   rbind(     c(1, 0),     c(0, 1),     c(2 * (x[1] - a[1]), 0),     c(2 * (x[1] - a[1]), 2 * (x[2] - a[2])),     c(3 * (x[1] - a[2]) ^ 2, 0),     c(0, 3 * (x[2] - a[2]) ^ 2)   ) } res <- check.derivatives(   .x = c(1, 2),   func = g,   func_grad = g_grad,   check_derivatives_print = \"all\",   a = c(0.3, 0.8) ) ## Derivative checker results: 2 error(s) detected. ##  ##   grad_f[1, 1] = 1.00e+00 ~ 1.00e+00   [0.000000e+00] ##   grad_f[2, 1] = 0.00e+00 ~ 0.00e+00   [0.000000e+00] ##   grad_f[3, 1] = 1.40e+00 ~ 1.40e+00   [9.579318e-09] ## * grad_f[4, 1] = 1.40e+00 ~ 0.00e+00   [1.400000e+00] ## * grad_f[5, 1] = 1.20e-01 ~ 1.47e+00   [9.183673e-01] ##   grad_f[6, 1] = 0.00e+00 ~ 0.00e+00   [0.000000e+00] ##   grad_f[1, 2] = 0.00e+00 ~ 0.00e+00   [0.000000e+00] ##   grad_f[2, 2] = 1.00e+00 ~ 1.00e+00   [0.000000e+00] ##   grad_f[3, 2] = 0.00e+00 ~ 0.00e+00   [0.000000e+00] ##   grad_f[4, 2] = 2.40e+00 ~ 2.40e+00   [1.179675e-08] ##   grad_f[5, 2] = 0.00e+00 ~ 0.00e+00   [0.000000e+00] ##   grad_f[6, 2] = 4.32e+00 ~ 4.32e+00   [2.593906e-08] res ## $analytic ##      [,1] [,2] ## [1,] 1.00 0.00 ## [2,] 0.00 1.00 ## [3,] 1.40 0.00 ## [4,] 1.40 2.40 ## [5,] 0.12 0.00 ## [6,] 0.00 4.32 ##  ## $finite_difference ##      [,1] [,2] ## [1,] 1.00 0.00 ## [2,] 0.00 1.00 ## [3,] 1.40 0.00 ## [4,] 0.00 2.40 ## [5,] 1.47 0.00 ## [6,] 0.00 4.32 ##  ## $relative_error ##              [,1]         [,2] ## [1,] 0.000000e+00 0.000000e+00 ## [2,] 0.000000e+00 0.000000e+00 ## [3,] 9.579318e-09 0.000000e+00 ## [4,] 1.400000e+00 1.179675e-08 ## [5,] 9.183673e-01 0.000000e+00 ## [6,] 0.000000e+00 2.593906e-08 ##  ## $flag_derivative_warning ##       [,1]  [,2] ## [1,] FALSE FALSE ## [2,] FALSE FALSE ## [3,] FALSE FALSE ## [4,]  TRUE FALSE ## [5,]  TRUE FALSE ## [6,] FALSE FALSE"},{"path":"https://astamm.github.io/nloptr/articles/nloptr.html","id":"notes","dir":"Articles","previous_headings":"","what":"Notes","title":"nloptr","text":".R scripts tests directory contain examples. instance, hs071.R systemofeq.R show solve problems equality constraints. See NLopt website details. Please let us know features implemented NLopt implemented nloptr. Sometimes optimization procedure terminates message maxtime reached without evaluating objective function. Submitting problem usually solves problem.","code":""},{"path":"https://astamm.github.io/nloptr/articles/nloptr.html","id":"description-of-options","dir":"Articles","previous_headings":"","what":"Description of options","title":"nloptr","text":"","code":"nloptr::nloptr.print.options() ## algorithm ##  possible values: NLOPT_GN_DIRECT, NLOPT_GN_DIRECT_L, ##           NLOPT_GN_DIRECT_L_RAND, NLOPT_GN_DIRECT_NOSCAL, ##           NLOPT_GN_DIRECT_L_NOSCAL, ##           NLOPT_GN_DIRECT_L_RAND_NOSCAL, ##           NLOPT_GN_ORIG_DIRECT, NLOPT_GN_ORIG_DIRECT_L, ##           NLOPT_GD_STOGO, NLOPT_GD_STOGO_RAND, ##           NLOPT_LD_SLSQP, NLOPT_LD_LBFGS, NLOPT_LN_PRAXIS, ##           NLOPT_LD_VAR1, NLOPT_LD_VAR2, NLOPT_LD_TNEWTON, ##           NLOPT_LD_TNEWTON_RESTART, ##           NLOPT_LD_TNEWTON_PRECOND, ##           NLOPT_LD_TNEWTON_PRECOND_RESTART, ##           NLOPT_GN_CRS2_LM, NLOPT_GN_MLSL, NLOPT_GD_MLSL, ##           NLOPT_GN_MLSL_LDS, NLOPT_GD_MLSL_LDS, ##           NLOPT_LD_MMA, NLOPT_LD_CCSAQ, NLOPT_LN_COBYLA, ##           NLOPT_LN_NEWUOA, NLOPT_LN_NEWUOA_BOUND, ##           NLOPT_LN_NELDERMEAD, NLOPT_LN_SBPLX, ##           NLOPT_LN_AUGLAG, NLOPT_LD_AUGLAG, ##           NLOPT_LN_AUGLAG_EQ, NLOPT_LD_AUGLAG_EQ, ##           NLOPT_LN_BOBYQA, NLOPT_GN_ISRES ##  default value:   none ##  ##  This option is required. Check the NLopt website for a description of ##  the algorithms. ##  ## stopval ##  possible values: -Inf <= stopval <= Inf ##  default value:   -Inf ##  ##  Stop minimization when an objective value <= stopval is found. ##  Setting stopval to -Inf disables this stopping criterion (default). ##  ## ftol_rel ##  possible values: ftol_rel > 0 ##  default value:   0.0 ##  ##  Stop when an optimization step (or an estimate of the optimum) ##  changes the objective function value by less than ftol_rel multiplied ##  by the absolute value of the function value. If there is any chance ##  that your optimum function value is close to zero, you might want to ##  set an absolute tolerance with ftol_abs as well. Criterion is ##  disabled if ftol_rel is non-positive (default). ##  ## ftol_abs ##  possible values: ftol_abs > 0 ##  default value:   0.0 ##  ##  Stop when an optimization step (or an estimate of the optimum) ##  changes the function value by less than ftol_abs. Criterion is ##  disabled if ftol_abs is non-positive (default). ##  ## xtol_rel ##  possible values: xtol_rel > 0 ##  default value:   1.0e-04 ##  ##  Stop when an optimization step (or an estimate of the optimum) ##  changes every parameter by less than xtol_rel multiplied by the ##  absolute value of the parameter. If there is any chance that an ##  optimal parameter is close to zero, you might want to set an absolute ##  tolerance with xtol_abs as well. Criterion is disabled if xtol_rel is ##  non-positive. ##  ## xtol_abs ##  possible values: xtol_abs > 0 ##  default value:   rep(0.0, length(x0)) ##  ##  xtol_abs is a vector of length n (the number of elements in x) giving ##  the tolerances: stop when an optimization step (or an estimate of the ##  optimum) changes every parameter x[i] by less than xtol_abs[i]. ##  Criterion is disabled if all elements of xtol_abs are non-positive ##  (default). ##  ## maxeval ##  possible values: maxeval is a positive integer ##  default value:   100 ##  ##  Stop when the number of function evaluations exceeds maxeval. This is ##  not a strict maximum: the number of function evaluations may exceed ##  maxeval slightly, depending upon the algorithm. Criterion is disabled ##  if maxeval is non-positive. ##  ## maxtime ##  possible values: maxtime > 0 ##  default value:   -1.0 ##  ##  Stop when the optimization time (in seconds) exceeds maxtime. This is ##  not a strict maximum: the time may exceed maxtime slightly, depending ##  upon the algorithm and on how slow your function evaluation is. ##  Criterion is disabled if maxtime is non-positive (default). ##  ## tol_constraints_ineq ##  possible values: tol_constraints_ineq > 0.0 ##  default value:   rep(1e-8, num_constraints_ineq) ##  ##  The parameter tol_constraints_ineq is a vector of tolerances. Each ##  tolerance corresponds to one of the inequality constraints. The ##  tolerance is used for the purpose of stopping criteria only: a point ##  x is considered feasible for judging whether to stop the optimization ##  if eval_g_ineq(x) <= tol. A tolerance of zero means that NLopt will ##  try not to consider any x to be converged unless eval_g_ineq(x) is ##  strictly non-positive; generally, at least a small positive tolerance ##  is advisable to reduce sensitivity to rounding errors. By default the ##  tolerances for all inequality constraints are set to 1e-8. ##  ## tol_constraints_eq ##  possible values: tol_constraints_eq > 0.0 ##  default value:   rep(1e-8, num_constraints_eq) ##  ##  The parameter tol_constraints_eq is a vector of tolerances. Each ##  tolerance corresponds to one of the equality constraints. The ##  tolerance is used for the purpose of stopping criteria only: a point ##  x is considered feasible for judging whether to stop the optimization ##  if abs(eval_g_ineq(x)) <= tol. For equality constraints, a small ##  positive tolerance is strongly advised in order to allow NLopt to ##  converge even if the equality constraint is slightly nonzero. By ##  default the tolerances for all quality constraints are set to 1e-8. ##  ## print_level ##  possible values: 0, 1, 2, or 3 ##  default value:   0 ##  ##  The option print_level controls how much output is shown during the ##  optimization process. Possible values: 0 (default): no output; 1: ##  show iteration number and value of objective function; 2: 1 + show ##  value of (in)equalities; 3: 2 + show value of controls. ##  ## check_derivatives ##  possible values: TRUE or FALSE ##  default value:   FALSE ##  ##  The option check_derivatives can be activated to compare the ##  user-supplied analytic gradients with finite difference ##  approximations. ##  ## check_derivatives_tol ##  possible values: check_derivatives_tol > 0.0 ##  default value:   1e-04 ##  ##  The option check_derivatives_tol determines when a difference between ##  an analytic gradient and its finite difference approximation is ##  flagged as an error. ##  ## check_derivatives_print ##  possible values: 'none', 'all', 'errors', ##  default value:   all ##  ##  The option check_derivatives_print controls the output of the ##  derivative checker (if check_derivatives == TRUE). All comparisons ##  are shown ('all'), only those comparisions that resulted in an error ##  ('error'), or only the number of errors is shown ('none'). ##  ## print_options_doc ##  possible values: TRUE or FALSE ##  default value:   FALSE ##  ##  If TRUE, a description of all options and their current and default ##  values is printed to the screen. ##  ## population ##  possible values: population is a positive integer ##  default value:   0 ##  ##  Several of the stochastic search algorithms (e.g., CRS, MLSL, and ##  ISRES) start by generating some initial population of random points ##  x. By default, this initial population size is chosen heuristically ##  in some algorithm-specific way, but the initial population can by ##  changed by setting a positive integer value for population. A ##  population of zero implies that the heuristic default will be used. ##  ## vector_storage ##  possible values: vector_storage is a positive integer ##  default value:   20 ##  ##  Number of gradients to remember from previous optimization steps. ##  ## ranseed ##  possible values: ranseed is a positive integer ##  default value:   0 ##  ##  For stochastic optimization algorithms, pseudorandom numbers are ##  generated. Set the random seed using ranseed if you want to use a ##  'deterministic' sequence of pseudorandom numbers, i.e. the same ##  sequence from run to run. If ranseed is 0 (default), the seed for the ##  random numbers is generated from the system time, so that you will ##  get a different sequence of pseudorandom numbers each time you run ##  your program."},{"path":[]},{"path":"https://astamm.github.io/nloptr/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jelmer Ypma. Author. Steven G. Johnson. Author.           author NLopt C library Aymeric Stamm. Contributor, maintainer. Hans W. Borchers. Contributor. Dirk Eddelbuettel. Contributor. Brian Ripley. Contributor.           build process multiple OS Kurt Hornik. Contributor.           build process multiple OS Julien Chiquet. Contributor. Avraham Adler. Contributor. Xiongtao Dai. Contributor. Jeroen Ooms. Contributor. Tomas Kalibera. Contributor. Mikael Jagan. Contributor.","code":""},{"path":"https://astamm.github.io/nloptr/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Johnson S (2008). NLopt nonlinear-optimization package. https://github.com/stevengj/nlopt.","code":"@Manual{,   author = {Steven G. Johnson},   title = {The NLopt nonlinear-optimization package},   year = {2008},   url = {https://github.com/stevengj/nlopt}, }"},{"path":"https://astamm.github.io/nloptr/index.html","id":"nloptr-","dir":"","previous_headings":"","what":"R Interface to NLopt","title":"R Interface to NLopt","text":"nloptr R interface NLopt, free/open-source library nonlinear optimization started Steven G. Johnson, providing common interface number different free optimization routines available online well original implementations various algorithms. can used solve general nonlinear programming problems nonlinear constraints lower upper bounds controls, minx∈ℝnf(x), \\min_{x \\\\mathbb{R}^n} \\quad f(x), s.t. g(x)≤0g(x) \\le 0, h(x)=0h(x) = 0 ℓ≤x≤u\\ell \\le x \\le u. NLopt library available GNU Lesser General Public License (LGPL), copyrights owned variety authors. See website information cite NLopt algorithms use.","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/index.html","id":"windows","dir":"","previous_headings":"Installation","what":"Windows","title":"R Interface to NLopt","text":"Windows, old versions R (R <= 4.1.x), nlopt v2.7.1 rwinlib used. newer versions R (R >= 4.2.0), nlopt version corresponding RTools toolchain used.","code":""},{"path":"https://astamm.github.io/nloptr/index.html","id":"linux-and-macos","dir":"","previous_headings":"Installation","what":"Linux and macOS","title":"R Interface to NLopt","text":"Unix-like platforms, use pkg-config find suitable system build NLopt (.e. version >= 2.7.0). found used. Otherwise, NLopt 2.7.1 built included sources using CMake. case, binary CMake stored environment variable CMAKE_BIN searched PATH , alternatively, macOS-specific location. variable set, install abort suggesting ways installing CMake. minimal version requirement cmake >= 3.2.0.","code":""},{"path":"https://astamm.github.io/nloptr/index.html","id":"installing-cmake-macos-and-linux-only","dir":"","previous_headings":"Installation","what":"Installing CMake (macOS and Linux only)","title":"R Interface to NLopt","text":"Minimal version requirement cmake 3.2.0. can install CMake following CMake installation instructions. important thing add CMake binary PATH: macOS, can install CMake run . menu bar, item Install Command Line Use can click proper instructions update PATH. Note location CMake binary always /Applications/CMake.app/Contents/bin/cmake. Hence, nloptr knows find even update PATH. Linux, automatically added unless specifically change default installation directory building CMake. Alternatively, can set environment variable CMAKE_BIN pointing CMake binary liking computer nloptr use.","code":""},{"path":"https://astamm.github.io/nloptr/index.html","id":"installing-nloptr","dir":"","previous_headings":"Installation","what":"Installing nloptr","title":"R Interface to NLopt","text":"can install nloptr CRAN using: Alternatively, can install development version GitHub:","code":"install.packages(\"nloptr\") # install.packages(\"remotes\") remotes::install_github(\"astamm/nloptr\")"},{"path":"https://astamm.github.io/nloptr/index.html","id":"acknowledgments","dir":"","previous_headings":"","what":"Acknowledgments","title":"R Interface to NLopt","text":"like express sincere gratitude Avraham Adler, Dirk Eddelbuettel, Mikael Jagan, Tomas Kalibera, Jeroen Ooms Jelmer Ypma contributions instructive discussions pros cons various build strategies R packages.","code":""},{"path":"https://astamm.github.io/nloptr/index.html","id":"reference","dir":"","previous_headings":"","what":"Reference","title":"R Interface to NLopt","text":"Steven G. Johnson, NLopt nonlinear-optimization package, https://nlopt.readthedocs.io/en/latest/","code":""},{"path":"https://astamm.github.io/nloptr/reference/auglag.html","id":null,"dir":"Reference","previous_headings":"","what":"Augmented Lagrangian Algorithm — auglag","title":"Augmented Lagrangian Algorithm — auglag","text":"Augmented Lagrangian method adds additional terms unconstrained objective function, designed emulate Lagrangian multiplier.","code":""},{"path":"https://astamm.github.io/nloptr/reference/auglag.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Augmented Lagrangian Algorithm — auglag","text":"","code":"auglag(   x0,   fn,   gr = NULL,   lower = NULL,   upper = NULL,   hin = NULL,   hinjac = NULL,   heq = NULL,   heqjac = NULL,   localsolver = \"COBYLA\",   localtol = 1e-06,   ineq2local = FALSE,   nl.info = FALSE,   control = list(),   deprecatedBehavior = TRUE,   ... )"},{"path":"https://astamm.github.io/nloptr/reference/auglag.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Augmented Lagrangian Algorithm — auglag","text":"x0 starting point searching optimum. fn objective function minimized. gr gradient objective function; provided provided NULL solver requires derivatives. lower, upper lower upper bound constraints. hin, hinjac defines inequality constraints, hin(x) >= 0 heq, heqjac defines equality constraints, heq(x) = 0. localsolver available local solvers: COBYLA, LBFGS, MMA, SLSQP. localtol tolerance applied selected local solver. ineq2local logical; shall inequality constraints treated local solver?; possible moment. nl.info logical; shall original NLopt info shown. control list options, see nl.opts help. deprecatedBehavior logical; TRUE (default now), old behavior Jacobian function used, equality \\(\\ge 0\\) instead \\(\\le 0\\). reversed future release eventually removed. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/auglag.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Augmented Lagrangian Algorithm — auglag","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. global_solver global NLOPT solver used. local_solver local NLOPT solver used, LBFGS COBYLA. convergence integer code indicating successful completion (> 0) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/auglag.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Augmented Lagrangian Algorithm — auglag","text":"method combines objective function nonlinear inequality/equality constraints () single function: essentially, objective plus `penalty' violated constraints. modified objective function passed another optimization algorithm nonlinear constraints. constraints violated solution sub-problem, size penalties increased process repeated; eventually, process must converge desired solution (exists). Since actual optimization performed subsidiary optimizer, subsidiary algorithm specify determines whether optimization gradient-based derivative-free. local solvers available moment COBYLA'' (derivative-free approach) LBFGS”, MMA'', SLSQP” (smooth functions). tolerance local solver provided. variant uses penalty functions equality constraints inequality constraints passed subsidiary algorithm handled directly; case, subsidiary algorithm must handle inequality constraints.  (moment, variant turned problems NLOPT library.)","code":""},{"path":"https://astamm.github.io/nloptr/reference/auglag.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Augmented Lagrangian Algorithm — auglag","text":"Birgin Martinez provide free implementation method part TANGO project; implementations can found semi-free packages like LANCELOT.","code":""},{"path":"https://astamm.github.io/nloptr/reference/auglag.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Augmented Lagrangian Algorithm — auglag","text":"Andrew R. Conn, Nicholas . M. Gould, Philippe L. Toint, “globally convergent augmented Lagrangian algorithm optimization general constraints simple bounds,” SIAM J. Numer. Anal. vol. 28, . 2, p. 545-572 (1991). E. G. Birgin J. M. Martinez, “Improving ultimate convergence augmented Lagrangian method,\" Optimization Methods Software vol. 23, . 2, p. 177-195 (2008).","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/auglag.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Augmented Lagrangian Algorithm — auglag","text":"Hans W. Borchers","code":""},{"path":"https://astamm.github.io/nloptr/reference/auglag.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Augmented Lagrangian Algorithm — auglag","text":"","code":"x0 <- c(1, 1) fn <- function(x) (x[1] - 2) ^ 2 + (x[2] - 1) ^ 2 hin <- function(x) 0.25 * x[1]^2 + x[2] ^ 2 - 1  # hin <= 0 heq <- function(x) x[1] - 2 * x[2] + 1           # heq = 0 gr <- function(x) nl.grad(x, fn) hinjac <- function(x) nl.jacobian(x, hin) heqjac <- function(x) nl.jacobian(x, heq)  # with COBYLA auglag(x0, fn, gr = NULL, hin = hin, heq = heq, deprecatedBehavior = FALSE) #> $par #> [1] 0.8228755 0.9114379 #>  #> $value #> [1] 1.393465 #>  #> $iter #> [1] 1001 #>  #> $global_solver #> [1] \"NLOPT_LN_AUGLAG\" #>  #> $local_solver #> [1] \"NLOPT_LN_COBYLA\" #>  #> $convergence #> [1] 5 #>  #> $message #> [1] \"NLOPT_MAXEVAL_REACHED: Optimization stopped because maxeval (above) was reached.\" #>   # $par:   0.8228761 0.9114382 # $value:   1.393464 # $iter:  1001  auglag(x0, fn, gr = NULL, hin = hin, heq = heq, localsolver = \"SLSQP\",        deprecatedBehavior = FALSE) #> $par #> [1] 0.8228757 0.9114378 #>  #> $value #> [1] 1.393465 #>  #> $iter #> [1] 184 #>  #> $global_solver #> [1] \"NLOPT_LD_AUGLAG\" #>  #> $local_solver #> [1] \"NLOPT_LD_SLSQP\" #>  #> $convergence #> [1] 4 #>  #> $message #> [1] \"NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached.\" #>   # $par:   0.8228757 0.9114378 # $value:   1.393465 # $iter   184  ##  Example from the alabama::auglag help page ##  Parameters should be roughly (0, 0, 1) with an objective value of 1.  fn <- function(x) (x[1] + 3 * x[2] + x[3]) ^ 2 + 4 * (x[1] - x[2]) ^ 2 heq <- function(x) x[1] + x[2] + x[3] - 1 # hin restated from alabama example to be <= 0. hin <- function(x) c(-6 * x[2] - 4 * x[3] + x[1] ^ 3 + 3, -x[1], -x[2], -x[3])  set.seed(12) auglag(runif(3), fn, hin = hin, heq = heq, localsolver= \"lbfgs\",        deprecatedBehavior = FALSE) #> $par #> [1] 4.861756e-08 4.732373e-08 9.999999e-01 #>  #> $value #> [1] 1 #>  #> $iter #> [1] 145 #>  #> $global_solver #> [1] \"NLOPT_LD_AUGLAG\" #>  #> $local_solver #> [1] \"NLOPT_LD_LBFGS\" #>  #> $convergence #> [1] 4 #>  #> $message #> [1] \"NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached.\" #>   # $par:   4.861756e-08 4.732373e-08 9.999999e-01 # $value:   1 # $iter:  145  ##  Powell problem from the Rsolnp::solnp help page ##  Parameters should be roughly (-1.7171, 1.5957, 1.8272, -0.7636, -0.7636) ##  with an objective value of 0.0539498478.  x0 <- c(-2, 2, 2, -1, -1) fn1  <- function(x) exp(x[1] * x[2] * x[3] * x[4] * x[5]) eqn1 <-function(x)   c(x[1] * x[1] + x[2] * x[2] + x[3] * x[3] + x[4] * x[4] + x[5] * x[5] - 10,     x[2] * x[3] - 5 * x[4] * x[5],     x[1] * x[1] * x[1] + x[2] * x[2] * x[2] + 1)  auglag(x0, fn1, heq = eqn1, localsolver = \"mma\", deprecatedBehavior = FALSE) #> $par #> [1] -1.7172821  1.5958701  1.8269884 -0.7636277 -0.7636277 #>  #> $value #> [1] 0.05394986 #>  #> $iter #> [1] 932 #>  #> $global_solver #> [1] \"NLOPT_LD_AUGLAG\" #>  #> $local_solver #> [1] \"NLOPT_LD_MMA\" #>  #> $convergence #> [1] 4 #>  #> $message #> [1] \"NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached.\" #>   # $par: -1.7173645  1.5959655  1.8268352 -0.7636185 -0.7636185 # $value:   0.05394987 # $iter:  916"},{"path":"https://astamm.github.io/nloptr/reference/bobyqa.html","id":null,"dir":"Reference","previous_headings":"","what":"Bound Optimization by Quadratic Approximation — bobyqa","title":"Bound Optimization by Quadratic Approximation — bobyqa","text":"BOBYQA performs derivative-free bound-constrained optimization using iteratively constructed quadratic approximation objective function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/bobyqa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bound Optimization by Quadratic Approximation — bobyqa","text":"","code":"bobyqa(   x0,   fn,   lower = NULL,   upper = NULL,   nl.info = FALSE,   control = list(),   ... )"},{"path":"https://astamm.github.io/nloptr/reference/bobyqa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bound Optimization by Quadratic Approximation — bobyqa","text":"x0 starting point searching optimum. fn objective function minimized. lower, upper lower upper bound constraints. nl.info logical; shall original NLopt info shown. control list options, see nl.opts help. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/bobyqa.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bound Optimization by Quadratic Approximation — bobyqa","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 0) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/bobyqa.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bound Optimization by Quadratic Approximation — bobyqa","text":"algorithm derived BOBYQA Fortran subroutine Powell, converted C modified NLopt stopping criteria.","code":""},{"path":"https://astamm.github.io/nloptr/reference/bobyqa.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bound Optimization by Quadratic Approximation — bobyqa","text":"BOBYQA constructs quadratic approximation objective, may perform poorly objective functions twice-differentiable.","code":""},{"path":"https://astamm.github.io/nloptr/reference/bobyqa.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bound Optimization by Quadratic Approximation — bobyqa","text":"M. J. D. Powell. “BOBYQA algorithm bound constrained optimization without derivatives,” Department Applied Mathematics Theoretical Physics, Cambridge England, technical reportNA2009/06 (2009).","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/bobyqa.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bound Optimization by Quadratic Approximation — bobyqa","text":"","code":"## Rosenbrock Banana function  rbf <- function(x) {(1 - x[1]) ^ 2 + 100 * (x[2] - x[1] ^ 2) ^ 2}  ## The function as written above has a minimum of 0 at (1, 1)  S <- bobyqa(c(0, 0), rbf)   S #> $par #> [1] 0.9999999 0.9999998 #>  #> $value #> [1] 9.022277e-15 #>  #> $iter #> [1] 118 #>  #> $convergence #> [1] 4 #>  #> $message #> [1] \"NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached.\" #>   ## Rosenbrock Banana function with both parameters constrained to [0, 0.5]  S <- bobyqa(c(0, 0), rbf, lower = c(0, 0), upper = c(0.5, 0.5))  S #> $par #> [1] 0.50 0.25 #>  #> $value #> [1] 0.25 #>  #> $iter #> [1] 44 #>  #> $convergence #> [1] 4 #>  #> $message #> [1] \"NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached.\" #>"},{"path":"https://astamm.github.io/nloptr/reference/ccsaq.html","id":null,"dir":"Reference","previous_headings":"","what":"Conservative Convex Separable Approximation with Affine Approximation plus Quadratic Penalty — ccsaq","title":"Conservative Convex Separable Approximation with Affine Approximation plus Quadratic Penalty — ccsaq","text":"variant CCSA (\"conservative convex separable approximation\") , instead constructing local MMA approximations, constructs simple quadratic approximations (rather, affine approximations plus quadratic penalty term stay conservative)","code":""},{"path":"https://astamm.github.io/nloptr/reference/ccsaq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conservative Convex Separable Approximation with Affine Approximation plus Quadratic Penalty — ccsaq","text":"","code":"ccsaq(   x0,   fn,   gr = NULL,   lower = NULL,   upper = NULL,   hin = NULL,   hinjac = NULL,   nl.info = FALSE,   control = list(),   deprecatedBehavior = TRUE,   ... )"},{"path":"https://astamm.github.io/nloptr/reference/ccsaq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conservative Convex Separable Approximation with Affine Approximation plus Quadratic Penalty — ccsaq","text":"x0 starting point searching optimum. fn objective function minimized. gr gradient function fn; calculated numerically specified. lower, upper lower upper bound constraints. hin function defining inequality constraints, hin>=0 components. hinjac Jacobian function hin; calculated numerically specified. nl.info logical; shall original NLopt info shown. control list options, see nl.opts help. deprecatedBehavior logical; TRUE (default now), old behavior Jacobian function used, equality \\(\\ge 0\\) instead \\(\\le 0\\). reversed future release eventually removed. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/ccsaq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conservative Convex Separable Approximation with Affine Approximation plus Quadratic Penalty — ccsaq","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 1) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/ccsaq.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Conservative Convex Separable Approximation with Affine Approximation plus Quadratic Penalty — ccsaq","text":"“Globally convergent” mean algorithm converges global optimum; means guaranteed converge local minimum feasible starting point.","code":""},{"path":"https://astamm.github.io/nloptr/reference/ccsaq.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Conservative Convex Separable Approximation with Affine Approximation plus Quadratic Penalty — ccsaq","text":"Krister Svanberg, “class globally convergent optimization methods based conservative convex separable approximations,” SIAM J. Optim. 12 (2), p. 555-573 (2002).","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/ccsaq.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conservative Convex Separable Approximation with Affine Approximation plus Quadratic Penalty — ccsaq","text":"","code":"##  Solve the Hock-Schittkowski problem no. 100 with analytic gradients ##  See https://apmonitor.com/wiki/uploads/Apps/hs100.apm  x0.hs100 <- c(1, 2, 0, 4, 0, 1, 1) fn.hs100 <- function(x) {(x[1] - 10) ^ 2 + 5 * (x[2] - 12) ^ 2 + x[3] ^ 4 +                          3 * (x[4] - 11) ^ 2 + 10 * x[5] ^ 6 + 7 * x[6] ^ 2 +                          x[7] ^ 4 - 4 * x[6] * x[7] - 10 * x[6] - 8 * x[7]}  hin.hs100 <- function(x) {c( 2 * x[1] ^ 2 + 3 * x[2] ^ 4 + x[3] + 4 * x[4] ^ 2 + 5 * x[5] - 127, 7 * x[1] + 3 * x[2] + 10 * x[3] ^ 2 + x[4] - x[5] - 282, 23 * x[1] + x[2] ^ 2 + 6 * x[6] ^ 2 - 8 * x[7] - 196, 4 * x[1] ^ 2 + x[2] ^ 2 - 3 * x[1] * x[2] + 2 * x[3] ^ 2 + 5 * x[6] -  11 * x[7]) }  gr.hs100 <- function(x) {  c( 2 * x[1] - 20,    10 * x[2] - 120,     4 * x[3] ^ 3,     6 * x[4] - 66,    60 * x[5] ^ 5,    14 * x[6] - 4 * x[7] - 10,     4 * x[7] ^ 3 - 4 * x[6] - 8) }  hinjac.hs100 <- function(x) {   matrix(c(4 * x[1], 12 * x[2] ^ 3, 1, 8 * x[4], 5, 0, 0,            7, 3, 20 * x[3], 1, -1, 0, 0,            23, 2 * x[2], 0, 0, 0, 12 * x[6], -8,            8 * x[1] - 3 * x[2], 2 * x[2] - 3 * x[1], 4 * x[3], 0, 0, 5, -11),            nrow = 4, byrow = TRUE) }  ##  The optimum value of the objective function should be 680.6300573 ##  A suitable parameter vector is roughly ##  (2.330, 1.9514, -0.4775, 4.3657, -0.6245, 1.0381, 1.5942)  # Results with exact Jacobian S <- ccsaq(x0.hs100, fn.hs100, gr = gr.hs100,       hin = hin.hs100, hinjac = hinjac.hs100,       nl.info = TRUE, control = list(xtol_rel = 1e-8),       deprecatedBehavior = FALSE) #>  #> Call: #> nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper,  #>     eval_g_ineq = hin, eval_jac_g_ineq = hinjac, opts = opts) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because  #> xtol_rel or xtol_abs (above) was reached. ) #>  #> Number of Iterations....: 173  #> Termination conditions:  stopval: -Inf\txtol_rel: 1e-08\tmaxeval: 1000\tftol_rel: 0\tftol_abs: 0  #> Number of inequality constraints:  4  #> Number of equality constraints:    0  #> Optimal value of objective function:  680.630049959532  #> Optimal value of controls: 2.330501 1.951372 -0.4775423 4.365727 -0.6244867 1.038133 1.59423 #>  #>   # Results without Jacobian S <- ccsaq(x0.hs100, fn.hs100, hin = hin.hs100,       nl.info = TRUE, control = list(xtol_rel = 1e-8),       deprecatedBehavior = FALSE) #>  #> Call: #> nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper,  #>     eval_g_ineq = hin, eval_jac_g_ineq = hinjac, opts = opts) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because  #> xtol_rel or xtol_abs (above) was reached. ) #>  #> Number of Iterations....: 52  #> Termination conditions:  stopval: -Inf\txtol_rel: 1e-08\tmaxeval: 1000\tftol_rel: 0\tftol_abs: 0  #> Number of inequality constraints:  4  #> Number of equality constraints:    0  #> Optimal value of objective function:  680.630056814947  #> Optimal value of controls: 2.330499 1.951374 -0.477733 4.365728 -0.6244887 1.038021 1.594209 #>  #>"},{"path":"https://astamm.github.io/nloptr/reference/check.derivatives.html","id":null,"dir":"Reference","previous_headings":"","what":"Check analytic gradients of a function using finite difference approximations — check.derivatives","title":"Check analytic gradients of a function using finite difference approximations — check.derivatives","text":"function compares analytic gradients function finite difference approximation prints results checks.","code":""},{"path":"https://astamm.github.io/nloptr/reference/check.derivatives.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check analytic gradients of a function using finite difference approximations — check.derivatives","text":"","code":"check.derivatives(   .x,   func,   func_grad,   check_derivatives_tol = 1e-04,   check_derivatives_print = \"all\",   func_grad_name = \"grad_f\",   ... )"},{"path":"https://astamm.github.io/nloptr/reference/check.derivatives.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check analytic gradients of a function using finite difference approximations — check.derivatives","text":".x point comparison done. func function evaluated. func_grad function calculating analytic gradients. check_derivatives_tol option determining differences analytic gradient finite difference approximation flagged error. check_derivatives_print option related amount output. '' means comparisons shown, 'errors' shows comparisons flagged error, 'none' shows number errors . func_grad_name option change name gradient function shows output. ... arguments passed functions func func_grad.","code":""},{"path":"https://astamm.github.io/nloptr/reference/check.derivatives.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check analytic gradients of a function using finite difference approximations — check.derivatives","text":"return value contains list analytic gradient, finite difference approximation, relative errors, vector comparing relative errors tolerance.","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/check.derivatives.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Check analytic gradients of a function using finite difference approximations — check.derivatives","text":"Jelmer Ypma","code":""},{"path":"https://astamm.github.io/nloptr/reference/check.derivatives.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check analytic gradients of a function using finite difference approximations — check.derivatives","text":"","code":"library('nloptr')  # example with correct gradient f <- function(x, a) sum((x - a) ^ 2)  f_grad <- function(x, a)  2 * (x - a)  check.derivatives(.x = 1:10, func = f, func_grad = f_grad,           check_derivatives_print = 'none', a = runif(10)) #> Derivative checker results: 0 error(s) detected. #> $analytic #>  [1]  1.461236  3.661304  5.932209  7.642430  8.716669 11.954245 13.983350 #>  [8] 15.214606 16.372239 19.247503 #>  #> $finite_difference #>  [1]  1.461239  3.661304  5.932209  7.642430  8.716669 11.954245 13.983350 #>  [8] 15.214606 16.372239 19.247503 #>  #> $relative_error #>  [1] 1.788727e-06 1.967545e-07 4.380139e-08 4.106753e-08 2.239447e-08 #>  [6] 8.413985e-09 9.619268e-09 1.331370e-08 3.355223e-08 9.943461e-09 #>  #> $flag_derivative_warning #>  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>   # example with incorrect gradient f_grad <- function(x, a)  2 * (x - a) + c(0, 0.1, rep(0, 8))  check.derivatives(.x = 1:10, func = f, func_grad = f_grad,           check_derivatives_print = 'errors', a = runif(10)) #> Derivative checker results: 1 error(s) detected. #>  #> * grad_f[ 2] = 3.570163e+00 ~ 3.470163e+00   [2.881706e-02] #>  #> $analytic #>  [1]  1.238376  3.570163  5.121331  7.084786  8.918585 10.668640 13.774602 #>  [8] 15.563266 16.424327 19.804294 #>  #> $finite_difference #>  [1]  1.238377  3.470163  5.121332  7.084785  8.918585 10.668641 13.774602 #>  [8] 15.563266 16.424327 19.804294 #>  #> $relative_error #>  [1] 7.952941e-07 2.881706e-02 3.233928e-08 3.332356e-08 3.358306e-08 #>  [6] 4.051958e-08 3.800736e-09 8.706877e-09 7.393552e-09 1.433832e-08 #>  #> $flag_derivative_warning #>  [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>   # example with incorrect gradient of vector-valued function g <- function(x, a) c(sum(x - a), sum((x - a) ^ 2))  g_grad <- function(x, a) {    rbind(rep(1, length(x)) + c(0, 0.01, rep(0, 8)),    2 * (x - a) + c(0, 0.1, rep(0, 8))) }  check.derivatives(.x = 1:10, func = g, func_grad = g_grad,           check_derivatives_print = 'all', a = runif(10)) #> Derivative checker results: 2 error(s) detected. #>  #>   grad_f[1,  1] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00] #>   grad_f[2,  1] = 5.803391e-01 ~ 5.803413e-01   [3.914069e-06] #> * grad_f[1,  2] = 1.010000e+00 ~ 1.000000e+00   [1.000000e-02] #> * grad_f[2,  2] = 3.664354e+00 ~ 3.564354e+00   [2.805557e-02] #>   grad_f[1,  3] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00] #>   grad_f[2,  3] = 5.464113e+00 ~ 5.464113e+00   [3.923042e-08] #>   grad_f[1,  4] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00] #>   grad_f[2,  4] = 6.990464e+00 ~ 6.990464e+00   [1.632038e-08] #>   grad_f[1,  5] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00] #>   grad_f[2,  5] = 9.622826e+00 ~ 9.622826e+00   [2.484410e-08] #>   grad_f[1,  6] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00] #>   grad_f[2,  6] = 1.112114e+01 ~ 1.112114e+01   [7.536909e-09] #>   grad_f[1,  7] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00] #>   grad_f[2,  7] = 1.266036e+01 ~ 1.266036e+01   [2.294086e-08] #>   grad_f[1,  8] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00] #>   grad_f[2,  8] = 1.551823e+01 ~ 1.551823e+01   [1.536377e-08] #>   grad_f[1,  9] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00] #>   grad_f[2,  9] = 1.621347e+01 ~ 1.621347e+01   [1.545171e-08] #>   grad_f[1, 10] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00] #>   grad_f[2, 10] = 1.823449e+01 ~ 1.823449e+01   [1.685468e-08] #>  #> $analytic #>           [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8] #> [1,] 1.0000000 1.010000 1.000000 1.000000 1.000000  1.00000  1.00000  1.00000 #> [2,] 0.5803391 3.664354 5.464113 6.990464 9.622826 11.12114 12.66036 15.51823 #>          [,9]    [,10] #> [1,]  1.00000  1.00000 #> [2,] 16.21347 18.23449 #>  #> $finite_difference #>           [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8] #> [1,] 1.0000000 1.000000 1.000000 1.000000 1.000000  1.00000  1.00000  1.00000 #> [2,] 0.5803413 3.564354 5.464113 6.990464 9.622826 11.12114 12.66036 15.51823 #>          [,9]    [,10] #> [1,]  1.00000  1.00000 #> [2,] 16.21347 18.23449 #>  #> $relative_error #>              [,1]       [,2]         [,3]         [,4]        [,5]         [,6] #> [1,] 0.000000e+00 0.01000000 0.000000e+00 0.000000e+00 0.00000e+00 0.000000e+00 #> [2,] 3.914069e-06 0.02805557 3.923042e-08 1.632038e-08 2.48441e-08 7.536909e-09 #>              [,7]         [,8]         [,9]        [,10] #> [1,] 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 #> [2,] 2.294086e-08 1.536377e-08 1.545171e-08 1.685468e-08 #>  #> $flag_derivative_warning #>       [,1] [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] #> [1,] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [2,] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>"},{"path":"https://astamm.github.io/nloptr/reference/cobyla.html","id":null,"dir":"Reference","previous_headings":"","what":"Constrained Optimization by Linear Approximations — cobyla","title":"Constrained Optimization by Linear Approximations — cobyla","text":"COBYLA algorithm derivative-free optimization nonlinear inequality equality constraints (see ).","code":""},{"path":"https://astamm.github.io/nloptr/reference/cobyla.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Constrained Optimization by Linear Approximations — cobyla","text":"","code":"cobyla(   x0,   fn,   lower = NULL,   upper = NULL,   hin = NULL,   nl.info = FALSE,   control = list(),   deprecatedBehavior = TRUE,   ... )"},{"path":"https://astamm.github.io/nloptr/reference/cobyla.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Constrained Optimization by Linear Approximations — cobyla","text":"x0 starting point searching optimum. fn objective function minimized. lower, upper lower upper bound constraints. hin function defining inequality constraints, hin>=0 components. nl.info logical; shall original NLopt info shown. control list options, see nl.opts help. deprecatedBehavior logical; TRUE (default now), old behavior Jacobian function used, equality \\(\\ge 0\\) instead \\(\\le 0\\). reversed future release eventually removed. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/cobyla.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Constrained Optimization by Linear Approximations — cobyla","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 0) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/cobyla.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Constrained Optimization by Linear Approximations — cobyla","text":"constructs successive linear approximations objective function constraints via simplex \\(n+1\\) points (\\(n\\) dimensions), optimizes approximations trust region step. COBYLA supports equality constraints transforming two inequality constraints. functionality added wrapper. use COBYLA equality constraints, please use full nloptr invocation.","code":""},{"path":"https://astamm.github.io/nloptr/reference/cobyla.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Constrained Optimization by Linear Approximations — cobyla","text":"original code, written Fortran Powell, converted C SciPy project.","code":""},{"path":"https://astamm.github.io/nloptr/reference/cobyla.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Constrained Optimization by Linear Approximations — cobyla","text":"M. J. D. Powell, “direct search optimization method models objective constraint functions linear interpolation,” Advances Optimization Numerical Analysis, eds. S. Gomez J.-P. Hennart (Kluwer Academic: Dordrecht, 1994), p. 51-67.","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/cobyla.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Constrained Optimization by Linear Approximations — cobyla","text":"Hans W. Borchers","code":""},{"path":"https://astamm.github.io/nloptr/reference/cobyla.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Constrained Optimization by Linear Approximations — cobyla","text":"","code":"##  Solve the Hock-Schittkowski problem no. 100 with analytic gradients ##  See https://apmonitor.com/wiki/uploads/Apps/hs100.apm  x0.hs100 <- c(1, 2, 0, 4, 0, 1, 1) fn.hs100 <- function(x) {(x[1] - 10) ^ 2 + 5 * (x[2] - 12) ^ 2 + x[3] ^ 4 +                          3 * (x[4] - 11) ^ 2 + 10 * x[5] ^ 6 + 7 * x[6] ^ 2 +                          x[7] ^ 4 - 4 * x[6] * x[7] - 10 * x[6] - 8 * x[7]}  hin.hs100 <- function(x) {c( 2 * x[1] ^ 2 + 3 * x[2] ^ 4 + x[3] + 4 * x[4] ^ 2 + 5 * x[5] - 127, 7 * x[1] + 3 * x[2] + 10 * x[3] ^ 2 + x[4] - x[5] - 282, 23 * x[1] + x[2] ^ 2 + 6 * x[6] ^ 2 - 8 * x[7] - 196, 4 * x[1] ^ 2 + x[2] ^ 2 - 3 * x[1] * x[2] + 2 * x[3] ^ 2 + 5 * x[6] -  11 * x[7]) }  S <- cobyla(x0.hs100, fn.hs100, hin = hin.hs100,       nl.info = TRUE, control = list(xtol_rel = 1e-8, maxeval = 2000),       deprecatedBehavior = FALSE) #>  #> Call: #> nloptr(x0 = x0, eval_f = fn, lb = lower, ub = upper, eval_g_ineq = hin,  #>     opts = opts) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because  #> xtol_rel or xtol_abs (above) was reached. ) #>  #> Number of Iterations....: 1623  #> Termination conditions:  stopval: -Inf\txtol_rel: 1e-08\tmaxeval: 2000\tftol_rel: 0\tftol_abs: 0  #> Number of inequality constraints:  4  #> Number of equality constraints:    0  #> Optimal value of objective function:  680.630057374426  #> Optimal value of controls: 2.330499 1.951372 -0.4775447 4.365726 -0.624487 1.038131 1.594227 #>  #>   ##  The optimum value of the objective function should be 680.6300573 ##  A suitable parameter vector is roughly ##  (2.330, 1.9514, -0.4775, 4.3657, -0.6245, 1.0381, 1.5942)  S #> $par #> [1]  2.3304990  1.9513724 -0.4775447  4.3657263 -0.6244870  1.0381308  1.5942267 #>  #> $value #> [1] 680.6301 #>  #> $iter #> [1] 1623 #>  #> $convergence #> [1] 4 #>  #> $message #> [1] \"NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached.\" #>"},{"path":"https://astamm.github.io/nloptr/reference/crs2lm.html","id":null,"dir":"Reference","previous_headings":"","what":"Controlled Random Search — crs2lm","title":"Controlled Random Search — crs2lm","text":"Controlled Random Search (CRS) algorithm (particular, CRS2 variant) `local mutation' modification.","code":""},{"path":"https://astamm.github.io/nloptr/reference/crs2lm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Controlled Random Search — crs2lm","text":"","code":"crs2lm(   x0,   fn,   lower,   upper,   maxeval = 10000,   pop.size = 10 * (length(x0) + 1),   ranseed = NULL,   xtol_rel = 1e-06,   nl.info = FALSE,   ... )"},{"path":"https://astamm.github.io/nloptr/reference/crs2lm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Controlled Random Search — crs2lm","text":"x0 initial point searching optimum. fn objective function minimized. lower, upper lower upper bound constraints. maxeval maximum number function evaluations. pop.size population size. ranseed prescribe seed random number generator. xtol_rel stopping criterion relative change reached. nl.info logical; shall original NLopt info shown. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/crs2lm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Controlled Random Search — crs2lm","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 0) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/crs2lm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Controlled Random Search — crs2lm","text":"CRS algorithms sometimes compared genetic algorithms, start random population points, randomly evolve points heuristic rules. case, evolution somewhat resembles randomized Nelder-Mead algorithm. published results CRS seem largely empirical.","code":""},{"path":"https://astamm.github.io/nloptr/reference/crs2lm.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Controlled Random Search — crs2lm","text":"initial population size CRS defaults \\(10x(n+1)\\) \\(n\\) dimensions, can changed. initial population must least \\(n+1\\).","code":""},{"path":"https://astamm.github.io/nloptr/reference/crs2lm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Controlled Random Search — crs2lm","text":"W. L. Price, “Global optimization controlled random search,” J. Optim. Theory Appl. 40 (3), p. 333-348 (1983). P. Kaelo M. M. Ali, “variants controlled random search algorithm global optimization,” J. Optim. Theory Appl. 130 (2), 253-264 (2006).","code":""},{"path":"https://astamm.github.io/nloptr/reference/crs2lm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Controlled Random Search — crs2lm","text":"","code":"## Minimize the Hartmann 6-Dimensional function ## See https://www.sfu.ca/~ssurjano/hart6.html  a <- c(1.0, 1.2, 3.0, 3.2) A <- matrix(c(10,  0.05, 3, 17,               3, 10, 3.5, 8,               17, 17, 1.7, 0.05,               3.5, 0.1, 10, 10,               1.7, 8, 17, 0.1,               8, 14, 8, 14), nrow = 4)  B  <- matrix(c(.1312, .2329, .2348, .4047,                .1696, .4135, .1451, .8828,                .5569, .8307, .3522, .8732,                .0124, .3736, .2883, .5743,                .8283, .1004, .3047, .1091,                .5886, .9991, .6650, .0381), nrow = 4)  hartmann6 <- function(x, a, A, B) {   fun <- 0   for (i in 1:4) {     fun <- fun - a[i] * exp(-sum(A[i, ] * (x - B[i, ]) ^ 2))   }    fun }  ## The function has a global minimum of -3.32237 at ## (0.20169, 0.150011, 0.476874, 0.275332, 0.311652, 0.6573)  S <- crs2lm(x0 = rep(0, 6), hartmann6, lower = rep(0, 6), upper = rep(1, 6),             ranseed = 10L, nl.info = TRUE, xtol_rel=1e-8, maxeval = 10000,             a = a, A = A, B = B) #>  #> Call: #> nloptr(x0 = x0, eval_f = fn, lb = lower, ub = upper, opts = opts) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because  #> xtol_rel or xtol_abs (above) was reached. ) #>  #> Number of Iterations....: 5690  #> Termination conditions:  maxeval: 10000\txtol_rel: 1e-08  #> Number of inequality constraints:  0  #> Number of equality constraints:    0  #> Optimal value of objective function:  -3.32236801141551  #> Optimal value of controls: 0.2016895 0.1500107 0.476874 0.2753324 0.3116516 0.6573005 #>  #>   S #> $par #> [1] 0.2016895 0.1500107 0.4768740 0.2753324 0.3116516 0.6573005 #>  #> $value #> [1] -3.322368 #>  #> $iter #> [1] 5690 #>  #> $convergence #> [1] 4 #>  #> $message #> [1] \"NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached.\" #>"},{"path":"https://astamm.github.io/nloptr/reference/direct.html","id":null,"dir":"Reference","previous_headings":"","what":"DIviding RECTangles Algorithm for Global Optimization — direct","title":"DIviding RECTangles Algorithm for Global Optimization — direct","text":"DIRECT deterministic search algorithm based systematic division search domain smaller smaller hyperrectangles. DIRECT_L makes algorithm biased towards local search (efficient functions without many minima).","code":""},{"path":"https://astamm.github.io/nloptr/reference/direct.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DIviding RECTangles Algorithm for Global Optimization — direct","text":"","code":"direct(   fn,   lower,   upper,   scaled = TRUE,   original = FALSE,   nl.info = FALSE,   control = list(),   ... )  directL(   fn,   lower,   upper,   randomized = FALSE,   original = FALSE,   nl.info = FALSE,   control = list(),   ... )"},{"path":"https://astamm.github.io/nloptr/reference/direct.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DIviding RECTangles Algorithm for Global Optimization — direct","text":"fn objective function minimized. lower, upper lower upper bound constraints. scaled logical; shall hypercube scaled starting. original logical; whether use original implementation Gablonsky – performance mostly similar. nl.info logical; shall original NLopt info shown. control list options, see nl.opts help. ... additional arguments passed function. randomized logical; shall randomization used decide dimension halve next case near-ties.","code":""},{"path":"https://astamm.github.io/nloptr/reference/direct.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"DIviding RECTangles Algorithm for Global Optimization — direct","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 0) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/direct.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"DIviding RECTangles Algorithm for Global Optimization — direct","text":"DIRECT DIRECT-L algorithms start rescaling bound constraints hypercube, gives dimensions equal weight search procedure. dimensions equal weight, e.g. “long skinny” search space function varies speed directions, may better use unscaled variant DIRECT algorithm. algorithms handle finite bound constraints must provided. original versions may include support arbitrary nonlinear inequality, tested. original versions randomized unscaled variants, options disregarded versions.","code":""},{"path":"https://astamm.github.io/nloptr/reference/direct.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"DIviding RECTangles Algorithm for Global Optimization — direct","text":"DIRECT_L algorithm tried first.","code":""},{"path":"https://astamm.github.io/nloptr/reference/direct.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"DIviding RECTangles Algorithm for Global Optimization — direct","text":"D. R. Jones, C. D. Perttunen, B. E. Stuckmann, “Lipschitzian optimization without Lipschitz constant,” J. Optimization Theory Applications, vol. 79, p. 157 (1993). J. M. Gablonsky C. T. Kelley, “locally-biased form DIRECT algorithm,\" J. Global Optimization, vol. 21 (1), p. 27-37 (2001).","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/direct.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"DIviding RECTangles Algorithm for Global Optimization — direct","text":"Hans W. Borchers","code":""},{"path":"https://astamm.github.io/nloptr/reference/direct.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"DIviding RECTangles Algorithm for Global Optimization — direct","text":"","code":"### Minimize the Hartmann6 function hartmann6 <- function(x) {   a <- c(1.0, 1.2, 3.0, 3.2)   A <- matrix(c(10.0,  0.05, 3.0, 17.0,          3.0, 10.0,  3.5,  8.0,           17.0, 17.0,  1.7,  0.05,          3.5,  0.1, 10.0, 10.0,          1.7,  8.0, 17.0,  0.1,          8.0, 14.0,  8.0, 14.0), nrow=4, ncol=6)   B  <- matrix(c(.1312,.2329,.2348,.4047,          .1696,.4135,.1451,.8828,          .5569,.8307,.3522,.8732,          .0124,.3736,.2883,.5743,          .8283,.1004,.3047,.1091,          .5886,.9991,.6650,.0381), nrow=4, ncol=6)   fun <- 0   for (i in 1:4) {     fun <- fun - a[i] * exp(-sum(A[i,] * (x - B[i,]) ^ 2))   }   fun } S <- directL(hartmann6, rep(0, 6), rep(1, 6),        nl.info = TRUE, control = list(xtol_rel = 1e-8, maxeval = 1000)) #>  #> Call: #> nloptr(x0 = x0, eval_f = fn, lb = lower, ub = upper, opts = opts) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 5 ( NLOPT_MAXEVAL_REACHED: Optimization stopped because  #> maxeval (above) was reached. ) #>  #> Number of Iterations....: 1000  #> Termination conditions:  stopval: -Inf\txtol_rel: 1e-08\tmaxeval: 1000\tftol_rel: 0\tftol_abs: 0  #> Number of inequality constraints:  0  #> Number of equality constraints:    0  #> Current value of objective function:  -3.32236800687327  #> Current value of controls: 0.2016884 0.1500025 0.4768667 0.2753391 0.311648 0.6572931 #>  #>  ## Number of Iterations....: 1000 ## Termination conditions:  stopval: -Inf ##   xtol_rel: 1e-08,  maxeval: 1000,  ftol_rel: 0,  ftol_abs: 0 ## Number of inequality constraints:  0 ## Number of equality constraints:  0 ## Current value of objective function:  -3.32236800687327 ## Current value of controls: ##   0.2016884 0.1500025 0.4768667 0.2753391 0.311648 0.6572931"},{"path":"https://astamm.github.io/nloptr/reference/is.nloptr.html","id":null,"dir":"Reference","previous_headings":"","what":"R interface to NLopt — is.nloptr","title":"R interface to NLopt — is.nloptr","text":".nloptr preforms checks see fully specified problem supplied nloptr. Mostly internal use.","code":""},{"path":"https://astamm.github.io/nloptr/reference/is.nloptr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"R interface to NLopt — is.nloptr","text":"","code":"is.nloptr(x)"},{"path":"https://astamm.github.io/nloptr/reference/is.nloptr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"R interface to NLopt — is.nloptr","text":"x object tested.","code":""},{"path":"https://astamm.github.io/nloptr/reference/is.nloptr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"R interface to NLopt — is.nloptr","text":"Logical. Return TRUE tests passed, otherwise return FALSE exit Error.","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/is.nloptr.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"R interface to NLopt — is.nloptr","text":"Jelmer Ypma","code":""},{"path":"https://astamm.github.io/nloptr/reference/isres.html","id":null,"dir":"Reference","previous_headings":"","what":"Improved Stochastic Ranking Evolution Strategy — isres","title":"Improved Stochastic Ranking Evolution Strategy — isres","text":"Improved Stochastic Ranking Evolution Strategy (ISRES) algorithm nonlinearly constrained global optimization, least semi-global, although heuristics escape local optima.","code":""},{"path":"https://astamm.github.io/nloptr/reference/isres.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Improved Stochastic Ranking Evolution Strategy — isres","text":"","code":"isres(   x0,   fn,   lower,   upper,   hin = NULL,   heq = NULL,   maxeval = 10000,   pop.size = 20 * (length(x0) + 1),   xtol_rel = 1e-06,   nl.info = FALSE,   deprecatedBehavior = TRUE,   ... )"},{"path":"https://astamm.github.io/nloptr/reference/isres.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Improved Stochastic Ranking Evolution Strategy — isres","text":"x0 initial point searching optimum. fn objective function minimized. lower, upper lower upper bound constraints. hin function defining inequality constraints, hin <= 0 components. heq function defining equality constraints, heq = 0 components. maxeval maximum number function evaluations. pop.size population size. xtol_rel stopping criterion relative change reached. nl.info logical; shall original NLopt info shown. deprecatedBehavior logical; TRUE (default now), old behavior Jacobian function used, equality \\(\\ge 0\\) instead \\(\\le 0\\). reversed future release eventually removed. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/isres.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Improved Stochastic Ranking Evolution Strategy — isres","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 0) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/isres.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Improved Stochastic Ranking Evolution Strategy — isres","text":"evolution strategy based combination mutation rule—log-normal step-size update exponential smoothing—differential variation—Nelder-Mead-like update rule). fitness ranking simply via objective function problems without nonlinear constraints, nonlinear constraints included stochastic ranking proposed Runarsson Yao employed. method supports arbitrary nonlinear inequality equality constraints addition bounds constraints.","code":""},{"path":"https://astamm.github.io/nloptr/reference/isres.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Improved Stochastic Ranking Evolution Strategy — isres","text":"initial population size CRS defaults \\(20x(n+1)\\) \\(n\\) dimensions, can changed. initial population must least \\(n+1\\).","code":""},{"path":"https://astamm.github.io/nloptr/reference/isres.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Improved Stochastic Ranking Evolution Strategy — isres","text":"Thomas Philip Runarsson Xin Yao, “Search biases constrained evolutionary optimization,” IEEE Trans. Systems, Man, Cybernetics Part C: Applications Reviews, vol. 35 (. 2), pp. 233-243 (2005).","code":""},{"path":"https://astamm.github.io/nloptr/reference/isres.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Improved Stochastic Ranking Evolution Strategy — isres","text":"Hans W. Borchers","code":""},{"path":"https://astamm.github.io/nloptr/reference/isres.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Improved Stochastic Ranking Evolution Strategy — isres","text":"","code":"## Rosenbrock Banana objective function  rbf <- function(x) {(1 - x[1]) ^ 2 + 100 * (x[2] - x[1] ^ 2) ^ 2}  x0 <- c(-1.2, 1) lb <- c(-3, -3) ub <- c(3,  3)  ## The function as written above has a minimum of 0 at (1, 1)  isres(x0 = x0, fn = rbf, lower = lb, upper = ub) #> $par #> [1] 1.000015 1.000029 #>  #> $value #> [1] 2.785245e-10 #>  #> $iter #> [1] 10000 #>  #> $convergence #> [1] 5 #>  #> $message #> [1] \"NLOPT_MAXEVAL_REACHED: Optimization stopped because maxeval (above) was reached.\" #>   ## Now subject to the inequality that x[1] + x[2] <= 1.5  hin <- function(x) {x[1] + x[2] - 1.5}  S <- isres(x0 = x0, fn = rbf, hin = hin, lower = lb, upper = ub,            maxeval = 2e5L, deprecatedBehavior = FALSE)  S #> $par #> [1] 0.8231316 0.6768683 #>  #> $value #> [1] 0.03132831 #>  #> $iter #> [1] 13126 #>  #> $convergence #> [1] 4 #>  #> $message #> [1] \"NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached.\" #>   sum(S$par) #> [1] 1.5"},{"path":"https://astamm.github.io/nloptr/reference/lbfgs.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-storage BFGS — lbfgs","title":"Low-storage BFGS — lbfgs","text":"Low-storage version Broyden-Fletcher-Goldfarb-Shanno (BFGS) method.","code":""},{"path":"https://astamm.github.io/nloptr/reference/lbfgs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-storage BFGS — lbfgs","text":"","code":"lbfgs(   x0,   fn,   gr = NULL,   lower = NULL,   upper = NULL,   nl.info = FALSE,   control = list(),   ... )"},{"path":"https://astamm.github.io/nloptr/reference/lbfgs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-storage BFGS — lbfgs","text":"x0 initial point searching optimum. fn objective function minimized. gr gradient function fn; calculated numerically specified. lower, upper lower upper bound constraints. nl.info logical; shall original NLopt info shown. control list control parameters, see nl.opts help. ... arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/lbfgs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Low-storage BFGS — lbfgs","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 0) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/lbfgs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Low-storage BFGS — lbfgs","text":"low-storage (limited-memory) algorithm member class quasi-Newton optimization methods. well suited optimization problems large number variables. One parameter algorithm number m gradients remember previous optimization steps. NLopt sets m heuristic value default. can changed NLopt function set_vector_storage.","code":""},{"path":"https://astamm.github.io/nloptr/reference/lbfgs.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Low-storage BFGS — lbfgs","text":"Based Fortran implementation low-storage BFGS algorithm written L. Luksan, posted GNU LGPL license.","code":""},{"path":"https://astamm.github.io/nloptr/reference/lbfgs.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Low-storage BFGS — lbfgs","text":"J. Nocedal, “Updating quasi-Newton matrices limited storage,” Math. Comput. 35, 773-782 (1980). D. C. Liu J. Nocedal, “limited memory BFGS method large scale optimization,” Math. Programming 45, p. 503-528 (1989).","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/lbfgs.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Low-storage BFGS — lbfgs","text":"Hans W. Borchers","code":""},{"path":"https://astamm.github.io/nloptr/reference/lbfgs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Low-storage BFGS — lbfgs","text":"","code":"flb <- function(x) {   p <- length(x)   sum(c(1, rep(4, p-1)) * (x - c(1, x[-p])^2)^2) } # 25-dimensional box constrained: par[24] is *not* at the boundary S <- lbfgs(rep(3, 25), flb, lower=rep(2, 25), upper=rep(4, 25),      nl.info = TRUE, control = list(xtol_rel=1e-8)) #>  #> Call: #> nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper,  #>     opts = opts) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success return value. ) #>  #> Number of Iterations....: 19  #> Termination conditions:  stopval: -Inf\txtol_rel: 1e-08\tmaxeval: 1000\tftol_rel: 0\tftol_abs: 0  #> Number of inequality constraints:  0  #> Number of equality constraints:    0  #> Optimal value of objective function:  368.105912874334  #> Optimal value of controls: 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2.109093 4 #>  #>  ## Optimal value of objective function:  368.105912874334 ## Optimal value of controls: 2  ...  2  2.109093  4"},{"path":"https://astamm.github.io/nloptr/reference/mlsl.html","id":null,"dir":"Reference","previous_headings":"","what":"Multi-level Single-linkage — mlsl","title":"Multi-level Single-linkage — mlsl","text":"“Multi-Level Single-Linkage” (MLSL) algorithm global optimization searches sequence local optimizations random starting points. modification MLSL included using low-discrepancy sequence (LDS) instead pseudorandom numbers.","code":""},{"path":"https://astamm.github.io/nloptr/reference/mlsl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi-level Single-linkage — mlsl","text":"","code":"mlsl(   x0,   fn,   gr = NULL,   lower,   upper,   local.method = \"LBFGS\",   low.discrepancy = TRUE,   nl.info = FALSE,   control = list(),   ... )"},{"path":"https://astamm.github.io/nloptr/reference/mlsl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multi-level Single-linkage — mlsl","text":"x0 initial point searching optimum. fn objective function minimized. gr gradient function fn; calculated numerically specified. lower, upper lower upper bound constraints. local.method BFGS moment. low.discrepancy logical; shall low discrepancy variation used. nl.info logical; shall original NLopt info shown. control list options, see nl.opts help. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/mlsl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multi-level Single-linkage — mlsl","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 0) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/mlsl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multi-level Single-linkage — mlsl","text":"MLSL ‘multistart’ algorithm: works sequence local optimizations—using local optimization algorithm—random low-discrepancy starting points. MLSL distinguished, however, `clustering' heuristic helps avoid repeated searches local optima also theoretical guarantees finding local optima finite number local minimizations. local-search portion MLSL can use algorithms NLopt, , particular, can use either gradient-based derivative-free algorithms. wrapper gradient-based LBFGS available local method.","code":""},{"path":"https://astamm.github.io/nloptr/reference/mlsl.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Multi-level Single-linkage — mlsl","text":"set stopping tolerance local-optimization algorithm, MLSL defaults ftol_rel = 1e-15 xtol_rel = 1e-7 local searches.","code":""},{"path":"https://astamm.github.io/nloptr/reference/mlsl.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Multi-level Single-linkage — mlsl","text":". H. G. Rinnooy Kan G. T. Timmer, “Stochastic global optimization methods” Mathematical Programming, vol. 39, p. 27-78 (1987). Sergei Kucherenko Yury Sytsko, “Application deterministic low-discrepancy sequences global optimization”, Computational Optimization Applications, vol. 30, p. 297-318 (2005).","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/mlsl.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Multi-level Single-linkage — mlsl","text":"Hans W. Borchers","code":""},{"path":"https://astamm.github.io/nloptr/reference/mlsl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multi-level Single-linkage — mlsl","text":"","code":"## Minimize the Hartmann 6-Dimensional function ## See https://www.sfu.ca/~ssurjano/hart6.html  a <- c(1.0, 1.2, 3.0, 3.2) A <- matrix(c(10,  0.05, 3, 17,               3, 10, 3.5, 8,               17, 17, 1.7, 0.05,               3.5, 0.1, 10, 10,               1.7, 8, 17, 0.1,               8, 14, 8, 14), nrow = 4)  B  <- matrix(c(.1312, .2329, .2348, .4047,                .1696, .4135, .1451, .8828,                .5569, .8307, .3522, .8732,                .0124, .3736, .2883, .5743,                .8283, .1004, .3047, .1091,                .5886, .9991, .6650, .0381), nrow = 4)  hartmann6 <- function(x, a, A, B) {   fun <- 0   for (i in 1:4) {     fun <- fun - a[i] * exp(-sum(A[i, ] * (x - B[i, ]) ^ 2))   }    fun }  ## The function has a global minimum of -3.32237 at ## (0.20169, 0.150011, 0.476874, 0.275332, 0.311652, 0.6573)  S <- mlsl(x0 = rep(0, 6), hartmann6, lower = rep(0, 6), upper = rep(1, 6),       nl.info = TRUE, control = list(xtol_rel = 1e-8, maxeval = 1000),       a = a, A = A, B = B) #>  #> Call: #> nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper,  #>     opts = opts) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 5 ( NLOPT_MAXEVAL_REACHED: Optimization stopped because  #> maxeval (above) was reached. ) #>  #> Number of Iterations....: 1000  #> Termination conditions:  stopval: -Inf\txtol_rel: 1e-08\tmaxeval: 1000\tftol_rel: 0\tftol_abs: 0  #> Number of inequality constraints:  0  #> Number of equality constraints:    0  #> Current value of objective function:  -3.32236801141544  #> Current value of controls: 0.2016895 0.1500107 0.476874 0.2753324 0.3116516 0.6573005 #>  #>"},{"path":"https://astamm.github.io/nloptr/reference/mma.html","id":null,"dir":"Reference","previous_headings":"","what":"Method of Moving Asymptotes — mma","title":"Method of Moving Asymptotes — mma","text":"Globally-convergent method--moving-asymptotes (MMA) algorithm gradient-based local optimization, including nonlinear inequality constraints (equality constraints).","code":""},{"path":"https://astamm.github.io/nloptr/reference/mma.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Method of Moving Asymptotes — mma","text":"","code":"mma(   x0,   fn,   gr = NULL,   lower = NULL,   upper = NULL,   hin = NULL,   hinjac = NULL,   nl.info = FALSE,   control = list(),   deprecatedBehavior = TRUE,   ... )"},{"path":"https://astamm.github.io/nloptr/reference/mma.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Method of Moving Asymptotes — mma","text":"x0 starting point searching optimum. fn objective function minimized. gr gradient function fn; calculated numerically specified. lower, upper lower upper bound constraints. hin function defining inequality constraints, hin <= 0 components. hinjac Jacobian function hin; calculated numerically specified. nl.info logical; shall original NLopt info shown. control list options, see nl.opts help. deprecatedBehavior logical; TRUE (default now), old behavior Jacobian function used, equality \\(\\ge 0\\) instead \\(\\le 0\\). reversed future release eventually removed. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/mma.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Method of Moving Asymptotes — mma","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 1) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/mma.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Method of Moving Asymptotes — mma","text":"improved CCSA (\"conservative convex separable approximation\") variant original MMA algorithm published Svanberg 1987, become popular topology optimization.","code":""},{"path":"https://astamm.github.io/nloptr/reference/mma.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Method of Moving Asymptotes — mma","text":"“Globally convergent” mean algorithm converges global optimum; rather, means algorithm guaranteed converge local minimum feasible starting point.","code":""},{"path":"https://astamm.github.io/nloptr/reference/mma.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Method of Moving Asymptotes — mma","text":"Krister Svanberg, “class globally convergent optimization methods based conservative convex separable approximations”, SIAM J. Optim. 12 (2), p. 555-573 (2002).","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/mma.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Method of Moving Asymptotes — mma","text":"Hans W. Borchers","code":""},{"path":"https://astamm.github.io/nloptr/reference/mma.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Method of Moving Asymptotes — mma","text":"","code":"#  Solve the Hock-Schittkowski problem no. 100 with analytic gradients #  See https://apmonitor.com/wiki/uploads/Apps/hs100.apm  x0.hs100 <- c(1, 2, 0, 4, 0, 1, 1) fn.hs100 <- function(x) {(x[1] - 10) ^ 2 + 5 * (x[2] - 12) ^ 2 + x[3] ^ 4 +                          3 * (x[4] - 11) ^ 2 + 10 * x[5] ^ 6 + 7 * x[6] ^ 2 +                          x[7] ^ 4 - 4 * x[6] * x[7] - 10 * x[6] - 8 * x[7]}  hin.hs100 <- function(x) {c( 2 * x[1] ^ 2 + 3 * x[2] ^ 4 + x[3] + 4 * x[4] ^ 2 + 5 * x[5] - 127, 7 * x[1] + 3 * x[2] + 10 * x[3] ^ 2 + x[4] - x[5] - 282, 23 * x[1] + x[2] ^ 2 + 6 * x[6] ^ 2 - 8 * x[7] - 196, 4 * x[1] ^ 2 + x[2] ^ 2 - 3 * x[1] * x[2] + 2 * x[3] ^ 2 + 5 * x[6] -  11 * x[7]) }  gr.hs100 <- function(x) {  c( 2 * x[1] - 20,    10 * x[2] - 120,     4 * x[3] ^ 3,     6 * x[4] - 66,    60 * x[5] ^ 5,    14 * x[6] - 4 * x[7] - 10,     4 * x[7] ^ 3 - 4 * x[6] - 8) }  hinjac.hs100 <- function(x) {   matrix(c(4 * x[1], 12 * x[2] ^ 3, 1, 8 * x[4], 5, 0, 0,            7, 3, 20 * x[3], 1, -1, 0, 0,            23, 2 * x[2], 0, 0, 0, 12 * x[6], -8,            8 * x[1] - 3 * x[2], 2 * x[2] - 3 * x[1], 4 * x[3], 0, 0, 5, -11),            nrow = 4, byrow = TRUE) }  #  The optimum value of the objective function should be 680.6300573 #  A suitable parameter vector is roughly #  (2.330, 1.9514, -0.4775, 4.3657, -0.6245, 1.0381, 1.5942)  # Using analytic Jacobian S <- mma(x0.hs100, fn.hs100, gr = gr.hs100,       hin = hin.hs100, hinjac = hinjac.hs100,       nl.info = TRUE, control = list(xtol_rel = 1e-8),       deprecatedBehavior = FALSE) #>  #> Call: #> nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper,  #>     eval_g_ineq = hin, eval_jac_g_ineq = hinjac, opts = opts) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because  #> xtol_rel or xtol_abs (above) was reached. ) #>  #> Number of Iterations....: 229  #> Termination conditions:  stopval: -Inf\txtol_rel: 1e-08\tmaxeval: 1000\tftol_rel: 0\tftol_abs: 0  #> Number of inequality constraints:  4  #> Number of equality constraints:    0  #> Optimal value of objective function:  680.630044400453  #> Optimal value of controls: 2.3305 1.951372 -0.477538 4.365726 -0.6244892 1.038134 1.594228 #>  #>   # Using computed Jacobian S <- mma(x0.hs100, fn.hs100, hin = hin.hs100,       nl.info = TRUE, control = list(xtol_rel = 1e-8),       deprecatedBehavior = FALSE) #>  #> Call: #> nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper,  #>     eval_g_ineq = hin, eval_jac_g_ineq = hinjac, opts = opts) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because  #> xtol_rel or xtol_abs (above) was reached. ) #>  #> Number of Iterations....: 277  #> Termination conditions:  stopval: -Inf\txtol_rel: 1e-08\tmaxeval: 1000\tftol_rel: 0\tftol_abs: 0  #> Number of inequality constraints:  4  #> Number of equality constraints:    0  #> Optimal value of objective function:  680.630053840435  #> Optimal value of controls: 2.330504 1.951372 -0.477535 4.365726 -0.6244867 1.038129 1.59423 #>  #>"},{"path":"https://astamm.github.io/nloptr/reference/neldermead.html","id":null,"dir":"Reference","previous_headings":"","what":"Nelder-Mead Simplex — neldermead","title":"Nelder-Mead Simplex — neldermead","text":"implementation almost original Nelder-Mead simplex algorithm.","code":""},{"path":"https://astamm.github.io/nloptr/reference/neldermead.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nelder-Mead Simplex — neldermead","text":"","code":"neldermead(   x0,   fn,   lower = NULL,   upper = NULL,   nl.info = FALSE,   control = list(),   ... )"},{"path":"https://astamm.github.io/nloptr/reference/neldermead.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Nelder-Mead Simplex — neldermead","text":"x0 starting point searching optimum. fn objective function minimized. lower, upper lower upper bound constraints. nl.info logical; shall original NLopt info shown. control list options, see nl.opts help. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/neldermead.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Nelder-Mead Simplex — neldermead","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 0) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/neldermead.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Nelder-Mead Simplex — neldermead","text":"Provides explicit support bound constraints, using essentially method proposed Box.  Whenever new point lie outside bound constraints point moved back exactly onto constraint.","code":""},{"path":"https://astamm.github.io/nloptr/reference/neldermead.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Nelder-Mead Simplex — neldermead","text":"author NLopt tend recommend Subplex method instead.","code":""},{"path":"https://astamm.github.io/nloptr/reference/neldermead.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Nelder-Mead Simplex — neldermead","text":"J. . Nelder R. Mead, “simplex method function minimization,” Computer Journal 7, p. 308-313 (1965). M. J. Box, “new method constrained optimization comparison methods,” Computer J. 8 (1), 42-52 (1965).","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/neldermead.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Nelder-Mead Simplex — neldermead","text":"Hans W. Borchers","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/newuoa.html","id":null,"dir":"Reference","previous_headings":"","what":"New Unconstrained Optimization with quadratic Approximation — newuoa","title":"New Unconstrained Optimization with quadratic Approximation — newuoa","text":"NEWUOA solves quadratic subproblems spherical trust region via truncated conjugate-gradient algorithm. bound-constrained problems, BOBYQA used instead, Powell developed enhancement thereof bound constraints.","code":""},{"path":"https://astamm.github.io/nloptr/reference/newuoa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"New Unconstrained Optimization with quadratic Approximation — newuoa","text":"","code":"newuoa(x0, fn, nl.info = FALSE, control = list(), ...)"},{"path":"https://astamm.github.io/nloptr/reference/newuoa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"New Unconstrained Optimization with quadratic Approximation — newuoa","text":"x0 starting point searching optimum. fn objective function minimized. nl.info logical; shall original NLopt info shown. control list options, see nl.opts help. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/newuoa.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"New Unconstrained Optimization with quadratic Approximation — newuoa","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 0) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/newuoa.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"New Unconstrained Optimization with quadratic Approximation — newuoa","text":"algorithm derived NEWUOA Fortran subroutine Powell, converted C modified NLopt stopping criteria.","code":""},{"path":"https://astamm.github.io/nloptr/reference/newuoa.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"New Unconstrained Optimization with quadratic Approximation — newuoa","text":"NEWUOA may largely superseded BOBYQA.","code":""},{"path":"https://astamm.github.io/nloptr/reference/newuoa.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"New Unconstrained Optimization with quadratic Approximation — newuoa","text":"M. J. D. Powell. “BOBYQA algorithm bound constrained optimization without derivatives,” Department Applied Mathematics Theoretical Physics, Cambridge England, technical reportNA2009/06 (2009).","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/newuoa.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"New Unconstrained Optimization with quadratic Approximation — newuoa","text":"Hans W. Borchers","code":""},{"path":"https://astamm.github.io/nloptr/reference/newuoa.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"New Unconstrained Optimization with quadratic Approximation — newuoa","text":"","code":"## Rosenbrock Banana function  rbf <- function(x) {(1 - x[1]) ^ 2 + 100 * (x[2] - x[1] ^ 2) ^ 2}  S <- newuoa(c(1, 2), rbf)  ## The function as written above has a minimum of 0 at (1, 1)  S #> $par #> [1] 1 1 #>  #> $value #> [1] 0 #>  #> $iter #> [1] 33 #>  #> $convergence #> [1] 1 #>  #> $message #> [1] \"NLOPT_SUCCESS: Generic success return value.\" #>"},{"path":"https://astamm.github.io/nloptr/reference/nl.grad.html","id":null,"dir":"Reference","previous_headings":"","what":"Numerical Gradients and Jacobians — nl.grad","title":"Numerical Gradients and Jacobians — nl.grad","text":"Provides numerical gradients Jacobians.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nl.grad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Numerical Gradients and Jacobians — nl.grad","text":"","code":"nl.grad(x0, fn, heps = .Machine$double.eps^(1/3), ...)"},{"path":"https://astamm.github.io/nloptr/reference/nl.grad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Numerical Gradients and Jacobians — nl.grad","text":"x0 point vector gradient calculated. fn scalar function one several variables. heps step size used. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nl.grad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Numerical Gradients and Jacobians — nl.grad","text":"grad returns gradient vector; jacobian returns Jacobian matrix usual dimensions.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nl.grad.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Numerical Gradients and Jacobians — nl.grad","text":"functions apply “central difference formula” step size recommended literature.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nl.grad.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Numerical Gradients and Jacobians — nl.grad","text":"Hans W. Borchers","code":""},{"path":"https://astamm.github.io/nloptr/reference/nl.grad.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Numerical Gradients and Jacobians — nl.grad","text":"","code":"fn1 <- function(x) sum(x ^ 2)   nl.grad(seq(0, 1, by = 0.2), fn1) #> [1] 0.0 0.4 0.8 1.2 1.6 2.0   ## [1] 0.0  0.4  0.8  1.2  1.6  2.0   nl.grad(rep(1, 5), fn1) #> [1] 2 2 2 2 2   ## [1] 2  2  2  2  2    fn2 <- function(x) c(sin(x), cos(x))   x <- (0:1) * 2 * pi   nl.jacobian(x, fn2) #>      [,1] [,2] #> [1,]    1    0 #> [2,]    0    1 #> [3,]    0    0 #> [4,]    0    0   ##    [,1] [,2]   ## [1,]  1  0   ## [2,]  0  1   ## [3,]  0  0   ## [4,]  0  0"},{"path":"https://astamm.github.io/nloptr/reference/nl.opts.html","id":null,"dir":"Reference","previous_headings":"","what":"Setting NL Options — nl.opts","title":"Setting NL Options — nl.opts","text":"Sets changes NLOPT options.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nl.opts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Setting NL Options — nl.opts","text":"","code":"nl.opts(optlist = NULL)"},{"path":"https://astamm.github.io/nloptr/reference/nl.opts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Setting NL Options — nl.opts","text":"optlist list options, see .","code":""},{"path":"https://astamm.github.io/nloptr/reference/nl.opts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Setting NL Options — nl.opts","text":"returns list default changed options.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nl.opts.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Setting NL Options — nl.opts","text":"following options can set (default values): stopval = -Inf, # stop minimization valuextol_rel = 1e-6, # stop small optimization stepmaxeval = 1000, # stop many function evaluationsftol_rel = 0.0, # stop change times function valueftol_abs = 0.0, # stop small change function valuecheck_derivatives = FALSE","code":""},{"path":"https://astamm.github.io/nloptr/reference/nl.opts.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Setting NL Options — nl.opts","text":"options can set solvers NLOPT. set wrapper functions. see full list options algorithms, type nloptr.print.options().","code":""},{"path":"https://astamm.github.io/nloptr/reference/nl.opts.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Setting NL Options — nl.opts","text":"Hans W. Borchers","code":""},{"path":"https://astamm.github.io/nloptr/reference/nl.opts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Setting NL Options — nl.opts","text":"","code":"nl.opts(list(xtol_rel = 1e-8, maxeval = 2000)) #> $stopval #> [1] -Inf #>  #> $xtol_rel #> [1] 1e-08 #>  #> $maxeval #> [1] 2000 #>  #> $ftol_rel #> [1] 0 #>  #> $ftol_abs #> [1] 0 #>  #> $check_derivatives #> [1] FALSE #>  #> $algorithm #> NULL #>"},{"path":"https://astamm.github.io/nloptr/reference/nloptr-package.html","id":null,"dir":"Reference","previous_headings":"","what":"R interface to NLopt — nloptr-package","title":"R interface to NLopt — nloptr-package","text":"nloptr R interface NLopt, free/open-source library nonlinear optimization started Steven G. Johnson, providing common interface number different free optimization routines available online well original implementations various algorithms. NLopt library available GNU Lesser General Public License (LGPL), copyrights owned variety authors. information taken NLopt website, details available.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nloptr-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"R interface to NLopt — nloptr-package","text":"NLopt addresses general nonlinear optimization problems form: $$\\min f(x)\\quad x\\R^n$$ $$\\textrm{s.t. }\\\\ g(x) \\leq 0\\\\ h(x) = 0\\\\ lb \\leq x \\leq ub$$ \\(f(x)\\) objective function minimized \\(x\\) represents \\(n\\) optimization parameters. problem may optionally subject bound constraints (also called box constraints), \\(lb\\) \\(ub\\). partially totally unconstrained problems bounds can take -Inf Inf. One may also optionally \\(m\\) nonlinear inequality constraints (sometimes called nonlinear programming problem), can specified \\(g(x)\\), equality constraints can specified \\(h(x)\\). Note algorithms NLopt can handle constraints. optimization problem can solved general nloptr interface, using one wrapper functions separate algorithms; auglag, bobyqa, ccsaq, cobyla, crs2lm, direct, directL, isres, lbfgs, mlsl, mma, neldermead, newuoa, sbplx, slsqp, stogo, tnewton, varmetric.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nloptr-package.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"R interface to NLopt — nloptr-package","text":"See ?nloptr examples.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nloptr-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"R interface to NLopt — nloptr-package","text":"Steven G. Johnson, NLopt nonlinear-optimization package, https://nlopt.readthedocs.io/en/latest/","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/nloptr-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"R interface to NLopt — nloptr-package","text":"Steven G. Johnson others (C code)  Jelmer Ypma (R interface)  Hans W. Borchers (wrappers)","code":""},{"path":"https://astamm.github.io/nloptr/reference/nloptr-package.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"R interface to NLopt — nloptr-package","text":"","code":"# Example problem, number 71 from the Hock-Schittkowsky test suite. # # \\min_{x} x1 * x4 * (x1 + x2 + x3) + x3 # s.t. #    x1 * x2 * x3 * x4 >= 25 #    x1 ^ 2 + x2 ^ 2 + x3 ^ 2 + x4 ^ 2 = 40 #    1 <= x1, x2, x3, x4 <= 5 # # we re-write the inequality as #   25 - x1 * x2 * x3 * x4 <= 0 # # and the equality as #   x1 ^ 2 + x2 ^ 2 + x3 ^ 2 + x4 ^ 2 - 40 = 0 # # x0 = (1, 5, 5, 1) # # optimal solution = (1.000000, 4.742999, 3.821151, 1.379408)  library('nloptr')  # # f(x) = x1 * x4 * (x1 + x2 + x3) + x3 # eval_f <- function(x) {     list(\"objective\" = x[1] * x[4] * (x[1] + x[2] + x[3]) + x[3],          \"gradient\" = c(x[1] * x[4] + x[4] * (x[1] + x[2] + x[3]),                         x[1] * x[4],                         x[1] * x[4] + 1.0,                         x[1] * (x[1] + x[2] + x[3]))) }  # constraint functions # inequalities eval_g_ineq <- function(x) {     constr <- c(25 - x[1] * x[2] * x[3] * x[4])      grad   <- c(-x[2] * x[3] * x[4],                 -x[1] * x[3] * x[4],                 -x[1] * x[2] * x[4],                 -x[1] * x[2] * x[3] )     list(\"constraints\" = constr, \"jacobian\" = grad) }  # equalities eval_g_eq <- function(x) {     constr <- c(x[1] ^ 2 + x[2] ^ 2 + x[3] ^ 2 + x[4] ^ 2 - 40)      grad <- c(2.0 * x[1],               2.0 * x[2],               2.0 * x[3],               2.0 * x[4])     list(\"constraints\" = constr, \"jacobian\" = grad) }  # initial values x0 <- c(1, 5, 5, 1)  # lower and upper bounds of control lb <- c(1, 1, 1, 1) ub <- c(5, 5, 5, 5)   local_opts <- list(\"algorithm\" = \"NLOPT_LD_MMA\", \"xtol_rel\"  = 1.0e-7) opts <- list(\"algorithm\"  = \"NLOPT_LD_AUGLAG\",              \"xtol_rel\"   = 1.0e-7,              \"maxeval\"    = 1000,              \"local_opts\" = local_opts)  res <- nloptr(x0 = x0,               eval_f = eval_f,               lb = lb,               ub = ub,               eval_g_ineq = eval_g_ineq,               eval_g_eq = eval_g_eq,               opts = opts) print(res) #>  #> Call: #>  #> nloptr(x0 = x0, eval_f = eval_f, lb = lb, ub = ub, eval_g_ineq = eval_g_ineq,  #>     eval_g_eq = eval_g_eq, opts = opts) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because  #> xtol_rel or xtol_abs (above) was reached. ) #>  #> Number of Iterations....: 476  #> Termination conditions:  xtol_rel: 1e-07\tmaxeval: 1000  #> Number of inequality constraints:  1  #> Number of equality constraints:    1  #> Optimal value of objective function:  17.0140172892472  #> Optimal value of controls: 1 4.742999 3.821151 1.379408 #>  #>"},{"path":"https://astamm.github.io/nloptr/reference/nloptr.get.default.options.html","id":null,"dir":"Reference","previous_headings":"","what":"Return a data.frame with all the options that can be supplied to nloptr. — nloptr.get.default.options","title":"Return a data.frame with all the options that can be supplied to nloptr. — nloptr.get.default.options","text":"function returns data.frame options can supplied nloptr. data.frame contains default values options explanation. user-friendly way show options using function nloptr.print.options.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nloptr.get.default.options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return a data.frame with all the options that can be supplied to nloptr. — nloptr.get.default.options","text":"","code":"nloptr.get.default.options()"},{"path":"https://astamm.github.io/nloptr/reference/nloptr.get.default.options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return a data.frame with all the options that can be supplied to nloptr. — nloptr.get.default.options","text":"return value contains data.frame following elements name name option type type (numeric, logical, integer, character) possible_values string explaining values option can take default default value option (string) is_termination_condition option part termination conditions? description description option (taken NLopt website option passed NLopt).","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/nloptr.get.default.options.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Return a data.frame with all the options that can be supplied to nloptr. — nloptr.get.default.options","text":"Jelmer Ypma","code":""},{"path":"https://astamm.github.io/nloptr/reference/nloptr.html","id":null,"dir":"Reference","previous_headings":"","what":"R interface to NLopt — nloptr","title":"R interface to NLopt — nloptr","text":"nloptr R interface NLopt, free/open-source library nonlinear optimization started Steven G. Johnson, providing common interface number different free optimization routines available online well original implementations various algorithms. NLopt library available GNU Lesser General Public License (LGPL), copyrights owned variety authors. information taken NLopt website, details available.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nloptr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"R interface to NLopt — nloptr","text":"","code":"nloptr(   x0,   eval_f,   eval_grad_f = NULL,   lb = NULL,   ub = NULL,   eval_g_ineq = NULL,   eval_jac_g_ineq = NULL,   eval_g_eq = NULL,   eval_jac_g_eq = NULL,   opts = list(),   ... )"},{"path":"https://astamm.github.io/nloptr/reference/nloptr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"R interface to NLopt — nloptr","text":"x0 vector starting values optimization. eval_f function returns value objective function. can also return gradient information time list elements \"objective\" \"gradient\" (see example). eval_grad_f function returns value gradient objective function. algorithms require gradient. lb vector lower bounds controls (use -Inf controls without lower bound), default lower bounds controls. ub vector upper bounds controls (use Inf controls without upper bound), default upper bounds controls. eval_g_ineq function evaluate (non-)linear inequality constraints hold solution.  can also return gradient information time list elements \"constraints\" \"jacobian\" (see example). eval_jac_g_ineq function evaluate Jacobian (non-)linear inequality constraints hold solution. eval_g_eq function evaluate (non-)linear equality constraints hold solution.  can also return gradient information time list elements \"constraints\" \"jacobian\" (see example). eval_jac_g_eq function evaluate Jacobian (non-)linear equality constraints hold solution. opts list options. option \"algorithm\" required. Check NLopt website full list available algorithms. options control termination conditions (minf_max, ftol_rel, ftol_abs, xtol_rel, xtol_abs, maxeval, maxtime). Default xtol_rel = 1e-4. information . #nolint full description options shown function nloptr.print.options(). algorithms equality constraints require option local_opts, contains list algorithm termination condition local algorithm. See ?`nloptr-package` example. option print_level controls much output shown optimization process. Possible values: option check_derivatives (default = FALSE) can used run compare analytic gradients finite difference approximations. option check_derivatives_print ('' (default), 'errors', 'none') controls output derivative checker, run, showing comparisons, resulted error, none.  option check_derivatives_tol (default = 1e-04), determines difference analytic gradient finite difference approximation flagged error. ... arguments passed user-defined objective constraints functions.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nloptr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"R interface to NLopt — nloptr","text":"return value contains list inputs, additional elements call call made solve status integer value status optimization (0 success) message informative message status optimization iterations number iterations executed objective value objective function solution solution optimal value controls version version NLopt used","code":""},{"path":"https://astamm.github.io/nloptr/reference/nloptr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"R interface to NLopt — nloptr","text":"NLopt addresses general nonlinear optimization problems form: $$\\min f(x)\\quad x\\R^n$$ $$\\textrm{s.t. }\\\\ g(x) \\leq 0\\\\ h(x) = 0\\\\ lb \\leq x \\leq ub$$ \\(f(x)\\) objective function minimized \\(x\\) represents \\(n\\) optimization parameters. problem may optionally subject bound constraints (also called box constraints), \\(lb\\) \\(ub\\). partially totally unconstrained problems bounds can take -Inf Inf. One may also optionally \\(m\\) nonlinear inequality constraints (sometimes called nonlinear programming problem), can specified \\(g(x)\\), equality constraints can specified \\(h(x)\\). Note algorithms NLopt can handle constraints.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nloptr.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"R interface to NLopt — nloptr","text":"See ?`nloptr-package` extended example.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nloptr.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"R interface to NLopt — nloptr","text":"Steven G. Johnson, NLopt nonlinear-optimization package, https://github.com/stevengj/nlopt","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/nloptr.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"R interface to NLopt — nloptr","text":"Steven G. Johnson others (C code)  Jelmer Ypma (R interface)","code":""},{"path":"https://astamm.github.io/nloptr/reference/nloptr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"R interface to NLopt — nloptr","text":"","code":"library('nloptr')  ## Rosenbrock Banana function and gradient in separate functions eval_f <- function(x) {   return(100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2) }  eval_grad_f <- function(x) {   return(c(-400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),         200 * (x[2] - x[1] * x[1]))) }   # initial values x0 <- c(-1.2, 1)  opts <- list(\"algorithm\"=\"NLOPT_LD_LBFGS\",        \"xtol_rel\"=1.0e-8)  # solve Rosenbrock Banana function res <- nloptr(x0=x0,        eval_f=eval_f,        eval_grad_f=eval_grad_f,        opts=opts) print(res) #>  #> Call: #> nloptr(x0 = x0, eval_f = eval_f, eval_grad_f = eval_grad_f, opts = opts) #>  #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success return value. ) #>  #> Number of Iterations....: 56  #> Termination conditions:  xtol_rel: 1e-08  #> Number of inequality constraints:  0  #> Number of equality constraints:    0  #> Optimal value of objective function:  7.35727226897802e-23  #> Optimal value of controls: 1 1 #>  #>    ## Rosenbrock Banana function and gradient in one function # this can be used to economize on calculations eval_f_list <- function(x) {   return(   list(     \"objective\" = 100 * (x[2] - x[1] * x[1]) ^ 2 + (1 - x[1]) ^ 2,     \"gradient\"  = c(-400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),             200 * (x[2] - x[1] * x[1])))) }  # solve Rosenbrock Banana function using an objective function that # returns a list with the objective value and its gradient res <- nloptr(x0=x0,        eval_f=eval_f_list,        opts=opts) print(res) #>  #> Call: #> nloptr(x0 = x0, eval_f = eval_f_list, opts = opts) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success return value. ) #>  #> Number of Iterations....: 56  #> Termination conditions:  xtol_rel: 1e-08  #> Number of inequality constraints:  0  #> Number of equality constraints:    0  #> Optimal value of objective function:  7.35727226897802e-23  #> Optimal value of controls: 1 1 #>  #>     # Example showing how to solve the problem from the NLopt tutorial. # # min sqrt(x2) # s.t. x2 >= 0 #    x2 >= (a1*x1 + b1)^3 #    x2 >= (a2*x1 + b2)^3 # where # a1 = 2, b1 = 0, a2 = -1, b2 = 1 # # re-formulate constraints to be of form g(x) <= 0 #    (a1*x1 + b1)^3 - x2 <= 0 #    (a2*x1 + b2)^3 - x2 <= 0  library('nloptr')   # objective function eval_f0 <- function(x, a, b) {   return(sqrt(x[2])) }  # constraint function eval_g0 <- function(x, a, b) {   return((a*x[1] + b)^3 - x[2]) }  # gradient of objective function eval_grad_f0 <- function(x, a, b) {   return(c(0, .5/sqrt(x[2]))) }  # Jacobian of constraint eval_jac_g0 <- function(x, a, b) {   return(rbind(c(3*a[1]*(a[1]*x[1] + b[1])^2, -1.0),          c(3*a[2]*(a[2]*x[1] + b[2])^2, -1.0))) }   # functions with gradients in objective and constraint function # this can be useful if the same calculations are needed for # the function value and the gradient eval_f1 <- function(x, a, b) {   return(list(\"objective\"=sqrt(x[2]),          \"gradient\"=c(0,.5/sqrt(x[2])))) }  eval_g1 <- function(x, a, b) {   return(list(\"constraints\"=(a*x[1] + b)^3 - x[2],           \"jacobian\"=rbind(c(3*a[1]*(a[1]*x[1] + b[1])^2, -1.0),                   c(3*a[2]*(a[2]*x[1] + b[2])^2, -1.0)))) }   # define parameters a <- c(2,-1) b <- c(0, 1)  # Solve using NLOPT_LD_MMA with gradient information supplied in separate # function. res0 <- nloptr(x0=c(1.234,5.678),         eval_f=eval_f0,         eval_grad_f=eval_grad_f0,         lb = c(-Inf,0),         ub = c(Inf,Inf),         eval_g_ineq = eval_g0,         eval_jac_g_ineq = eval_jac_g0,         opts = list(\"algorithm\"=\"NLOPT_LD_MMA\"),         a = a,         b = b) #> Warning: No termination criterion specified, using default(relative x-tolerance = 1e-04) print(res0) #>  #> Call: #>  #> nloptr(x0 = c(1.234, 5.678), eval_f = eval_f0, eval_grad_f = eval_grad_f0,  #>     lb = c(-Inf, 0), ub = c(Inf, Inf), eval_g_ineq = eval_g0,  #>     eval_jac_g_ineq = eval_jac_g0, opts = list(algorithm = \"NLOPT_LD_MMA\"),  #>     a = a, b = b) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because  #> xtol_rel or xtol_abs (above) was reached. ) #>  #> Number of Iterations....: 11  #> Termination conditions:  relative x-tolerance = 1e-04 (DEFAULT)  #> Number of inequality constraints:  2  #> Number of equality constraints:    0  #> Optimal value of objective function:  0.54433104762009  #> Optimal value of controls: 0.3333333 0.2962963 #>  #>   # Solve using NLOPT_LN_COBYLA without gradient information res1 <- nloptr(x0=c(1.234,5.678),         eval_f=eval_f0,         lb = c(-Inf, 0),         ub = c(Inf, Inf),         eval_g_ineq = eval_g0,         opts = list(\"algorithm\" = \"NLOPT_LN_COBYLA\"),         a = a,         b = b) #> Warning: No termination criterion specified, using default(relative x-tolerance = 1e-04) print(res1) #>  #> Call: #> nloptr(x0 = c(1.234, 5.678), eval_f = eval_f0, lb = c(-Inf, 0),  #>     ub = c(Inf, Inf), eval_g_ineq = eval_g0, opts = list(algorithm = \"NLOPT_LN_COBYLA\"),  #>     a = a, b = b) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because  #> xtol_rel or xtol_abs (above) was reached. ) #>  #> Number of Iterations....: 31  #> Termination conditions:  relative x-tolerance = 1e-04 (DEFAULT)  #> Number of inequality constraints:  2  #> Number of equality constraints:    0  #> Optimal value of objective function:  0.544242301658176  #> Optimal value of controls: 0.3333292 0.2961997 #>  #>    # Solve using NLOPT_LD_MMA with gradient information in objective function res2 <- nloptr(x0=c(1.234, 5.678),         eval_f=eval_f1,         lb = c(-Inf, 0),         ub = c(Inf, Inf),         eval_g_ineq = eval_g1,         opts = list(\"algorithm\"=\"NLOPT_LD_MMA\",               \"check_derivatives\" = TRUE),         a = a,         b = b) #> Warning: No termination criterion specified, using default(relative x-tolerance = 1e-04) #> Checking gradients of objective function. #> Derivative checker results: 0 error(s) detected. #>  #>   eval_grad_f[1] = 0.000000e+00 ~ 0.000000e+00   [0.000000e+00] #>   eval_grad_f[2] = 2.098323e-01 ~ 2.098323e-01   [1.422937e-09] #>  #> Checking gradients of inequality constraints. #> Derivative checker results: 0 error(s) detected. #>  #>   eval_jac_g_ineq[1, 1] =  3.654614e+01 ~  3.654614e+01   [1.667794e-08] #>   eval_jac_g_ineq[2, 1] = -1.642680e-01 ~ -1.642680e-01   [2.103453e-07] #>   eval_jac_g_ineq[1, 2] = -1.000000e+00 ~ -1.000000e+00   [0.000000e+00] #>   eval_jac_g_ineq[2, 2] = -1.000000e+00 ~ -1.000000e+00   [0.000000e+00] #>  print(res2) #>  #> Call: #> nloptr(x0 = c(1.234, 5.678), eval_f = eval_f1, lb = c(-Inf, 0),  #>     ub = c(Inf, Inf), eval_g_ineq = eval_g1, opts = list(algorithm = \"NLOPT_LD_MMA\",  #>         check_derivatives = TRUE), a = a, b = b) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because  #> xtol_rel or xtol_abs (above) was reached. ) #>  #> Number of Iterations....: 11  #> Termination conditions:  relative x-tolerance = 1e-04 (DEFAULT)  #> Number of inequality constraints:  2  #> Number of equality constraints:    0  #> Optimal value of objective function:  0.54433104762009  #> Optimal value of controls: 0.3333333 0.2962963 #>  #>"},{"path":"https://astamm.github.io/nloptr/reference/nloptr.print.options.html","id":null,"dir":"Reference","previous_headings":"","what":"Print description of nloptr options — nloptr.print.options","title":"Print description of nloptr options — nloptr.print.options","text":"function prints list options can set solving minimization problem using nloptr.","code":""},{"path":"https://astamm.github.io/nloptr/reference/nloptr.print.options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print description of nloptr options — nloptr.print.options","text":"","code":"nloptr.print.options(opts.show = NULL, opts.user = NULL)"},{"path":"https://astamm.github.io/nloptr/reference/nloptr.print.options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print description of nloptr options — nloptr.print.options","text":"opts.show list vector names options. description shown options list. default, description options shown. opts.user object containing user supplied options. argument optional. used nloptr.print.options called nloptr. case options listed print_options_doc set TRUE passing minimization problem nloptr.","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/nloptr.print.options.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print description of nloptr options — nloptr.print.options","text":"Jelmer Ypma","code":""},{"path":"https://astamm.github.io/nloptr/reference/nloptr.print.options.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print description of nloptr options — nloptr.print.options","text":"","code":"library('nloptr') nloptr.print.options() #> algorithm #> \tpossible values: NLOPT_GN_DIRECT, NLOPT_GN_DIRECT_L, #> \t         NLOPT_GN_DIRECT_L_RAND, NLOPT_GN_DIRECT_NOSCAL, #> \t         NLOPT_GN_DIRECT_L_NOSCAL, #> \t         NLOPT_GN_DIRECT_L_RAND_NOSCAL, #> \t         NLOPT_GN_ORIG_DIRECT, NLOPT_GN_ORIG_DIRECT_L, #> \t         NLOPT_GD_STOGO, NLOPT_GD_STOGO_RAND, #> \t         NLOPT_LD_SLSQP, NLOPT_LD_LBFGS, NLOPT_LN_PRAXIS, #> \t         NLOPT_LD_VAR1, NLOPT_LD_VAR2, NLOPT_LD_TNEWTON, #> \t         NLOPT_LD_TNEWTON_RESTART, #> \t         NLOPT_LD_TNEWTON_PRECOND, #> \t         NLOPT_LD_TNEWTON_PRECOND_RESTART, #> \t         NLOPT_GN_CRS2_LM, NLOPT_GN_MLSL, NLOPT_GD_MLSL, #> \t         NLOPT_GN_MLSL_LDS, NLOPT_GD_MLSL_LDS, #> \t         NLOPT_LD_MMA, NLOPT_LD_CCSAQ, NLOPT_LN_COBYLA, #> \t         NLOPT_LN_NEWUOA, NLOPT_LN_NEWUOA_BOUND, #> \t         NLOPT_LN_NELDERMEAD, NLOPT_LN_SBPLX, #> \t         NLOPT_LN_AUGLAG, NLOPT_LD_AUGLAG, #> \t         NLOPT_LN_AUGLAG_EQ, NLOPT_LD_AUGLAG_EQ, #> \t         NLOPT_LN_BOBYQA, NLOPT_GN_ISRES #> \tdefault value:   none #>  #> \tThis option is required. Check the NLopt website for a description of #> \tthe algorithms. #>  #> stopval #> \tpossible values: -Inf <= stopval <= Inf #> \tdefault value:   -Inf #>  #> \tStop minimization when an objective value <= stopval is found. #> \tSetting stopval to -Inf disables this stopping criterion (default). #>  #> ftol_rel #> \tpossible values: ftol_rel > 0 #> \tdefault value:   0.0 #>  #> \tStop when an optimization step (or an estimate of the optimum) #> \tchanges the objective function value by less than ftol_rel multiplied #> \tby the absolute value of the function value. If there is any chance #> \tthat your optimum function value is close to zero, you might want to #> \tset an absolute tolerance with ftol_abs as well. Criterion is #> \tdisabled if ftol_rel is non-positive (default). #>  #> ftol_abs #> \tpossible values: ftol_abs > 0 #> \tdefault value:   0.0 #>  #> \tStop when an optimization step (or an estimate of the optimum) #> \tchanges the function value by less than ftol_abs. Criterion is #> \tdisabled if ftol_abs is non-positive (default). #>  #> xtol_rel #> \tpossible values: xtol_rel > 0 #> \tdefault value:   1.0e-04 #>  #> \tStop when an optimization step (or an estimate of the optimum) #> \tchanges every parameter by less than xtol_rel multiplied by the #> \tabsolute value of the parameter. If there is any chance that an #> \toptimal parameter is close to zero, you might want to set an absolute #> \ttolerance with xtol_abs as well. Criterion is disabled if xtol_rel is #> \tnon-positive. #>  #> xtol_abs #> \tpossible values: xtol_abs > 0 #> \tdefault value:   rep(0.0, length(x0)) #>  #> \txtol_abs is a vector of length n (the number of elements in x) giving #> \tthe tolerances: stop when an optimization step (or an estimate of the #> \toptimum) changes every parameter x[i] by less than xtol_abs[i]. #> \tCriterion is disabled if all elements of xtol_abs are non-positive #> \t(default). #>  #> maxeval #> \tpossible values: maxeval is a positive integer #> \tdefault value:   100 #>  #> \tStop when the number of function evaluations exceeds maxeval. This is #> \tnot a strict maximum: the number of function evaluations may exceed #> \tmaxeval slightly, depending upon the algorithm. Criterion is disabled #> \tif maxeval is non-positive. #>  #> maxtime #> \tpossible values: maxtime > 0 #> \tdefault value:   -1.0 #>  #> \tStop when the optimization time (in seconds) exceeds maxtime. This is #> \tnot a strict maximum: the time may exceed maxtime slightly, depending #> \tupon the algorithm and on how slow your function evaluation is. #> \tCriterion is disabled if maxtime is non-positive (default). #>  #> tol_constraints_ineq #> \tpossible values: tol_constraints_ineq > 0.0 #> \tdefault value:   rep(1e-8, num_constraints_ineq) #>  #> \tThe parameter tol_constraints_ineq is a vector of tolerances. Each #> \ttolerance corresponds to one of the inequality constraints. The #> \ttolerance is used for the purpose of stopping criteria only: a point #> \tx is considered feasible for judging whether to stop the optimization #> \tif eval_g_ineq(x) <= tol. A tolerance of zero means that NLopt will #> \ttry not to consider any x to be converged unless eval_g_ineq(x) is #> \tstrictly non-positive; generally, at least a small positive tolerance #> \tis advisable to reduce sensitivity to rounding errors. By default the #> \ttolerances for all inequality constraints are set to 1e-8. #>  #> tol_constraints_eq #> \tpossible values: tol_constraints_eq > 0.0 #> \tdefault value:   rep(1e-8, num_constraints_eq) #>  #> \tThe parameter tol_constraints_eq is a vector of tolerances. Each #> \ttolerance corresponds to one of the equality constraints. The #> \ttolerance is used for the purpose of stopping criteria only: a point #> \tx is considered feasible for judging whether to stop the optimization #> \tif abs(eval_g_ineq(x)) <= tol. For equality constraints, a small #> \tpositive tolerance is strongly advised in order to allow NLopt to #> \tconverge even if the equality constraint is slightly nonzero. By #> \tdefault the tolerances for all quality constraints are set to 1e-8. #>  #> print_level #> \tpossible values: 0, 1, 2, or 3 #> \tdefault value:   0 #>  #> \tThe option print_level controls how much output is shown during the #> \toptimization process. Possible values: 0 (default): no output; 1: #> \tshow iteration number and value of objective function; 2: 1 + show #> \tvalue of (in)equalities; 3: 2 + show value of controls. #>  #> check_derivatives #> \tpossible values: TRUE or FALSE #> \tdefault value:   FALSE #>  #> \tThe option check_derivatives can be activated to compare the #> \tuser-supplied analytic gradients with finite difference #> \tapproximations. #>  #> check_derivatives_tol #> \tpossible values: check_derivatives_tol > 0.0 #> \tdefault value:   1e-04 #>  #> \tThe option check_derivatives_tol determines when a difference between #> \tan analytic gradient and its finite difference approximation is #> \tflagged as an error. #>  #> check_derivatives_print #> \tpossible values: 'none', 'all', 'errors', #> \tdefault value:   all #>  #> \tThe option check_derivatives_print controls the output of the #> \tderivative checker (if check_derivatives == TRUE). All comparisons #> \tare shown ('all'), only those comparisions that resulted in an error #> \t('error'), or only the number of errors is shown ('none'). #>  #> print_options_doc #> \tpossible values: TRUE or FALSE #> \tdefault value:   FALSE #>  #> \tIf TRUE, a description of all options and their current and default #> \tvalues is printed to the screen. #>  #> population #> \tpossible values: population is a positive integer #> \tdefault value:   0 #>  #> \tSeveral of the stochastic search algorithms (e.g., CRS, MLSL, and #> \tISRES) start by generating some initial population of random points #> \tx. By default, this initial population size is chosen heuristically #> \tin some algorithm-specific way, but the initial population can by #> \tchanged by setting a positive integer value for population. A #> \tpopulation of zero implies that the heuristic default will be used. #>  #> vector_storage #> \tpossible values: vector_storage is a positive integer #> \tdefault value:   20 #>  #> \tNumber of gradients to remember from previous optimization steps. #>  #> ranseed #> \tpossible values: ranseed is a positive integer #> \tdefault value:   0 #>  #> \tFor stochastic optimization algorithms, pseudorandom numbers are #> \tgenerated. Set the random seed using ranseed if you want to use a #> \t'deterministic' sequence of pseudorandom numbers, i.e. the same #> \tsequence from run to run. If ranseed is 0 (default), the seed for the #> \trandom numbers is generated from the system time, so that you will #> \tget a different sequence of pseudorandom numbers each time you run #> \tyour program. #>   nloptr.print.options(opts.show = c(\"algorithm\", \"check_derivatives\")) #> algorithm #> \tpossible values: NLOPT_GN_DIRECT, NLOPT_GN_DIRECT_L, #> \t         NLOPT_GN_DIRECT_L_RAND, NLOPT_GN_DIRECT_NOSCAL, #> \t         NLOPT_GN_DIRECT_L_NOSCAL, #> \t         NLOPT_GN_DIRECT_L_RAND_NOSCAL, #> \t         NLOPT_GN_ORIG_DIRECT, NLOPT_GN_ORIG_DIRECT_L, #> \t         NLOPT_GD_STOGO, NLOPT_GD_STOGO_RAND, #> \t         NLOPT_LD_SLSQP, NLOPT_LD_LBFGS, NLOPT_LN_PRAXIS, #> \t         NLOPT_LD_VAR1, NLOPT_LD_VAR2, NLOPT_LD_TNEWTON, #> \t         NLOPT_LD_TNEWTON_RESTART, #> \t         NLOPT_LD_TNEWTON_PRECOND, #> \t         NLOPT_LD_TNEWTON_PRECOND_RESTART, #> \t         NLOPT_GN_CRS2_LM, NLOPT_GN_MLSL, NLOPT_GD_MLSL, #> \t         NLOPT_GN_MLSL_LDS, NLOPT_GD_MLSL_LDS, #> \t         NLOPT_LD_MMA, NLOPT_LD_CCSAQ, NLOPT_LN_COBYLA, #> \t         NLOPT_LN_NEWUOA, NLOPT_LN_NEWUOA_BOUND, #> \t         NLOPT_LN_NELDERMEAD, NLOPT_LN_SBPLX, #> \t         NLOPT_LN_AUGLAG, NLOPT_LD_AUGLAG, #> \t         NLOPT_LN_AUGLAG_EQ, NLOPT_LD_AUGLAG_EQ, #> \t         NLOPT_LN_BOBYQA, NLOPT_GN_ISRES #> \tdefault value:   none #>  #> \tThis option is required. Check the NLopt website for a description of #> \tthe algorithms. #>  #> check_derivatives #> \tpossible values: TRUE or FALSE #> \tdefault value:   FALSE #>  #> \tThe option check_derivatives can be activated to compare the #> \tuser-supplied analytic gradients with finite difference #> \tapproximations. #>   opts <- list(\"algorithm\"=\"NLOPT_LD_LBFGS\",        \"xtol_rel\"=1.0e-8) nloptr.print.options(opts.user = opts) #> algorithm #> \tpossible values: NLOPT_GN_DIRECT, NLOPT_GN_DIRECT_L, #> \t         NLOPT_GN_DIRECT_L_RAND, NLOPT_GN_DIRECT_NOSCAL, #> \t         NLOPT_GN_DIRECT_L_NOSCAL, #> \t         NLOPT_GN_DIRECT_L_RAND_NOSCAL, #> \t         NLOPT_GN_ORIG_DIRECT, NLOPT_GN_ORIG_DIRECT_L, #> \t         NLOPT_GD_STOGO, NLOPT_GD_STOGO_RAND, #> \t         NLOPT_LD_SLSQP, NLOPT_LD_LBFGS, NLOPT_LN_PRAXIS, #> \t         NLOPT_LD_VAR1, NLOPT_LD_VAR2, NLOPT_LD_TNEWTON, #> \t         NLOPT_LD_TNEWTON_RESTART, #> \t         NLOPT_LD_TNEWTON_PRECOND, #> \t         NLOPT_LD_TNEWTON_PRECOND_RESTART, #> \t         NLOPT_GN_CRS2_LM, NLOPT_GN_MLSL, NLOPT_GD_MLSL, #> \t         NLOPT_GN_MLSL_LDS, NLOPT_GD_MLSL_LDS, #> \t         NLOPT_LD_MMA, NLOPT_LD_CCSAQ, NLOPT_LN_COBYLA, #> \t         NLOPT_LN_NEWUOA, NLOPT_LN_NEWUOA_BOUND, #> \t         NLOPT_LN_NELDERMEAD, NLOPT_LN_SBPLX, #> \t         NLOPT_LN_AUGLAG, NLOPT_LD_AUGLAG, #> \t         NLOPT_LN_AUGLAG_EQ, NLOPT_LD_AUGLAG_EQ, #> \t         NLOPT_LN_BOBYQA, NLOPT_GN_ISRES #> \tdefault value:   none #> \tcurrent value:   NLOPT_LD_LBFGS #>  #> \tThis option is required. Check the NLopt website for a description of #> \tthe algorithms. #>  #> stopval #> \tpossible values: -Inf <= stopval <= Inf #> \tdefault value:   -Inf #> \tcurrent value:   (default) #>  #> \tStop minimization when an objective value <= stopval is found. #> \tSetting stopval to -Inf disables this stopping criterion (default). #>  #> ftol_rel #> \tpossible values: ftol_rel > 0 #> \tdefault value:   0.0 #> \tcurrent value:   (default) #>  #> \tStop when an optimization step (or an estimate of the optimum) #> \tchanges the objective function value by less than ftol_rel multiplied #> \tby the absolute value of the function value. If there is any chance #> \tthat your optimum function value is close to zero, you might want to #> \tset an absolute tolerance with ftol_abs as well. Criterion is #> \tdisabled if ftol_rel is non-positive (default). #>  #> ftol_abs #> \tpossible values: ftol_abs > 0 #> \tdefault value:   0.0 #> \tcurrent value:   (default) #>  #> \tStop when an optimization step (or an estimate of the optimum) #> \tchanges the function value by less than ftol_abs. Criterion is #> \tdisabled if ftol_abs is non-positive (default). #>  #> xtol_rel #> \tpossible values: xtol_rel > 0 #> \tdefault value:   1.0e-04 #> \tcurrent value:   1e-08 #>  #> \tStop when an optimization step (or an estimate of the optimum) #> \tchanges every parameter by less than xtol_rel multiplied by the #> \tabsolute value of the parameter. If there is any chance that an #> \toptimal parameter is close to zero, you might want to set an absolute #> \ttolerance with xtol_abs as well. Criterion is disabled if xtol_rel is #> \tnon-positive. #>  #> xtol_abs #> \tpossible values: xtol_abs > 0 #> \tdefault value:   rep(0.0, length(x0)) #> \tcurrent value:   (default) #>  #> \txtol_abs is a vector of length n (the number of elements in x) giving #> \tthe tolerances: stop when an optimization step (or an estimate of the #> \toptimum) changes every parameter x[i] by less than xtol_abs[i]. #> \tCriterion is disabled if all elements of xtol_abs are non-positive #> \t(default). #>  #> maxeval #> \tpossible values: maxeval is a positive integer #> \tdefault value:   100 #> \tcurrent value:   (default) #>  #> \tStop when the number of function evaluations exceeds maxeval. This is #> \tnot a strict maximum: the number of function evaluations may exceed #> \tmaxeval slightly, depending upon the algorithm. Criterion is disabled #> \tif maxeval is non-positive. #>  #> maxtime #> \tpossible values: maxtime > 0 #> \tdefault value:   -1.0 #> \tcurrent value:   (default) #>  #> \tStop when the optimization time (in seconds) exceeds maxtime. This is #> \tnot a strict maximum: the time may exceed maxtime slightly, depending #> \tupon the algorithm and on how slow your function evaluation is. #> \tCriterion is disabled if maxtime is non-positive (default). #>  #> tol_constraints_ineq #> \tpossible values: tol_constraints_ineq > 0.0 #> \tdefault value:   rep(1e-8, num_constraints_ineq) #> \tcurrent value:   (default) #>  #> \tThe parameter tol_constraints_ineq is a vector of tolerances. Each #> \ttolerance corresponds to one of the inequality constraints. The #> \ttolerance is used for the purpose of stopping criteria only: a point #> \tx is considered feasible for judging whether to stop the optimization #> \tif eval_g_ineq(x) <= tol. A tolerance of zero means that NLopt will #> \ttry not to consider any x to be converged unless eval_g_ineq(x) is #> \tstrictly non-positive; generally, at least a small positive tolerance #> \tis advisable to reduce sensitivity to rounding errors. By default the #> \ttolerances for all inequality constraints are set to 1e-8. #>  #> tol_constraints_eq #> \tpossible values: tol_constraints_eq > 0.0 #> \tdefault value:   rep(1e-8, num_constraints_eq) #> \tcurrent value:   (default) #>  #> \tThe parameter tol_constraints_eq is a vector of tolerances. Each #> \ttolerance corresponds to one of the equality constraints. The #> \ttolerance is used for the purpose of stopping criteria only: a point #> \tx is considered feasible for judging whether to stop the optimization #> \tif abs(eval_g_ineq(x)) <= tol. For equality constraints, a small #> \tpositive tolerance is strongly advised in order to allow NLopt to #> \tconverge even if the equality constraint is slightly nonzero. By #> \tdefault the tolerances for all quality constraints are set to 1e-8. #>  #> print_level #> \tpossible values: 0, 1, 2, or 3 #> \tdefault value:   0 #> \tcurrent value:   (default) #>  #> \tThe option print_level controls how much output is shown during the #> \toptimization process. Possible values: 0 (default): no output; 1: #> \tshow iteration number and value of objective function; 2: 1 + show #> \tvalue of (in)equalities; 3: 2 + show value of controls. #>  #> check_derivatives #> \tpossible values: TRUE or FALSE #> \tdefault value:   FALSE #> \tcurrent value:   (default) #>  #> \tThe option check_derivatives can be activated to compare the #> \tuser-supplied analytic gradients with finite difference #> \tapproximations. #>  #> check_derivatives_tol #> \tpossible values: check_derivatives_tol > 0.0 #> \tdefault value:   1e-04 #> \tcurrent value:   (default) #>  #> \tThe option check_derivatives_tol determines when a difference between #> \tan analytic gradient and its finite difference approximation is #> \tflagged as an error. #>  #> check_derivatives_print #> \tpossible values: 'none', 'all', 'errors', #> \tdefault value:   all #> \tcurrent value:   (default) #>  #> \tThe option check_derivatives_print controls the output of the #> \tderivative checker (if check_derivatives == TRUE). All comparisons #> \tare shown ('all'), only those comparisions that resulted in an error #> \t('error'), or only the number of errors is shown ('none'). #>  #> print_options_doc #> \tpossible values: TRUE or FALSE #> \tdefault value:   FALSE #> \tcurrent value:   (default) #>  #> \tIf TRUE, a description of all options and their current and default #> \tvalues is printed to the screen. #>  #> population #> \tpossible values: population is a positive integer #> \tdefault value:   0 #> \tcurrent value:   (default) #>  #> \tSeveral of the stochastic search algorithms (e.g., CRS, MLSL, and #> \tISRES) start by generating some initial population of random points #> \tx. By default, this initial population size is chosen heuristically #> \tin some algorithm-specific way, but the initial population can by #> \tchanged by setting a positive integer value for population. A #> \tpopulation of zero implies that the heuristic default will be used. #>  #> vector_storage #> \tpossible values: vector_storage is a positive integer #> \tdefault value:   20 #> \tcurrent value:   (default) #>  #> \tNumber of gradients to remember from previous optimization steps. #>  #> ranseed #> \tpossible values: ranseed is a positive integer #> \tdefault value:   0 #> \tcurrent value:   (default) #>  #> \tFor stochastic optimization algorithms, pseudorandom numbers are #> \tgenerated. Set the random seed using ranseed if you want to use a #> \t'deterministic' sequence of pseudorandom numbers, i.e. the same #> \tsequence from run to run. If ranseed is 0 (default), the seed for the #> \trandom numbers is generated from the system time, so that you will #> \tget a different sequence of pseudorandom numbers each time you run #> \tyour program. #>"},{"path":"https://astamm.github.io/nloptr/reference/print.nloptr.html","id":null,"dir":"Reference","previous_headings":"","what":"Print results after running nloptr — print.nloptr","title":"Print results after running nloptr — print.nloptr","text":"function prints nloptr object holds results minimization using nloptr.","code":""},{"path":"https://astamm.github.io/nloptr/reference/print.nloptr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print results after running nloptr — print.nloptr","text":"","code":"# S3 method for class 'nloptr' print(x, show.controls = TRUE, ...)"},{"path":"https://astamm.github.io/nloptr/reference/print.nloptr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print results after running nloptr — print.nloptr","text":"x object containing result minimization. show.controls Logical vector indices. show value control variables solution? show.controls vector indices, used select control variables shown. can useful model contains set parameters interest set nuisance parameters immediate interest. ... arguments passed methods.","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/print.nloptr.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print results after running nloptr — print.nloptr","text":"Jelmer Ypma","code":""},{"path":"https://astamm.github.io/nloptr/reference/sbplx.html","id":null,"dir":"Reference","previous_headings":"","what":"Subplex Algorithm — sbplx","title":"Subplex Algorithm — sbplx","text":"Subplex variant Nelder-Mead uses Nelder-Mead sequence subspaces.","code":""},{"path":"https://astamm.github.io/nloptr/reference/sbplx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subplex Algorithm — sbplx","text":"","code":"sbplx(   x0,   fn,   lower = NULL,   upper = NULL,   nl.info = FALSE,   control = list(),   ... )"},{"path":"https://astamm.github.io/nloptr/reference/sbplx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subplex Algorithm — sbplx","text":"x0 starting point searching optimum. fn objective function minimized. lower, upper lower upper bound constraints. nl.info logical; shall original NLopt info shown. control list options, see nl.opts help. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/sbplx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Subplex Algorithm — sbplx","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 0) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/sbplx.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Subplex Algorithm — sbplx","text":"SUBPLEX claimed much efficient robust original Nelder-Mead retaining latter's facility discontinuous objectives. implementation explicit support bound constraints via method Box paper described neldermead help page.","code":""},{"path":"https://astamm.github.io/nloptr/reference/sbplx.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Subplex Algorithm — sbplx","text":"request Tom Rowan reimplementations algorithm shall use name `subplex'.","code":""},{"path":"https://astamm.github.io/nloptr/reference/sbplx.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Subplex Algorithm — sbplx","text":"T. Rowan, “Functional Stability Analysis Numerical Algorithms”, Ph.D.  thesis, Department Computer Sciences, University Texas Austin, 1990.","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/sbplx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Subplex Algorithm — sbplx","text":"","code":"# Fletcher and Powell's helic valley fphv <- function(x)   100*(x[3] - 10*atan2(x[2], x[1])/(2*pi))^2 +     (sqrt(x[1]^2 + x[2]^2) - 1)^2 +x[3]^2 x0 <- c(-1, 0, 0) sbplx(x0, fphv)  #  1 0 0 #> $par #> [1] 1.000000e+00 3.706887e-12 5.858708e-12 #>  #> $value #> [1] 3.449246e-23 #>  #> $iter #> [1] 994 #>  #> $convergence #> [1] 4 #>  #> $message #> [1] \"NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached.\" #>   # Powell's Singular Function (PSF) psf <- function(x)  (x[1] + 10*x[2])^2 + 5*(x[3] - x[4])^2 +           (x[2] - 2*x[3])^4 + 10*(x[1] - x[4])^4 x0 <- c(3, -1, 0, 1) sbplx(x0, psf, control = list(maxeval = Inf, ftol_rel = 1e-6)) #  0 0 0 0 (?) #> Warning: NAs introduced by coercion to integer range #> $par #> [1]  0.012385093 -0.001238441  0.007823193  0.007827134 #>  #> $value #> [1] 8.567466e-08 #>  #> $iter #> [1] 78796 #>  #> $convergence #> [1] 3 #>  #> $message #> [1] \"NLOPT_FTOL_REACHED: Optimization stopped because ftol_rel or ftol_abs (above) was reached.\" #>"},{"path":"https://astamm.github.io/nloptr/reference/slsqp.html","id":null,"dir":"Reference","previous_headings":"","what":"Sequential Quadratic Programming (SQP) — slsqp","title":"Sequential Quadratic Programming (SQP) — slsqp","text":"Sequential (least-squares) quadratic programming (SQP) algorithm nonlinearly constrained, gradient-based optimization, supporting equality inequality constraints.","code":""},{"path":"https://astamm.github.io/nloptr/reference/slsqp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sequential Quadratic Programming (SQP) — slsqp","text":"","code":"slsqp(   x0,   fn,   gr = NULL,   lower = NULL,   upper = NULL,   hin = NULL,   hinjac = NULL,   heq = NULL,   heqjac = NULL,   nl.info = FALSE,   control = list(),   deprecatedBehavior = TRUE,   ... )"},{"path":"https://astamm.github.io/nloptr/reference/slsqp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sequential Quadratic Programming (SQP) — slsqp","text":"x0 starting point searching optimum. fn objective function minimized. gr gradient function fn; calculated numerically specified. lower, upper lower upper bound constraints. hin function defining inequality constraints, hin <= 0 components. new behavior line rest nloptr arguments. use old behavior, please set deprecatedBehavior TRUE. hinjac Jacobian function hin; calculated numerically specified. heq function defining equality constraints, heq = 0 components. heqjac Jacobian function heq; calculated numerically specified. nl.info logical; shall original NLopt info shown. control list options, see nl.opts help. deprecatedBehavior logical; TRUE (default now), old behavior Jacobian function used, equality \\(\\ge 0\\) instead \\(\\le 0\\). reversed future release eventually removed. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/slsqp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sequential Quadratic Programming (SQP) — slsqp","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 1) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/slsqp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sequential Quadratic Programming (SQP) — slsqp","text":"algorithm optimizes successive second-order (quadratic/least-squares) approximations objective function (via BFGS updates), first-order (affine) approximations constraints.","code":""},{"path":"https://astamm.github.io/nloptr/reference/slsqp.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Sequential Quadratic Programming (SQP) — slsqp","text":"See infos https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/.","code":""},{"path":"https://astamm.github.io/nloptr/reference/slsqp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sequential Quadratic Programming (SQP) — slsqp","text":"Dieter Kraft, “software package sequential quadratic programming”, Technical Report DFVLR-FB 88-28, Institut fuer Dynamik der Flugsysteme, Oberpfaffenhofen, July 1988.","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/slsqp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Sequential Quadratic Programming (SQP) — slsqp","text":"Hans W. Borchers","code":""},{"path":"https://astamm.github.io/nloptr/reference/slsqp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sequential Quadratic Programming (SQP) — slsqp","text":"","code":"##  Solve the Hock-Schittkowski problem no. 100 with analytic gradients ##  See https://apmonitor.com/wiki/uploads/Apps/hs100.apm  x0.hs100 <- c(1, 2, 0, 4, 0, 1, 1) fn.hs100 <- function(x) {(x[1] - 10) ^ 2 + 5 * (x[2] - 12) ^ 2 + x[3] ^ 4 +                          3 * (x[4] - 11) ^ 2 + 10 * x[5] ^ 6 + 7 * x[6] ^ 2 +                          x[7] ^ 4 - 4 * x[6] * x[7] - 10 * x[6] - 8 * x[7]}  hin.hs100 <- function(x) {c( 2 * x[1] ^ 2 + 3 * x[2] ^ 4 + x[3] + 4 * x[4] ^ 2 + 5 * x[5] - 127, 7 * x[1] + 3 * x[2] + 10 * x[3] ^ 2 + x[4] - x[5] - 282, 23 * x[1] + x[2] ^ 2 + 6 * x[6] ^ 2 - 8 * x[7] - 196, 4 * x[1] ^ 2 + x[2] ^ 2 - 3 * x[1] * x[2] + 2 * x[3] ^ 2 + 5 * x[6] -  11 * x[7]) }  S <- slsqp(x0.hs100, fn = fn.hs100,   # no gradients and jacobians provided      hin = hin.hs100,      nl.info = TRUE,      control = list(xtol_rel = 1e-8, check_derivatives = TRUE),      deprecatedBehavior = FALSE) #> Checking gradients of objective function. #> Derivative checker results: 0 error(s) detected. #>  #>   eval_grad_f[1] = -1.800000e+01 ~ -1.8e+01   [ 3.023892e-10] #>   eval_grad_f[2] = -1.000000e+02 ~ -1.0e+02   [ 8.540724e-14] #>   eval_grad_f[3] =  0.000000e+00 ~  0.0e+00   [ 0.000000e+00] #>   eval_grad_f[4] = -4.200000e+01 ~ -4.2e+01   [ 4.384556e-12] #>   eval_grad_f[5] =  0.000000e+00 ~  0.0e+00   [ 0.000000e+00] #>   eval_grad_f[6] = -1.877429e-08 ~  0.0e+00   [-1.877429e-08] #>   eval_grad_f[7] = -8.000000e+00 ~ -8.0e+00   [ 6.102499e-10] #>  #> Checking gradients of inequality constraints. #> Derivative checker results: 0 error(s) detected. #>  #>   eval_jac_g_ineq[1, 1] =  4.0e+00 ~  4.0e+00   [2.355338e-11] #>   eval_jac_g_ineq[2, 1] =  7.0e+00 ~  7.0e+00   [2.278881e-10] #>   eval_jac_g_ineq[3, 1] =  2.3e+01 ~  2.3e+01   [5.297235e-11] #>   eval_jac_g_ineq[4, 1] =  2.0e+00 ~  2.0e+00   [1.311518e-11] #>   eval_jac_g_ineq[1, 2] =  9.6e+01 ~  9.6e+01   [1.985688e-08] #>   eval_jac_g_ineq[2, 2] =  3.0e+00 ~  3.0e+00   [2.191188e-10] #>   eval_jac_g_ineq[3, 2] =  4.0e+00 ~  4.0e+00   [5.631432e-10] #>   eval_jac_g_ineq[4, 2] =  1.0e+00 ~  1.0e+00   [4.978373e-11] #>   eval_jac_g_ineq[1, 3] =  1.0e+00 ~  1.0e+00   [5.631432e-10] #>   eval_jac_g_ineq[2, 3] =  0.0e+00 ~  0.0e+00   [0.000000e+00] #>   eval_jac_g_ineq[3, 3] =  0.0e+00 ~  0.0e+00   [0.000000e+00] #>   eval_jac_g_ineq[4, 3] =  0.0e+00 ~  0.0e+00   [0.000000e+00] #>   eval_jac_g_ineq[1, 4] =  3.2e+01 ~  3.2e+01   [7.463696e-09] #>   eval_jac_g_ineq[2, 4] =  1.0e+00 ~  1.0e+00   [2.909929e-09] #>   eval_jac_g_ineq[3, 4] =  0.0e+00 ~  0.0e+00   [0.000000e+00] #>   eval_jac_g_ineq[4, 4] =  0.0e+00 ~  0.0e+00   [0.000000e+00] #>   eval_jac_g_ineq[1, 5] =  5.0e+00 ~  5.0e+00   [9.378596e-11] #>   eval_jac_g_ineq[2, 5] = -1.0e+00 ~ -1.0e+00   [2.909929e-09] #>   eval_jac_g_ineq[3, 5] =  0.0e+00 ~  0.0e+00   [0.000000e+00] #>   eval_jac_g_ineq[4, 5] =  0.0e+00 ~  0.0e+00   [0.000000e+00] #>   eval_jac_g_ineq[1, 6] =  0.0e+00 ~  0.0e+00   [0.000000e+00] #>   eval_jac_g_ineq[2, 6] =  0.0e+00 ~  0.0e+00   [0.000000e+00] #>   eval_jac_g_ineq[3, 6] =  1.2e+01 ~  1.2e+01   [1.720122e-10] #>   eval_jac_g_ineq[4, 6] =  5.0e+00 ~  5.0e+00   [5.781509e-12] #>   eval_jac_g_ineq[1, 7] =  0.0e+00 ~  0.0e+00   [0.000000e+00] #>   eval_jac_g_ineq[2, 7] =  0.0e+00 ~  0.0e+00   [0.000000e+00] #>   eval_jac_g_ineq[3, 7] = -8.0e+00 ~ -8.0e+00   [2.355338e-11] #>   eval_jac_g_ineq[4, 7] = -1.1e+01 ~ -1.1e+01   [3.114761e-12] #>  #>  #> Call: #> nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper,  #>     eval_g_ineq = hin, eval_jac_g_ineq = hinjac, eval_g_eq = heq,  #>     eval_jac_g_eq = heqjac, opts = opts) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because  #> xtol_rel or xtol_abs (above) was reached. ) #>  #> Number of Iterations....: 60  #> Termination conditions:  stopval: -Inf\txtol_rel: 1e-08\tmaxeval: 1000\tftol_rel: 0\tftol_abs: 0  #> Number of inequality constraints:  4  #> Number of equality constraints:    0  #> Optimal value of objective function:  680.630057364075  #> Optimal value of controls: 2.330497 1.951371 -0.4775421 4.36573 -0.6244872 1.038139 1.594228 #>  #>   ##  The optimum value of the objective function should be 680.6300573 ##  A suitable parameter vector is roughly ##  (2.330, 1.9514, -0.4775, 4.3657, -0.6245, 1.0381, 1.5942)  S #> $par #> [1]  2.3304967  1.9513713 -0.4775421  4.3657298 -0.6244872  1.0381386  1.5942275 #>  #> $value #> [1] 680.6301 #>  #> $iter #> [1] 60 #>  #> $convergence #> [1] 4 #>  #> $message #> [1] \"NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached.\" #>"},{"path":"https://astamm.github.io/nloptr/reference/stogo.html","id":null,"dir":"Reference","previous_headings":"","what":"Stochastic Global Optimization — stogo","title":"Stochastic Global Optimization — stogo","text":"StoGO global optimization algorithm works systematically dividing search space—must bound-constrained—smaller hyper-rectangles via branch--bound technique, searching using gradient-based local-search algorithm (BFGS variant), optionally including randomness.","code":""},{"path":"https://astamm.github.io/nloptr/reference/stogo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stochastic Global Optimization — stogo","text":"","code":"stogo(   x0,   fn,   gr = NULL,   lower = NULL,   upper = NULL,   maxeval = 10000,   xtol_rel = 1e-06,   randomized = FALSE,   nl.info = FALSE,   ... )"},{"path":"https://astamm.github.io/nloptr/reference/stogo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stochastic Global Optimization — stogo","text":"x0 initial point searching optimum. fn objective function minimized. gr optional gradient objective function. lower, upper lower upper bound constraints. maxeval maximum number function evaluations. xtol_rel stopping criterion relative change reached. randomized logical; shall randomizing variant used? nl.info logical; shall original NLopt info shown. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/stogo.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Stochastic Global Optimization — stogo","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 0) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/stogo.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Stochastic Global Optimization — stogo","text":"bounds-constrained problems supported algorithm.","code":""},{"path":"https://astamm.github.io/nloptr/reference/stogo.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Stochastic Global Optimization — stogo","text":"S. Zertchaninov K. Madsen, “C++ Programme Global Optimization,” IMM-REP-1998-04, Department Mathematical Modelling, Technical University Denmark.","code":""},{"path":"https://astamm.github.io/nloptr/reference/stogo.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Stochastic Global Optimization — stogo","text":"Hans W. Borchers","code":""},{"path":"https://astamm.github.io/nloptr/reference/stogo.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Stochastic Global Optimization — stogo","text":"","code":"## Rosenbrock Banana objective function  rbf <- function(x) {(1 - x[1]) ^ 2 + 100 * (x[2] - x[1] ^ 2) ^ 2}  x0 <- c(-1.2, 1) lb <- c(-3, -3) ub <- c(3,  3)  ## The function as written above has a minimum of 0 at (1, 1)  stogo(x0 = x0, fn = rbf, lower = lb, upper = ub) #> $par #> [1] 0.9999934 0.9999865 #>  #> $value #> [1] 5.618383e-11 #>  #> $iter #> [1] 10000 #>  #> $convergence #> [1] 1 #>  #> $message #> [1] \"NLOPT_SUCCESS: Generic success return value.\" #>"},{"path":"https://astamm.github.io/nloptr/reference/tnewton.html","id":null,"dir":"Reference","previous_headings":"","what":"Preconditioned Truncated Newton — tnewton","title":"Preconditioned Truncated Newton — tnewton","text":"Truncated Newton methods, also called Newton-iterative methods, solve approximating Newton system using conjugate-gradient approach related limited-memory BFGS.","code":""},{"path":"https://astamm.github.io/nloptr/reference/tnewton.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preconditioned Truncated Newton — tnewton","text":"","code":"tnewton(   x0,   fn,   gr = NULL,   lower = NULL,   upper = NULL,   precond = TRUE,   restart = TRUE,   nl.info = FALSE,   control = list(),   ... )"},{"path":"https://astamm.github.io/nloptr/reference/tnewton.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preconditioned Truncated Newton — tnewton","text":"x0 starting point searching optimum. fn objective function minimized. gr gradient function fn; calculated numerically specified. lower, upper lower upper bound constraints. precond logical; preset L-BFGS steepest descent. restart logical; restarting L-BFGS steepest descent. nl.info logical; shall original NLopt info shown. control list options, see nl.opts help. ... additional arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/tnewton.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preconditioned Truncated Newton — tnewton","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 1) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/tnewton.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preconditioned Truncated Newton — tnewton","text":"Truncated Newton methods based approximating objective quadratic function applying iterative scheme linear conjugate-gradient algorithm.","code":""},{"path":"https://astamm.github.io/nloptr/reference/tnewton.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Preconditioned Truncated Newton — tnewton","text":"Less reliable Newton's method, can handle large problems.","code":""},{"path":"https://astamm.github.io/nloptr/reference/tnewton.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Preconditioned Truncated Newton — tnewton","text":"R. S. Dembo T. Steihaug, “Truncated Newton algorithms large-scale optimization,” Math. Programming 26, p. 190-212 (1982).","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/tnewton.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Preconditioned Truncated Newton — tnewton","text":"Hans W. Borchers","code":""},{"path":"https://astamm.github.io/nloptr/reference/tnewton.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preconditioned Truncated Newton — tnewton","text":"","code":"flb <- function(x) {   p <- length(x)   sum(c(1, rep(4, p - 1)) * (x - c(1, x[-p]) ^ 2) ^ 2) } # 25-dimensional box constrained: par[24] is *not* at boundary S <- tnewton(rep(3, 25L), flb, lower = rep(2, 25L), upper = rep(4, 25L),        nl.info = TRUE, control = list(xtol_rel = 1e-8)) #>  #> Call: #> nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper,  #>     opts = opts) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success return value. ) #>  #> Number of Iterations....: 17  #> Termination conditions:  stopval: -Inf\txtol_rel: 1e-08\tmaxeval: 1000\tftol_rel: 0\tftol_abs: 0  #> Number of inequality constraints:  0  #> Number of equality constraints:    0  #> Optimal value of objective function:  368.105912874334  #> Optimal value of controls: 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2.109093 4 #>  #>  ## Optimal value of objective function:  368.105912874334 ## Optimal value of controls: 2  ...  2  2.109093  4"},{"path":"https://astamm.github.io/nloptr/reference/varmetric.html","id":null,"dir":"Reference","previous_headings":"","what":"Shifted Limited-memory Variable-metric — varmetric","title":"Shifted Limited-memory Variable-metric — varmetric","text":"Shifted limited-memory variable-metric algorithm.","code":""},{"path":"https://astamm.github.io/nloptr/reference/varmetric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shifted Limited-memory Variable-metric — varmetric","text":"","code":"varmetric(   x0,   fn,   gr = NULL,   rank2 = TRUE,   lower = NULL,   upper = NULL,   nl.info = FALSE,   control = list(),   ... )"},{"path":"https://astamm.github.io/nloptr/reference/varmetric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shifted Limited-memory Variable-metric — varmetric","text":"x0 initial point searching optimum. fn objective function minimized. gr gradient function fn; calculated numerically specified. rank2 logical; true uses rank-2 update method, else rank-1. lower, upper lower upper bound constraints. nl.info logical; shall original NLopt info shown. control list control parameters, see nl.opts help. ... arguments passed function.","code":""},{"path":"https://astamm.github.io/nloptr/reference/varmetric.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Shifted Limited-memory Variable-metric — varmetric","text":"List components: par optimal solution found far. value function value corresponding par. iter number (outer) iterations, see maxeval. convergence integer code indicating successful completion (> 0) possible error number (< 0). message character string produced NLopt giving additional information.","code":""},{"path":"https://astamm.github.io/nloptr/reference/varmetric.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Shifted Limited-memory Variable-metric — varmetric","text":"Variable-metric methods variant quasi-Newton methods, especially adapted large-scale unconstrained (bound constrained) minimization.","code":""},{"path":"https://astamm.github.io/nloptr/reference/varmetric.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Shifted Limited-memory Variable-metric — varmetric","text":"Based L. Luksan's Fortran implementation shifted limited-memory variable-metric algorithm.","code":""},{"path":"https://astamm.github.io/nloptr/reference/varmetric.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Shifted Limited-memory Variable-metric — varmetric","text":"J. Vlcek L. Luksan, “Shifted limited-memory variable metric methods large-scale unconstrained minimization,” J. Computational Appl. Math. 186, p. 365-390 (2006).","code":""},{"path":[]},{"path":"https://astamm.github.io/nloptr/reference/varmetric.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Shifted Limited-memory Variable-metric — varmetric","text":"Hans W. Borchers","code":""},{"path":"https://astamm.github.io/nloptr/reference/varmetric.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Shifted Limited-memory Variable-metric — varmetric","text":"","code":"flb <- function(x) {   p <- length(x)   sum(c(1, rep(4, p-1)) * (x - c(1, x[-p])^2)^2) } # 25-dimensional box constrained: par[24] is *not* at the boundary S <- varmetric(rep(3, 25), flb, lower=rep(2, 25), upper=rep(4, 25),      nl.info = TRUE, control = list(xtol_rel=1e-8)) #>  #> Call: #> nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper,  #>     opts = opts) #>  #>  #> Minimization using NLopt version 2.7.1  #>  #> NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success return value. ) #>  #> Number of Iterations....: 19  #> Termination conditions:  stopval: -Inf\txtol_rel: 1e-08\tmaxeval: 1000\tftol_rel: 0\tftol_abs: 0  #> Number of inequality constraints:  0  #> Number of equality constraints:    0  #> Optimal value of objective function:  368.105912874334  #> Optimal value of controls: 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2.109093 4 #>  #>  ## Optimal value of objective function:  368.105912874334 ## Optimal value of controls: 2  ...  2  2.109093  4"},{"path":[]},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-220","dir":"Changelog","previous_headings":"","what":"nloptr 2.2.0","title":"nloptr 2.2.0","text":"CRAN release: 2025-03-12 release fixes breaking bug affects nloptr reverse dependencies brings minor tweaks corrections along way: Minimal patch build nlopt 2.9.x (#176, @jaganmn): specifically, issuses () nlopt versions 2.9.x one less algorithm enum list, namely NLOPT_LD_LBFGS_NOCEDAL removed versions put back 2.10 (ii) using inst/include copied nlopt headers conditionally build package (e.g. listing PKG_CPPFLAGS) resulting possible version conflicts. Fix 2 failed tests adding one termination criterion. Update GHA workflows latest versions. Fix reverse LinkingTo dependencies () unconditionally copying headers inst/include (ii) fixing cmake path search (#179, @astamm). Fix newly broken kergp package due wrong usage statement inside paste() works differently used inside c() (#180, @astamm). Update artifact action v4 (#174, @eddelbuettel). Correcting unit tests test-banana (#167, @aadler). Correcting unit tests test-global-wrapper (#166, @aadler). Update code nloptr.c compatibility R API, efficiency, formatting (#169, @aadler). Bugfix: ranseed expects unsigned long passed integer, thus reducing range random seeds. now passed double converted long (#169, @aadler).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-211","dir":"Changelog","previous_headings":"","what":"nloptr 2.1.1","title":"nloptr 2.1.1","text":"CRAN release: 2024-06-25 patch release work around bug CRAN checks. Specifically, one unit tests isres() algorithm failing CRAN builds convergence stochastic slightly different results even fixed seed prior calling function.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-210","dir":"Changelog","previous_headings":"","what":"nloptr 2.1.0","title":"nloptr 2.1.0","text":"CRAN release: 2024-06-19 release deprecates default behavior inequality equations wrapper function uses . Currently, calibrated >= 0. version allows equations consistent main nloptr function, requires <= 0. future release, default behavior switch assuming calibration <= 0, eventually, >= 0 behavior removed. also includes large number safety efficiency changes, expansion unit tests 100% coverage files one. major changes include: Reversed direction inequality equations hin hinjac wrapper functions use , bringing compliance main nloptr call. addresses Issue #148; Cleaned Hock-Schittkowski problem . 100, Hartmann 6-dimensional, Powell exponential examples. addresses Issue #152 Issue #156; Updated roxygen version; Updated maintainer email; Deal NA returns parallel::detectCores() (contributed @jeroen PR #150); Setup rhub v2 checks; Update cmake installation instructions Mac brew (#146); Allow use equality constraints COBYLA (#135); Replaced unit testing framework testthat tinytest (See Issue #136); Brought coverage .nloptr 100%. file completely covered unit tests nloptr.c. uncovered calls error messages get trapped tests R call gets C; Linted package code correctness consistency; Updated vignette, DESCRIPTION, NEWS; Updated package website use bootstrap 5; Expanded unit tests: coverage now 97% file 90%; Removed forcing C++11; Added safety checks C code; Added many safety efficiency enhancements R code; R code style made self-consistent; Updated documentation messages accuracy mathematical formatting Updated Github actions; bugfixes (e.g. isres warning nl.grad). Please see commit logs detailed descriptions changes.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-203","dir":"Changelog","previous_headings":"","what":"nloptr 2.0.3","title":"nloptr 2.0.3","text":"CRAN release: 2022-05-26 Improved compatibility RHEL/CentOS first searching cmake3 binary PATH (#104). Improved backward compatibility older versions cmake (#119).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-202","dir":"Changelog","previous_headings":"","what":"nloptr 2.0.2","title":"nloptr 2.0.2","text":"CRAN release: 2022-05-19 patch version : link nlopt library via nlopt/lib/libnlopt.instead -Lnlopt/lib -lnlopt building nlopt included sources avoid potential mess -lnlopt look nlopt library places possibly link existing old system build nlopt. Additionally, contacted Simon Urbanek updating nlopt recipe macOS users now match latest v2.7.1, avoid nlopt built fly CRAN machines.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-201","dir":"Changelog","previous_headings":"","what":"nloptr 2.0.1","title":"nloptr 2.0.1","text":"CRAN release: 2022-05-03 release mainly increasing direct compatibility user cases. details, list changes made: Update SystemRequirements description make clearer minimal versions cmake (>= 3.15.0) nlopt (>= 2.7.0) required (#100, @HenrikBengtsson). End configuration sooner louder cmake missing needed clearer message (#103, @eddelbuettel). Ensure system-wide installation cmake list suggestions install missing. Update GHA scripts latest versions. Configure git always use LF line endings configure.ac file. Add CI R-devel Windows Rtools42. Fix compatibility versions R anterior 4.0 (#111). Look cmake3 binary current path cmake increasing compatibility RHEL/CentOS users (#104, @bhogan-mitre, @HenrikBengtsson).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-200","dir":"Changelog","previous_headings":"","what":"nloptr 2.0.0","title":"nloptr 2.0.0","text":"CRAN release: 2022-01-26","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"major-changes-2-0-0","dir":"Changelog","previous_headings":"","what":"Major changes","title":"nloptr 2.0.0","text":"Use CMake build nlopt included sources macOS Linux system build NLopt (>= 2.7.0) found. Update included sources NLopt latest version (2.7.1). Put back ability Linux platforms re-use existing external build NLopt instead building included sources (contributed Dirk Eddelbuettel, #88). Now builds using NLopt rwinlib Windows current release (contributed Jeroen Ooms, #92), NLopt Rtools42 Windows devel (contributed Tomas Kalibera).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"minor-changes-2-0-0","dir":"Changelog","previous_headings":"","what":"Minor changes","title":"nloptr 2.0.0","text":"Added NEWS.md file track changes package. Use markdown Roxygen documentation. Added logo proper nloptr website. Added coverage. Switch Travis Github Actions CI. Use Catch unit testing C/C++ code. Now tracking code coverage. Update NLopt-related URLs following migration NLopt website. Fixed bug avoid linking issues using C API via #include <nloptrAPI.h> several source files. Fix precision issue test example hs071 (#81, @Tom-python0121). Made NLopt algorithm NLOPT_GN_ESCH available R interface (contributed Xiongtao Dai).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-122-29-february-2020","dir":"Changelog","previous_headings":"","what":"nloptr 1.2.2 (29 February 2020)","title":"nloptr 1.2.2 (29 February 2020)","text":"CRAN release: 2020-02-29 Replaced deprecated functions testthat framework unit tests (contributed Avraham Adler).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"id_26-february-1-2-2","dir":"Changelog","previous_headings":"","what":"26 February 2020:","title":"nloptr 1.2.2 (29 February 2020)","text":"Fixed warnings (requested CRAN): R CMD config variables ‘CPP’ ‘CXXCPP’ deprecated.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"id_20-october-1-2-2","dir":"Changelog","previous_headings":"","what":"20 October 2018:","title":"nloptr 1.2.2 (29 February 2020)","text":"Exposed CCSAQ algorithm R interface (contributed Julien Chiquet).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-121-03-october-2018","dir":"Changelog","previous_headings":"","what":"nloptr 1.2.1 (03 October 2018)","title":"nloptr 1.2.1 (03 October 2018)","text":"CRAN release: 2018-10-03 Build process changed solve issues several OS (many thanks CRAN maintainers).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-120-30-september-2018","dir":"Changelog","previous_headings":"","what":"nloptr 1.2.0 (30 September 2018)","title":"nloptr 1.2.0 (30 September 2018)","text":"CRAN release: 2018-09-30","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"id_21-april-1-2-0","dir":"Changelog","previous_headings":"","what":"21 April 2018:","title":"nloptr 1.2.0 (30 September 2018)","text":"Changed installation procedure. NLopt source code now part nloptr package downloaded separately configure Unix systems case NLopt library found. Registered NLopt C functions used external R package included C API. Documentation generated using roxygen.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"id_25-september-1-2-0","dir":"Changelog","previous_headings":"","what":"25 September 2017:","title":"nloptr 1.2.0 (30 September 2018)","text":"Fixed bug auglag. BOBYQA now allowed local solver (thanks Léo Belzile).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-110-22-august-2016","dir":"Changelog","previous_headings":"","what":"nloptr 1.1.0 (22 August 2016)","title":"nloptr 1.1.0 (22 August 2016)","text":"Fixed bug sometimes caused segmentation faults due uninitialized vector tolerances inequality /equality constraints (thanks Florian Schwendinger).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-109-22-march-2015","dir":"Changelog","previous_headings":"","what":"nloptr 1.0.9 (22 March 2015)","title":"nloptr 1.0.9 (22 March 2015)","text":"problem fails error status 6 (NLOPT_MAXTIME_REACHED) maxtime set positive number options, nloptr tries couple times solve problem. new approach solve bug NLopt sometimes exits problem stating maximum available time reached, even limit time set. Changed warning message order show consistency rest package inequality sign functions auglag, cobyla, isres, mma, slsqp switched >= <= future nloptr version.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-108-22-february-2015","dir":"Changelog","previous_headings":"","what":"nloptr 1.0.8 (22 February 2015)","title":"nloptr 1.0.8 (22 February 2015)","text":"Changed description DESCRIPTION start package name (requested CRAN).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-107-14-february-2015","dir":"Changelog","previous_headings":"","what":"nloptr 1.0.7 (14 February 2015)","title":"nloptr 1.0.7 (14 February 2015)","text":"Changed title field DESCRIPTION title case (requested CRAN). Added donttest around example mma documentation.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-106-8-february-2015","dir":"Changelog","previous_headings":"","what":"nloptr 1.0.6 (8 February 2015)","title":"nloptr 1.0.6 (8 February 2015)","text":"Updated description better reflect installation procedure Linux NLopt pre-installed (requested CRAN).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-105-28-january-2015","dir":"Changelog","previous_headings":"","what":"nloptr 1.0.5 (28 January 2015)","title":"nloptr 1.0.5 (28 January 2015)","text":"Added non-exported functions CFlags LdFlags used packages want link NLopt C library. consistency rest package inequality sign functions auglag, cobyla, isres, mma, slsqp switched >= <= next nloptr version. current version nloptr shows warning using functions inequality constraints. warning can turned options('nloptr.show.inequality.warning' = FALSE).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-104-02-august-2014","dir":"Changelog","previous_headings":"","what":"nloptr 1.0.4 (02 August 2014)","title":"nloptr 1.0.4 (02 August 2014)","text":"CRAN release: 2014-08-04 Increased version number re-submit package CRAN CRLF line endings removed configure configure.ac.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-103-25-july-2014","dir":"Changelog","previous_headings":"","what":"nloptr 1.0.3 (25 July 2014)","title":"nloptr 1.0.3 (25 July 2014)","text":"Changed NLOPT_VERSION 2.4.2 Linux. Changed nloptr.default.options data.frame function returning data.frame.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-102-25-july-2014","dir":"Changelog","previous_headings":"","what":"nloptr 1.0.2 (25 July 2014)","title":"nloptr 1.0.2 (25 July 2014)","text":"Added configure script tests system NLopt library via pkg-config uses sufficiently recent (ie 2.4.*), otherwise configure downloads, patches builds NLopt sources just src/Makevars used (thanks Dirk Eddelbuettel).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-101-05-may-2014","dir":"Changelog","previous_headings":"","what":"nloptr 1.0.1 (05 May 2014)","title":"nloptr 1.0.1 (05 May 2014)","text":"unit tests now enabled use package testthat. Install package argument INSTALL_opt = “–install-tests” supplied install.packages install tests. tests can run installation test_package(‘nloptr’). testthat package needs installed loaded able run tests. Changed default value maxtime option 0.0 -1.0. cases nloptr returned NLOPT_MAXTIME_REACHED without running iterations default setting. change solves . Replaced cat message warning. Messages can suppressed suppressMessages.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-100-27-january-2014","dir":"Changelog","previous_headings":"","what":"nloptr 1.0.0 (27 January 2014)","title":"nloptr 1.0.0 (27 January 2014)","text":"CRAN release: 2014-02-16 Merged wrappers nloptwrap package.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-096-19-november-2013","dir":"Changelog","previous_headings":"","what":"nloptr 0.9.6 (19 November 2013)","title":"nloptr 0.9.6 (19 November 2013)","text":"CRAN release: 2013-11-19 Added line Makevars replace code NLopt fix compilation Solaris requested Brian Ripley.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-095-12-november-2013","dir":"Changelog","previous_headings":"","what":"nloptr 0.9.5 (12 November 2013)","title":"nloptr 0.9.5 (12 November 2013)","text":"CRAN release: 2013-11-13 Updated references NLopt version 2.3 NLopt version 2.4 installation instructions INSTALL.windows. Added line Makevars replaces code related type-casting NLopt-2.4/isres/isres.c. Changed encoding src/nloptr.c CP1252 UTF-8.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-094-09-november-2013","dir":"Changelog","previous_headings":"","what":"nloptr 0.9.4 (09 November 2013)","title":"nloptr 0.9.4 (09 November 2013)","text":"Updated NLopt version 2.4. Changed tests use unit testing package testthat (currently disabled). Fixed segfault started occur latest version Ubuntu. Slightly changed build process (Removed -lstdc++ linker statement. file dummy.cpp (C++ extension) added source directory ensure linking C++. Thanks Brian Ripley bringing .)","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-093-31-july-2013","dir":"Changelog","previous_headings":"","what":"nloptr 0.9.3 (31 July 2013)","title":"nloptr 0.9.3 (31 July 2013)","text":"CRAN release: 2013-07-31 Split lines longer 100 characters check.derivatives examples two lines comply new package rules. Moved vignettes inst/doc vignettes comply new package rules. Removed dependency apacite vignette update apacite CRAN resulted errors.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-092-11-july-2013","dir":"Changelog","previous_headings":"","what":"nloptr 0.9.2 (11 July 2013)","title":"nloptr 0.9.2 (11 July 2013)","text":"Made changes bibtex file documentation. Removed CFLAGS, CXXFLAGS Makevars.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-090","dir":"Changelog","previous_headings":"","what":"nloptr 0.9.0","title":"nloptr 0.9.0","text":"Introduced new print_level = 3. Shows values controls (16 April 2012 R-Forge). Changed Makevars Makevars.win link version 2.3 NLopt compiled ---cxx option. makes StoGo algorithm available. (31 April 2013 R-Forge).","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-089-18-november-2011","dir":"Changelog","previous_headings":"","what":"nloptr 0.8.9 (18 November 2011)","title":"nloptr 0.8.9 (18 November 2011)","text":"CRAN release: 2011-11-23 Changed CRLF CR line endings src/Makevars LF line endings remove warning R CMD check. Adopted changes proposed Brian Ripley src/Makevars.win order nloptr work new toolchain.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-088-28-september-2011","dir":"Changelog","previous_headings":"","what":"nloptr 0.8.8 (28 September 2011)","title":"nloptr 0.8.8 (28 September 2011)","text":"CRAN release: 2011-11-02 Updated src/Makevars compile Solaris.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-087-24-september-2011","dir":"Changelog","previous_headings":"","what":"nloptr 0.8.7 (24 September 2011)","title":"nloptr 0.8.7 (24 September 2011)","text":"CRAN release: 2011-10-27 Updated src/Makevars compile Solaris.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-086-19-september-2011","dir":"Changelog","previous_headings":"","what":"nloptr 0.8.6 (19 September 2011)","title":"nloptr 0.8.6 (19 September 2011)","text":"CRAN release: 2011-10-24 Updated src/Makevars compile Solaris.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-085-03-september-2011","dir":"Changelog","previous_headings":"","what":"nloptr 0.8.5 (03 September 2011)","title":"nloptr 0.8.5 (03 September 2011)","text":"CRAN release: 2011-09-05 Updated src/Makevars compile working binary MacOS.","code":""},{"path":"https://astamm.github.io/nloptr/news/index.html","id":"nloptr-084-12-august-2011","dir":"Changelog","previous_headings":"","what":"nloptr 0.8.4 (12 August 2011)","title":"nloptr 0.8.4 (12 August 2011)","text":"CRAN release: 2011-08-13 data/nloptr.default.options.R: new file description options, mostly taken NLopt website (internal use). R/nloptr.print.options.R: function show description specific (set ) option(s). E.g. nloptr.print.options(option=\"maxeval\"). nloptr.print.options() shows description options called without arguments. added option print description options values (print_options_doc = TRUE/FALSE). added option population set population stochastic/global solvers (population = 1000). added option ranseed sets random seed stochastic solvers (ranseed = 3141). value 0 uses random seed generated system time. option check_derivatives longer listed termination condition. documented option set tolerance ()equality constraints (tol_constraints_eq, tol_constraints_ineq). tests/banana_global.R: new test file uses algorithms (CRS, ISRES, MLSL) options ranseed population. src/nloptr.c: capture error codes setting options. R/nloptr.print.R: output gives ‘optimal value controls’ status = -4 (error code), changed ‘current value controls’.","code":""}]
